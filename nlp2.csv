Title,Abstract,Discussion
"Leveraging Responsible, Explainable, and Local Artificial Intelligence Solutions for Clinical Public Health in the Global South","In the present paper, we will explore how artificial intelligence (AI) and big data analytics (BDA) can help address clinical public and global health needs in the Global South, leveraging and capitalizing on our experience with the ""Africa-Canada Artificial Intelligence and Data Innovation Consortium"" (ACADIC) Project in the Global South, and focusing on the ethical and regulatory challenges we had to face. ""Clinical public health"" can be defined as an interdisciplinary field, at the intersection of clinical medicine and public health, whilst ""clinical global health"" is the practice of clinical public health with a special focus on health issue management in resource-limited settings and contexts, including the Global South. As such, clinical public and global health represent vital approaches, instrumental in (i) applying a community/population perspective to clinical practice as well as a clinical lens to community/population health, (ii) identifying health needs both at the individual and community/population levels, (iii) systematically addressing the determinants of health, including the social and structural ones, (iv) reaching the goals of population's health and well-being, especially of socially vulnerable, underserved communities, (v) better coordinating and integrating the delivery of healthcare provisions, (vi) strengthening health promotion, health protection, and health equity, and (vii) closing gender inequality and other (ethnic and socio-economic) disparities and gaps. Clinical public and global health are called to respond to the more pressing healthcare needs and challenges of our contemporary society, for which AI and BDA can help unlock new options and perspectives. In the aftermath of the still ongoing COVID-19 pandemic, the future trend of AI and BDA in the healthcare field will be devoted to building a more healthy, resilient society, able to face several challenges arising from globally networked hyper-risks, including ageing, multimorbidity, chronic disease accumulation, and climate change.","In the present paper, we explored how AI and BDA can help address clinical public and global health needs in the Global South, leveraging and capitalizing on our experience with the ACADIC Project in the Global South, and focusing on the ethical and regulatory challenges we had to face.

Whilst clinical public health is at the intersection of clinical medicine and public health, clinical global health is the practice of clinical public health in the Global South. As such, clinical public and global health represent vital approaches, instrumental in combining a community/population perspective with clinical practice, identifying health needs, systematically addressing the determinants of health, better coordinating and integrating the delivery of healthcare provisions, reaching the goals of population’s health and well-being, strengthening health promotion, health protection, and health equity, and closing disparities and gaps. Clinical public and global health are called to respond to the more pressing healthcare needs and challenges of our contemporary society, for which AI and BDA can help unlock new options and perspectives. In the aftermath of the still ongoing COVID-19 pandemic, the future trend of AI and BDA in the healthcare field will be devoted to building a more healthy, resilient society, able to face several challenges arising from globally networked hyper-risks, including aging, multimorbidity, chronic disease accumulation, and climate change.

Based on the existing literature and our experience, we identified the following lessons (i) strengthening local research and healthcare capacity in the Global South, (ii) strengthening local epidemic/pandemic management planning, including the establishment and development of networks with simulator users in the Global South, (iii) a need for locally informed models in the Global South, (iv) a need for flexible modeling frameworks to respond rapidly to future emergencies in the Global South, (v) limitations and shortcomings of modeling should be communicated clearly and consistently to end users in the Global South, (vi) systematically monitoring the use and implementation of models in the process of decision-making in the Global South, (vii) a need for strengthening AI- and Big Data-related funding in the Global South, and (viii) a need for strengthening AI- and BDA-modeling capacity in the Global South.
This is in line with other experiences of authors from the Global South, which reaffirm that equity in health is paramount, being “the core value of health for all” [99], as advocated by the Alma-Ata declaration of 1978. This goal is even more ambitious in a world and society where inequities are prevailing and poverty is increasingly widening, with the emergence/re-emergence of new illnesses [100] and the accumulation of chronic diseases [101], which put a significant strain on already weakened healthcare systems [99].
As such, carrying out health research is vital, even though, in some resource-limited areas and environments, it may be particularly complex and difficult, with a limited capacity to undertake and translate research into practice [102]. There, it is fundamental to strengthen and develop sustainable health research capacity in the Global South [102]. Key enablers were identified in (i) ensuring adequate, vigorous funding, (ii) building effective stewardship and establishing equitable and sustainable research collaborations and partnerships, (iii) mentoring and training next-generation researchers, and (iv) effectively linking research-related outcomes to policies and practices [102].
Researchers and scholars from the Global South are terribly under-represented in research [103,104,105,106,107,108,109,110], project leadership and management, authorship, and funding allocations [106], paradoxically even in research concerning the Global South itself [103,104]. Therefore, decolonizing global health research, partnerships, and outreach could mean counteract and mitigate against global health iniquities [105,106,107,108,109]. This “paradigm shift” [106] implies (i) redefining equitable and fair models and best practices of collaboration and (ii) implementing mechanisms to monitor and track progress [106], as well as (iii) recognizing “non-western forms of knowledge and authority”, acknowledging discrimination and disrupting “colonial structures and legacies that influence access to healthcare” [108]."
Artificial intelligence in gastroenterology and hepatology: how to advance clinical practice while ensuring health equity,"Artificial intelligence (AI) and machine learning (ML) systems are increasingly used in medicine to improve clinical decision-making and healthcare delivery. In gastroenterology and hepatology, studies have explored a myriad of opportunities for AI/ML applications which are already making the transition to bedside. Despite these advances, there is a risk that biases and health inequities can be introduced or exacerbated by these technologies. If unrecognised, these technologies could generate or worsen systematic racial, ethnic and sex disparities when deployed on a large scale. There are several mechanisms through which AI/ML could contribute to health inequities in gastroenterology and hepatology, including diagnosis of oesophageal cancer, management of inflammatory bowel disease (IBD), liver transplantation, colorectal cancer screening and many others. This review adapts a framework for ethical AI/ML development and application to gastroenterology and hepatology such that clinical practice is advanced while minimising bias and optimising health equity.","We describe 5 themes to illustrate how AI/ML can lead to inequities in gastroenterology/hepatology, examples of the impact on health equity and several potential actionable solutions to ensure equity in AI. By the year 2045, White individuals will comprise less than 50% of the U.S. population, thus this work is critical as AI/ML becomes more common globally and the U.S becomes more diverse.95
Our primary limitation was the inability to measure or quantify inequities in each clinical example provided. Though each example provided relates directly to a major theme of mechanism of inequities in AI/ML, the degree to which each specific example led to bias cannot be directly measured. In addition, we did not have access to all of the model information used to develop the algorithms discussed nor to robust cost information that could enable a review of cost implications of current AI approaches. This fact highlights the importance of transparency to enable researchers’ access to data and inputs included in each algorithm to advance equity. Lastly, the examples provided in this paper are not an exhaustive list but rather focus on strong and relevant illustrations of how prediction models and AI/ML algorithms in gastroenterology and hepatology can lead to biased systems and inequitable health outcomes.
There are several key strengths of this paper. Firstly, we provide clear and actionable solutions to address health equity in machine learned that can be utilized by researchers and clinicians alike. Secondly, we provide concise themes that illustrate how AI/ML can lead to health inequity in gastroenterology matched to specific examples. Our overarching goal is to increase attention to an important potential downside of AI as its use become more prevalent and pervasive in the fields of gastroenterology and hepatology.
Here, we adapt a framework to consider equity in AI/ML algorithms used in gastroenterology/hepatology and a platform for discussion around an increasingly relevant topic. In other fields of medicine, we have started to reassess prediction models and algorithms and incorporate a health equity lens. The field of gastroenterology and hepatology has already taken a leading role in clinical applications for AI in medicine, and it is therefore especially important that, as a field, we take a leading role in ensuring that equity considerations are emphasized. This framework will help gastroenterology/hepatology researchers and clinicians prioritize equity in AI/ML development, implementation, and evaluation so that we can give every patient an opportunity to benefit from the technologic advances that the future brings."
Multimorbidity and equity in health,"Two major challenges are at play in the design of current and future health care delivery: the growing diversity of patient populations and the increasing burden of multiple long-term conditions. The growing prevalence of multiple co-occurring conditions signifies a greater burden for patients and a resultant increasing need for health care resources [1,2]. Yet, health care systems are ill suited for the complex, frequently interacting needs of patients with multiple chronic conditions [3]. Guidelines and incentives are aligned in a way that acknowledge high quality treatment of single conditions [4,5] without recognizing the impact of multiple conditions on the commissioning of care and on patients’ self management capabilities [6-9]. For individuals from diverse socioeconomic and cultural backgrounds, these challenges are even greater, as multiple co-occurring life and social circumstances interact with their health and health related needs.","This special series aims to contribute to the growing body of knowledge on MM and equity in diverse populations worldwide. The inaugural issue, which will be followed by ongoing publications, presents 11 papers from various (World Health Organization) regions - Eastern Mediterranean countries [36] America [37,38], Africa, [39,40], Europe [41-44], and the Western Pacific [45].
In a review of MM and equity in Eastern Mediterranean countries, Boutayeb and colleagues [36] summarize findings from 26 studies, showing that female gender, low income, low level of education, and unemployment are factors associated with MM. Given the rising prevalence of non-communicable diseases (NCDs) in the region [46], there is also increased awareness that NCDs do not occur in isolation, and studies on the prevalence and risks associated with multiple NCDs are emerging. The main outstanding areas of contribution that authors point to include: expanding geographic scope (particularly, studies from North Africa), types of conditions included in MM measurements and types of socioeconomic and ethnic/cultural risk factors examined. Additionally, it should be noted that the study of MM in low and middle income countries in general, and specifically in Eastern Mediterranean countries, is still mainly focused on co-morbidity, i.e., the co-occurrence of disease or conditions in the context of an index condition of interest [10], rather than on MM. To better understand patterns of MM, their causes and consequences, there is the need to expand this conceptualization toward acknowledging that the overall burden of co-occurring and interacting conditions is not necessarily associated with a single disease in various population groups.
The two studies from South Africa [39,40] further emphasize the relative robustness of the relationship between deprivation and MM. Each used a different survey with distinctive measures of MM and deprivation, and both concluded that income is inversely associated with MM. It is important to note, however, that comprehensive measurements (i.e. those that combine reports on both illness and disability) [40] may provide a more informative depiction of the phenomenon, thereby guiding further research and policy.
In a study from Taiwan, Kuo and colleagues [45] show that lower income individuals with MM incur higher total costs. Several possible explanations for this relationship are discussed by the authors. Future studies should examine this type of relationship with a more comprehensive MM measure that accounts for all types of conditions and their interactions encompassing more than the additive effect of specific conditions [47]. A more comprehensive MM measure might be able to better capture the multitude of health and health related needs which are more prevalent among disadvantaged populations, and which are potentially driving up total expenditures in this multimorbid population group.
That MM encompasses more than just chronic conditions is exemplified in the study by Reis-Santos and colleagues [37], who characterized MM in subjects with tuberculosis (TB) registered in the National Notification System in Brazil. This study, however, found that MM in persons with TB was not associated with deprivation indices such as education level or prior Institutionalization (e.g., imprisonment).
Three studies in this issue address another area of needed research: MM and equity in childhood and adolescence [38,41,42]. Cornish and colleagues [42] have demonstrated a link between levels of maternal education and MM among children, but not in adolescents. This study used several types of MM measures, including the Adjusted Clinical Groups® classification of morbidity burden, taking into account all types of conditions (chronic and acute) and their interactions [48]. The inconsistency of results across the various types of measures suggests that additional work is needed in development of both the conceptual framework as well as methodology of studies assessing MM, especially in children and youth [49]. Chao and colleagues [41] present a complimentary approach to the study of MM, in which they conducted a survey on the prevalence of multiple mental health, and behavioral difficulties (i.e. the consumption of alcohol, tobacco, cannabis, and hard drugs, obesity, depressive symptoms, suicide attempts, involvement in violence, and low school performance) in middle schools in north-eastern France. Although the representativeness of this sample should be further established, this study contributes new insights about the co-occurrence of a wide range of physical and mental health conditions, coupled with socio-economic deprivation in adolescents.
The third paper that studies children uniquely examines MM and the association with residential movement patterns and changes in neighborhood income of children with mild to severe chronic diseases compared to healthy children [38]. This study shows that young children with chronic conditions, particularly those born in low income neighborhoods, are more likely to move residence than other healthy young children. The reasons for--as well as outcomes of--such mobilization requires further research.
Another important contribution to the study of MM and equity is presented in a paper by Lawson and colleagues [43], which studied the association between MM and Health Related Quality of Life (HRQoL) and its variation by socioeconomic deprivation. This study found that MM has a substantial negative impact on HRQoL which is most severe in areas of deprivation, especially among younger adults. Given that young adults are prone to the negative effects of deprivation on MM, this constitutes an essential area for further investigation, especially as most current research on MM is still focused on older adults [49-52].
Another distinct area of inquiry in the field of MM is the study of incidence rather than prevalence of co-occurring conditions. While most research focuses on describing the prevalence of MM, several studies also point to the relationship between deprivation and onset of MM [53,54]. Demirchyan and colleagues add to this limited body of knowledge in a study on the determinants of incident MM in a cohort from the 1988 Armenian earthquake survivors [44]. This prospective study was able to show that the relationship between age and MM was largely mediated by the number of stressful life events, suggesting that experiencing stressful events during the lifespan might be a more important factor for incident MM than the lifespan itself. This study also suggested that BMI is an independent long-term predictor of incident MM, with higher BMI values contributing to MM two decades later--a finding, that as the authors justly state, deserves attention and further study.
Finally, this inaugural issue of the thematic series on equity and MM also includes a commentary [55] on equity in the selection of MM patients for inclusion in targeted care-management interventions. Multimorbid patient selection is complicated due to the lack of clear criteria - unlike disease management programs for which patients with a specific condition are identified. This ambiguity can potentially result in inequitable selection, as biases in selection may differentially affect patients from disadvantaged population groups. Acknowledging these biases is an important first step for preventing further widening of gaps in the care of multimorbid individuals."
Implementation of prognostic machine learning algorithms in paediatric chronic respiratory conditions: a scoping review,"Machine learning (ML) holds great potential for predicting clinical outcomes in heterogeneous chronic respiratory diseases (CRD) affecting children, where timely individualised treatments offer opportunities for health optimisation. This paper identifies rate-limiting steps in ML prediction model development that impair clinical translation and discusses regulatory, clinical and ethical considerations for ML implementation. A scoping review of ML prediction models in paediatric CRDs was undertaken using the PRISMA extension scoping review guidelines. From 1209 results, 25 articles published between 2013 and 2021 were evaluated for features of a good clinical prediction model using the Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) guidelines.Most of the studies were in asthma (80%), with few in cystic fibrosis (12%), bronchiolitis (4%) and childhood wheeze (4%). There were inconsistencies in model reporting and studies were limited by a lack of validation, and absence of equations or code for replication. Clinician involvement during ML model development is essential and diversity, equity and inclusion should be assessed at each step of the ML pipeline to ensure algorithms do not promote or amplify health disparities among marginalised groups. As ML prediction studies become more frequent, it is important that models are rigorously developed using published guidelines and take account of regulatory frameworks which depend on model complexity, patient safety, accountability and liability.","The 25 prognostic ML studies assessed in this scoping review were overwhelmingly focused on asthma and the majority were supervised models. The studies were mainly limited by a lack of validation or prospective study, and the absence of equations or code for replication, which are major steps required for clinical implementation. Some recent studies used data from 1 to 2 decades ago, which may have limited relevance to current populations for which treatments and care have changed. Some of the models were opaque, uninterpretable models that used high numbers of predictors and did not explain the resulting predictions. This is especially important in healthcare since a clinician needs to know not only who is at risk, but also what they can do to change the outcome.
A large proportion of studies did not report on the handling of missing data, which does not provide transparency to evaluate whether sample populations are under-represented, for example, towards those who are sicker and have more data. Smaller datasets were typically derived from research studies, where there is greater control over the variables collected or the inclusion criteria for the study. However, ML methods were typically developed for large datasets, and studies using national/regional databases, EHRs, or data from daily home monitoring benefit from large samples likely more representative of wider populations.
External validations are necessary to understand the generalisability of the predictions; however, only one was conducted. In the study, similar clusters of children with CF developed from data in Canada were identified in data from the UK, providing evidence for the generalisability of the model.22 Internal validations were frequent, but their performance relies heavily on the definition of the outcome. If the outcome is somewhat subjectively captured, for example, prescription of medication, the resulting predictions are biased towards the subjective. This is highlighted in the two prospective studies that identified no patient benefit despite good model performance during development.24 29 If the models are trained on data where the outcome is influenced by clinician decision, it is unsurprising that the models would not outperform a clinician. While these models may benefit areas of healthcare such as easing/increasing clinician workflow, objectively captured outcomes such as chest imaging, lung function, or physiological data may result in models with greater patient benefit.
This scoping review was limited in that the studies were not assessed with the full TRIPOD guidelines, and bias and clinical applicability were not assessed with the full Prediction model Risk Of Bias ASsessment Tool46 guidelines. A summarised reporting checklist was instead used, which investigated the articles at an overarching level rather than a granular level to identify key themes. Even without detailed assessment using the full reporting checklists, the summarised checklist revealed that studies still largely failed to report on or carry out key metrics, and thus more granular investigation at this point was not required to identify shortcomings in model reporting. Development of ML prediction models is still an unexplored area of research in paediatric CRDs other than asthma, highlighted here by a lack of studies identified in other respiratory conditions. As research into these areas continues, and as ML prediction studies in paediatric CRDs are becoming more frequent (72% published since 2018), it is important that the models are rigorously developed. A quality assessment tool for artificial intelligence-centered diagnostic studies is currently being developed, and combined with the TRIPOD guidelines for prediction studies will be useful for designing future ML prediction models with clinical implications.47"
Machine Learning Techniques for Personalised Medicine Approaches in Immune-Mediated Chronic Inflammatory Diseases: Applications and Challenges,"In the past decade, the emergence of machine learning (ML) applications has led to significant advances towards implementation of personalised medicine approaches for improved health care, due to the exceptional performance of ML models when utilising complex big data. The immune-mediated chronic inflammatory diseases are a group of complex disorders associated with dysregulated immune responses resulting in inflammation affecting various organs and systems. The heterogeneous nature of these diseases poses great challenges for tailored disease management and addressing unmet patient needs. Applying novel ML techniques to the clinical study of chronic inflammatory diseases shows promising results and great potential for precision medicine applications in clinical research and practice. In this review, we highlight the clinical applications of various ML techniques for prediction, diagnosis and prognosis of autoimmune rheumatic diseases, inflammatory bowel disease, autoimmune chronic kidney disease, and multiple sclerosis, as well as ML applications for patient stratification and treatment selection. We highlight the use of ML in drug development, including target identification, validation and drug repurposing, as well as challenges related to data interpretation and validation, and ethical concerns related to the use of artificial intelligence in clinical research.","Reproducibility and External Validation in Machine Learning
Issues with multiple testing and p-hacking has contributed largely to the reproducibility “crisis” in science. The 2016 Nature survey pointed out that more that 70% of scientists have failed to reproduce other scientists reported results (Baker, 2016). P-hacking in traditional statistics usually means that tests are done on data in an exploratory manner, if something significant is found, a hypothesis is formed based on this finding, i.e., working backwards from data to find patterns and relationships. However, the statistical tests are only valid if the hypothesis is formed first. In ML, working backwards from data to reveal patterns is exactly what is done. In the case of ML, overfitting can be considered the analogy to p-hacking. Overfitting usually means that the ML model can perfectly reproduce training data, but fails on independent data. The way to handle this by data scientists is appropriate internal and external validation of models.
To achieve the highest model performance, many clinical studies tend to avoid data splitting for model development. Resampling methods such as bootstrapping and k-fold cross-validation are economical internal validation, therefore, they are often applied to prevent model overfitting. On the other hand, external validation using an independent cohort is not often performed, potentially due to limited access to similar cohorts, despite being the most straightforward way to evaluate the generalizability of the model. Less than 10% of autoimmune studies combine cross-validation with an independent test dataset for validating model performance (Stafford et al., 2020). However, external validation remains a crucial step for model implementation in real-world clinical practice and the absence of external validation will raise several concerns for the model integrity including bias of the model, lack of reproducibility and lack of model generalizability (Ho et al., 2020). One example is the publication of GWAS studies that are required to have at least two independent data sets for validation to assure a creditable result (Oetting et al., 2017). As external validation requires data from independent sources, access to publicly available online datasets from different studies has become a suitable solution to overcome the lack of independent validation cohorts (Riley et al., 2016). These online databases provide a great opportunity to improve the research quality of ML applications in immune-mediated inflammatory diseases that are often rare conditions associated with a limited number of datasets available. They provide options for researchers to validate their models on more relevant populations, as most current external validation studies use small local datasets simply because of the better accessibility.

Model Implementation in Clinical Practice
Transforming a well-performed model into an actual clinical application associated with improvement in patient outcomes can be challenging; the term “AI Chasm” describes the discrepancy between the model development and translation of models to real-world applications (Keane and Topol, 2018). The clinical impact of potentially promising ML models requires careful evaluation before considering implementation in clinical settings. For example, a wide range of performance metrics (accuracy, AUC, precision, sensitivity, specificity etc.) (see Glossary) are applied to represent the predictive efficacy of ML models in clinical studies. However, most of the metrics do not directly affirm the clinical applicability and can be difficult to evaluate with limited interdisciplinary knowledge (Saito and Rehmsmeier, 2015; Shah et al., 2019). Another common obstacle for the clinical translatability of ML data arrives where emerging ML studies that stratify patients with novel signatures suffer from the lack of effective drugs for the newly identified targets. Furthermore, the reported predictive model needs to provide clinically meaningful advantages over traditional approaches, such as significantly outperform the existing standard statistical approach in relevant fields (Shah et al., 2019). To help address these questions, standard practice guidance is necessary. Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) guideline is an internationally accepted reporting guideline developed to improve the reliability and value of prediction models for diagnostic or prognostic purposes (Moons et al., 2015). TRIPOD-ML focuses on the standardised methodology of ML model development (Collins and Moons, 2019), which together with the interdisciplinary effort from trained experts in different clinical and technology areas of expertise, can ensure that ML applications maximise their chance to translate into precision medicine approaches associated with patient benefit.

Ethical Concerns
The upsurge of ML applications in personalised medicine has raised potential ethical concerns regarding data privacy, as a wide range of big datasets including personal information from genetics data, demographic data and medication history are stored and used in various studies. Anonymisation is the most straightforward and common way for privacy protection of medical datasets by removing personal data for de-identification purposes. However, advanced re-identification techniques were developed and used to target the vulnerability of the anonymisation system by data mining companies, and data were then exploited by health insurance companies (Tanner, 2017). Thus, more rigorous data handling methods such as data decentralisation (storing data in separate locations) and federated machine learning (training algorithm across different decentralised local data) are necessary for institutes and companies dealing with large-scale personal data (Rieke et al., 2020). From patients and the general public’s perspective, there is an innate scepticism related to the use of AI for clinical applications, especially with limited understanding about how ML and personal data are used in medical research. Face-to-face communication between specialists and patients is effective in conveying the scope of ML applications and addressing questions and concerns in terms of patient satisfaction (Mirzaei and Kashian, 2020). Public education events such as interactive Patient and Public Involvement and Engagement (PPIE) activities can inform patients about how AI and ML research can lead to better disease management and how data are handled within a secured framework. With a better understanding of ML approaches and how personal data are stored, used and protected, patients are more likely to engage with such research.
The phenomenon of ML algorithm-driven discriminating decisions has been well-observed in other areas of research using AI, such as racial discrimination in criminal charge facial recognition technology (Perkowitz, 2021) and gender discrimination in job recruitment algorithms (Yarger et al., 2019). Algorithm discrimination is not exempt in the clinical world. For example, an implemented algorithm in the US healthcare system for future health care needs prediction is heavily biased against black patients because of the lack of data on these patients (Obermeyer et al., 2019). This algorithm-intrinsic bias is inherited from existing inequality in society as black patients are generally less accessible to the healthcare system. Another study showed that the predicted hospital mortality of patients in critical care can vary by up to 20% according to their ethnic group (Chen et al., 2018). Many inflammatory diseases are independently associated with demographic variables such as age, sex and ethnicity. For example, autoimmune diseases are more frequent in the female population (Gleicher and Barad, 2007), which sometimes, for practical reasons, promotes research only within the most represented groups of patients, discriminating against the under-represented ones. Moreover, model development is highly data-driven with low tolerance to missing values in model training, which can also lead to potential bias by not capturing the real-life patient population of interest. For example, previous studies showed that vulnerable populations are less likely to attend the same clinic regularly due to limited access to healthcare, including diagnostic testing and medicines (Arpey et al., 2017; Gianfrancesco et al., 2018). Unintentionally excluding these incomplete datasets will lead to development of models that are less effective in populations with existing disadvantages. Thus, it is important for researchers and data scientists representing the diversity of the human condition to have opportunities to participate in the decision making and algorithm supervision process, assessment of the underlying biases associated with AI and ML and implementation of regulatory adjustments. This will avoid the development of discriminating decision-aiding algorithms."
Big data and machine learning algorithms for health-care delivery,"Analysis of big data by machine learning offers considerable advantages for assimilation and evaluation of large amounts of complex health-care data. However, to effectively use machine learning tools in health care, several limitations must be addressed and key issues considered, such as its clinical implementation and ethics in health-care delivery. Advantages of machine learning include flexibility and scalability compared with traditional biostatistical methods, which makes it deployable for many tasks, such as risk stratification, diagnosis and classification, and survival predictions. Another advantage of machine learning algorithms is the ability to analyse diverse data types (eg, demographic data, laboratory findings, imaging data, and doctors' free-text notes) and incorporate them into predictions for disease risk, diagnosis, prognosis, and appropriate treatments. Despite these advantages, the application of machine learning in health-care delivery also presents unique challenges that require data pre-processing, model training, and refinement of the system with respect to the actual clinical problem. Also crucial are ethical considerations, which include medico-legal implications, doctors' understanding of machine learning tools, and data privacy and security. In this Review, we discuss some of the benefits and challenges of big data and machine learning in health care.","By exploiting capabilities such as deep neural networks, machine learning presents a powerful tool for analysing large amounts of complex health-care data to improve the efficiency and cost-effectiveness of health-care delivery. When used to augment the capabilities of doctors, machine learning can perform routine, systematic tasks with high consistency, freeing up doctors’ time so that they can address clinical problems that are more complex or that require substantial human interaction. However, machine learning tools have several limitations or requirements that need to be addressed before they can be effectively deployed in health care. The algorithms require data pre-processing, training on datasets (which might be large), and iterative refinement with respect to the actual clinical problem. Adopting machine learning algorithms in clinical practice also raises several key ethical concerns. These concerns include liability in cases of medical error, doctors’ understanding of how machine learning tools produce predictions, patients’ understanding and control of how machine learning tools are used in their care, and issues of privacy, security, and control of patient data. The use of machine learning algorithms to augment rather than replace the doctor helps to increase clarity for medical liability issues. In instances of medical error, the liability still rests with the doctor as the clinical tasks either involve or are supervised by a doctor, although legal experts are still debating whether some liability should rest with the developer of the machine learning tool as well.60 When doctors override machine learningbased predictions, the law requires doctors to show that their actions are reasonable and consistent with those of a typical member of their profession.61 It could be argued that machine learning tools should be beyond such standard practice obligations, but this issue could change as these tools become more prevalent in the clinical decision-making process. Although doctors might not need to know the detailed mathematical calculations used in a machine learning algorithm, they could be educated about the types of data used in making the predictions and the relative weights assigned to each type of data (if the machine learning tool is designed to be explainable). Similar to a clinical test result, doctors could consider features such as sensitivity and specificity for predicting a particular disease risk or treatment response. Some countries are also recognising the patient’s right to understanding machine learning tools when they are used in their health care.62 Machine learning tools often use sensitive personal data to make stratified health-care recommendations. This use of personal data raises considerations of ensuring privacy and security of the data,62 transparent communication with the public about uses of their data, and protecting against the use of data or machine learning algorithms in discriminatory practices. To address some of these concerns, platforms that enable the building of multiple machine learning tools and feature linkage to a patient’s electronic health record and data governance systems offer a promising model for the future. Numerous machine learning tools can be custom developed for disease-specific indications, such as predicting breast cancer recurrences and hosting pharmacogenomics workflows for chemotherapy drugs. Machine learning tools can also be built for system-level interventions, including improving patient selection and recruitment for clinical trials, reducing patient readmission, and automated follow-up of patients for surveillance of complications. Similar to other clinical tools, these machine learning platforms will need to be evaluated as part of the clinical workflow in real-world health-care settings."
Machine Learning and Natural Language Processing in Mental Health: Systematic Review,"Background: Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining. Objective: The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice. Methods: This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed. Results: A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform.
Conclusions: Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patients' daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.","Strengths and Limitations of the Review
This study reviews ML and NLP models in the field of mental health, which has been a highly topical issue in recent years. The methodology was elaborated to screen a maximum number of specific medical studies by expanding the research to 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO). Furthermore, the characterization of the selected studies has been done very precisely in a qualitative manner to simultaneously depict the populations, methods, data sources, and technical aspects.
The primary limitation of this study is the lack of quantitative comparisons between the selected studies. It is indeed not feasible to compare highly heterogeneous studies that do not share common research patterns. In addition, the selected works were not scored on their risk of bias. Despite this shortcoming, their limitations and strengths are outlined in the individual tables in Multimedia Appendix 2.
Methodological and Technical Limitations of the Selected Studies ML and NLP methods may be considered as a new paradigm in medical research in which it becomes practical to analyze every possible, even unexpected, and innovative parameter of a topic to discern new clinical patterns. This new paradigm involves reconsidering the standard methodology, which consists of formulating a sound hypothesis, defining objectives, and collecting results to either uphold or reject the hypothesis. However, in practice, the selected studies tend to confirm clinical hypotheses based on fundamental clinical intuitions, namely language abnormalities in adults with ASD [42].
Other methodological limitations and potential bias sources have been noted. As stated in the Results section, one of the 3 main population categories is social network or chat users [40,41,66,77,79], whose members are predominantly young. Owing to this, Coppersmith et al [76,77] cautioned that these results may not be generalizable to other populations [77,106]. In addition, when Chary et al [66] focused on Lycaeum users and Coppersmith et al [76] mentioned participants from a company, the lack of precise information on the participants of a cohort was obvious. An exception to this is the group of OurDataHelps.org users [77] who volunteered to participate in scientific research and filled out a questionnaire to provide information about themselves. Even when participants volunteer to provide personal information, there is a high likelihood that personality bias plays a role, especially in studies on suicide and depression.
Similarly, studies rarely consider cultural or ethnic differences within a sample [80]. For example, in a study on violent behavior, researchers should acknowledge that spanking children for discipline purposes is considered inappropriate in some cultures but appropriate in others. In some cases, language-specific features can improve the performance of NLP methods. For example, in the case of Takano et al [62], the distribution of morphemes is used to distinguish between specific and nonspecific memories in the Autobiographical Memory Test. As shown in the paper, among the most important distinctive factors are grammatical particles that are specific to the Japanese language, such as ?/? (past tense), ?? (negation), ? (topic marker), and ? (place or method). In languages with different structures, the same method may be less efficient and other indicators may need to be investigated.
Is There an Advantage in Using ML and NLP for Mental Health Clinical Practice? The hallmark ML principle is to simultaneously analyze large quantities of data; however, this sometimes leads researchers to the implicit assumption that the more data they input, the more accurate will be the results. ML and NL allow the analysis of large amounts of data and the comparison of broad groups and patients. For example, Roysden et al [56] screened administrative data and EHRs from a population of 12,759 patients; Maguen et al [63] compared over 8,168,330 clinical notes collected over 15 years; and Yazdavar et al [79] analyzed posts authored by 4000 Twitter users. At the same time, even though thousands of papers have been published using medical data, very few have made meaningful contributions to clinical practices [111].
Twitter and other social networks, with almost 3 billion users globally, have become significant sources of information for medical use and research [112]. Moreover, the analysis of social media–based platforms can generate valuable details about people’s mental health and social or professional interactions. The alteration of daily habits is one of the core criteria for the diagnosis of a mental health disorder (in general, criterion B of DSM-5). A recent study by Fagherazzi and Ravaud [113] illustrates the idea that AI can be implemented in the so-called digitosome (data generated online and by digital technologies) that constitutes a powerful agent for detecting new digital markers and risk factors in medicine. By analyzing a global cohort of more than 85,000 tweets per week authored by people with diabetes, they were able to discuss different illness-related stress patterns of patients with type 1 or type 2 diabetes. By analyzing tweets, Mowery et al [106] found that there may be alternative ways in which people express depression. These findings indicate that there may be new ways for people to express mental illness.
From this perspective, different expressions of psychological distress (whether people are addressing health care professionals, relatives, or digital friend networks) could be accessible and useful to care providers. ML and NLP may be valuable in psychiatry for identifying people with clinical risks for depression, suicide attempts, anxiety, or even psychosis based on digital data or clinical notes.
Ethical Reflections AI in psychiatry and more broadly in medicine raises ethical issues and requires prudence in its application. As mentioned earlier, ML and NLP techniques have valuable advantages in psychiatry for analyzing large amounts of data with high diagnostic and prognostic validity. These tools, which have been groundbreaking in medicine and psychiatry, should receive more attention for their promising results with regard to clinical practice and medical research. In addition, recent studies suggest that people are becoming more comfortable when speaking with a machine compared with a clinician: Lucas et al [51] state that in a clinical trial, people who (believed they) were interacting with a computer disclosed information more openly than people who thought that an individual was controlling the computer. Perhaps the machine is viewed as being more objective than a human and therefore reduces the fear of judgment from a practitioner. The introduction of a computer in medical practice as a new type of clinician leads to a profound change in the physician-patient relationship and promotes the idea of having a new clinical model involving a third party. The relationship is crucial to psychiatric clinical practice, and the use of data processing should be discussed. Sassolas [114] questioned this technological psychiatry as a practice that is likely to avoid what he called the “psychic privacy proximity.” Technological psychiatry could generate an operative encounter whose unique purpose is to normalize the patient’s symptoms and reduce the fear of disclosure.
In addition to improved relationships, the application of ML and NLP in psychiatry should be done with special precautions to avoid clinical abuse. This review includes 2 studies about the prediction of psychosis in patients at high risk of this disease. One even introduced a model of ML+NLP that had a 100% accuracy in predicting psychosis among the latter patient sample [39], which was better than a simple clinical evaluation. Nevertheless, these results should be treated with caution because of the small sample size and the lack of detail on the statistical techniques used. The risk of overfitting needs to be considered. Although further research should be continued to improve technical issues, ethics should be taken into account. Martinez-Martin et al [115] questioned whether it is ethical to use prognostic estimates from ML to treat psychosis, as it is not known whether variables are present in the local context (such as differences in psychiatric practice and social support) that would affect the model’s validity. Moreover, when programming an ML algorithm, investigators can choose to strengthen the criteria they esteem to be more relevant, such as clinical criteria instead of socioeconomic factors. This could result in loss of opportunity for some patients when the automated machine analysis gives the illusion of greater objectivity. These adjustments should be done to respect the principle of equity.
In the case of predicting psychosis, the study involved only patients who consented to both psychiatric care and the completion of interviews. This was not the case in studies on suicide prevention, where researchers tracked information on patients by using social media. This could be considered a violation of confidentiality. Should information from social media be used to identify symptoms? Applying AI in this context raises significant ethical concerns, particularly in balancing beneficence and respecting confidentiality [53]. ML and NLP can help identify people at clinical risk for depression or suicidal ideation, who most likely do not have access to mental health providers and/or a primary care doctor [61]; however, this reduces confidentiality protection and can lead to increased vulnerability in certain populations [21]. To obtain informed consent from patients and protect their privacy, McKernan et al [53] proposed some recommendations: patients should be informed that (1) algorithms can be imperfect or wrong; (2) algorithm data should be considered highly sensitive or confidential; (3) algorithm data might recommend actions that are not immediately apparent; and (4) algorithms might prompt unnecessary intervention from the provider. Therefore, psychiatrists should be trained in ML and NLP techniques and be able to explain to patients their main characteristics and why they may require certain recommendations. This last point underlines the need for an explainable AI that goes further than black box methods.
Finally, ML and NLP should not lead to disempowerment of psychiatrists or replace the clinician-patient pair. On the contrary, the combination of ML with NLP should be considered as a tool to support clinical practice and medical research."
Ensuring Fairness in Machine Learning to Advance Health Equity,"Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency. Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources. This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity. For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation. The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle. Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.","In the Table, we present recommendations for how to incorporate fairness into machine learning. Researchers should consider how prior health care disparities may affect the design and data of a model. For example, if advanced-stage melanoma is diagnosed more frequently in patients with dark skin than in other groups, might a skin cancer detection model fail to detect early-stage disease in patients with dark skin (47, 48)? During training and evaluation, researchers should measure any deviations from equal accuracy and equal allocation, and consider mitigating them by using techniques during training (38–40) or by postprocessing a trained model (30, 36, 41). Before deployment, launch reviews should formally assess model performance and allocation of resources across groups. The reviews should determine whether a model promotes equal outcomes, broadly defined as “the patient’s care experience, functional status, and quality of life, as well as… personalization of care and resource stewardship” (49). If a model is deployed, the performance of the model and outcome measurements should be monitored, possibly through formal trial design (such as stepped-wedge trials [50]). Moreover, the model may be improved over time by collecting more representative or less biased data. We purposefully do not recommend the commonly discussed fairness principle of “unawareness,” which states that a model should not use the membership of the group as a feature. Complex models can infer a protected attribute even if it is not explicitly coded in a data set, such as a model identifying a patient’s self-reported sex from a retinal image even though ophthalmologists cannot (51). Moreover, removing features may lead to poorer performance for all patients.
Consideration of fairness in machine learning allows us to reexamine historical bias and proactively promote a more equitable future. We are optimistic that machine learning can substantially improve the care delivered to patients if it is thoughtfully designed and deployed. Case 2 is based on a University of Chicago Medicine example in which data scientists from the Center for Healthcare Delivery Science and Innovation collaborated with experts from the Diversity and Equity Committee to identify the equity problem and to design a local checklist for model building and deployment that advances equity.
Machine-learning fairness is not just about preventing a model from harming a protected group; it may also help focus care where it is really needed. Models could be used to provide translation services where inperson interpreters are scarce, provide medical expertise in areas with a limited number of specialists, and even improve diagnostic accuracy for rare conditions that are often misdiagnosed. By including fairness as a central consideration in how the models are designed, deployed, and evaluated, we can ensure that all patients benefit from this technology."
Artificial Intelligence in Rheumatoid Arthritis: Current Status and Future Perspectives: A State-of-the-Art Review,"Investigation of the potential applications of artificial intelligence (AI), including machine learning (ML) and deep learning (DL) techniques, is an exponentially growing field in medicine and healthcare. These methods can be critical in providing high-quality care to patients with chronic rheumatological diseases lacking an optimal treatment, like rheumatoid arthritis (RA), which is the second most prevalent autoimmune disease. Herein, following reviewing the basic concepts of AI, we summarize the advances in its applications in RA clinical practice and research. We provide directions for future investigations in this field after reviewing the current knowledge gaps and technical and ethical challenges in applying AI. Automated models have been largely used to improve RA diagnosis since the early 2000s, and they have used a wide variety of techniques, e.g., support vector machine, random forest, and artificial neural networks. AI algorithms can facilitate screening and identification of susceptible groups, diagnosis using omics, imaging, clinical, and sensor data, patient detection within electronic health record (EHR), i.e., phenotyping, treatment response assessment, monitoring disease course, determining prognosis, novel drug discovery, and enhancing basic science research. They can also aid in risk assessment for incidence of comorbidities, e.g., cardiovascular diseases, in patients with RA. However, the proposed models may vary significantly in their performance and reliability. Despite the promising results achieved by AI models in enhancing early diagnosis and management of patients with RA, they are not fully ready to be incorporated into clinical practice. Future investigations are required to ensure development of reliable and generalizable algorithms while they carefully look for any potential source of bias or misconduct. We showed that a growing body of evidence supports the potential role of AI in revolutionizing screening, diagnosis, and management of patients with RA. However, multiple obstacles hinder clinical applications of AI models. Incorporating the machine and/or deep learning algorithms into real-world settings would be a key step in the progress of AI in medicine.","This comprehensive updated study reviewed published investigations incorporating AI, including ML and DL related to RA, the second most prevalent autoimmune disease. Artificial intelligence models are used to assess RA development risk, diagnose RA using omics, imaging, clinical, and sensor data, detect RA patients within EHR, predict treatment response, monitor disease course, determine prognosis, discover novel drugs, and enhance basic science research (Fig. 3). We showed that a growing body of evidence supports the potential role of AI in revolutionizing screening, diagnosis, and management of patients with RA. However, the proposed models may vary significantly in their performance and reliability. Notably, since every decision made in the healthcare setting may have dire and irreversible consequences, considering the limitations of AI and the challenges of its implementation in healthcare is immensely important.In 2020, Stafford and colleagues systematically reviewed the available literature on AI applications in autoimmune diseases [113]. After MS, the RA had the highest number of manuscripts dedicated to itself (41 and 32, respectively), followed by inflammatory bowel syndrome (30) and type 1 diabetes (17). Although less in number, RA studies investigated more types of outcomes than MS, utilized more data sources and AI methods, and had a higher median sample size (338 versus 99). In fact, RA had the widest range of input data sources among all autoimmune diseases, indicating the vast potential of AI application in the field. Furthermore, AI-based precision medicine approaches could especially be effective in RA due to the diversity in treatment options and disease phenotypes.

Challenges and Limitations of Implementing AI
Multiple technical challenges hinder applying AI models in patient care. The need for large and accurately labeled data is a major issue in training supervised models. Importantly, small training datasets can result in over-fitted models. Creating large and high-quality open-access databases can aid in tackling this challenge. The presence of such datasets also facilitates performance comparison between different models. The variability of test datasets in various studies does not usually allow for making accurate comparisons [173, 174]. The osteoarthritis initiative study is an example of such datasets, which has been used to test and train dozens of AI models to improve diagnosis and prediction of pain progression and outcome in osteoarthritis [175–177].
Moreover, the clinical applicability of AI models cannot necessarily be represented by the accuracy of the model. In many cases, the accuracy measures reported in a scientific paper may represent the performance of the model in a small dataset from a specific population instead of providing generalizable results to other populations [178]. The variation between the input datasets is a limiting factor in the clinical implementation of AI models [179]. Datasets obtained from different healthcare environments may vary in data acquisition method, coding, and patient population. As a result, the model might perform differently when applied to datasets different from the training input. External validation can show the effect of input data variation on the performance of the model. However, in most of the studies included in this review (approximately 70%), validation using an independent external dataset was not performed.
The AI models are technically prone to several other challenges as well. These models use any signal that helps them achieve the highest performance. However, these signals may include unknown confounders, incorporation of which in the model may damage the generalizability of the model. For instance, a model designed to detect hip fractures used confounding features, including the scanner model and ""priority"" marks on scans, to classify the input data [180]. Moreover, data manipulation (adversarial attack) can have damaging effects on the performance of the AI model. Adversarial examples are inputs with small changes made to fool the model intentionally [181, 182].
The retrospective study design in most investigations in this field can also limit the real-world application of AI models. While historically labeled data are the most commonly used resources for training and testing AI models, the true additional value of AI algorithms in the diagnosis and management of patients can be best captured by trials with a prospective design. Nevertheless, only a few prospective studies have been conducted on the real-world applications of AI in the medical field [183], and research related to RA is not an exemption. As an example of prospective trials, a multi-center randomized controlled trial was performed to compare the accuracy of an AI algorithm with senior consultants in diagnosing childhood cataracts and choosing optimal treatment options [184].
In addition to the mentioned challenges, in many cases, particularly for neural networks, it is very difficult to convey the intuitive notions driving the conclusion of the model. These models that are too complicated for a straightforward interpretation of the factors involved in the decision making are also referred to as the ""black box"". The opaque rationale behind decisions made by the model can cause ethical and social challenges. Such models may fail in engendering user trust as transparency is a fundamental factor in gaining credence. Additionally, not understanding the rationale behind the decisions and the potential sources of error may increase the chances of inaccuracy in the decisions made by the model, especially in new datasets obtained in a different setting. Notably, given that healthcare is a high-stakes field, it is critical to minimize the margin of error as much as possible [185, 186].
Algorithmic bias is another ethical challenge raised by the use of AI. In 2019, Panch et al. defined algorithmic bias as when the application of an AI model aggravates existing inequities in society, such as racial and sexual discrimination [187]. For instance, a recent paper showed that one of the commonly used algorithms in healthcare is racially biased, considering the same risk score for White patients and Black patients while the Black patients are considerably sicker. They found that the underlying cause of this bias is that the algorithm predicts healthcare costs instead of disease severity. Due to the discrimination in access to care, as less money is spent on the care of Black patients compared to White patients, the model generates biased results [188]. In another example, under-representation of skin cancer images from patients with darker skin can result in less accurate results for patients of color as the model has not been trained on a sufficient number of observations representing these populations [173, 189].
The intention behind the development of AI algorithms should also be acknowledged as one of the potential ethical challenges of implementing AI in healthcare. Given the growing importance of quality measures, private-sector developers may be inclined to create algorithms suggesting clinical decisions that improve quality metrics without necessarily enhancing quality of care [190]. An example of this action has been observed in the car industry, where software was used to reduce emissions [191]. Additionally, AI algorithms might be designed in a way profiting their developers or buyers by suggesting certain drugs, tests, or devices to increase profit, while the clinicians using the algorithm may not be aware of such biases [190].
Future Directions: Our study shed light on eight recommendations for future investigations. Notably, these directions can be used in studies related to other autoimmune musculoskeletal disorders as well. (1) Adherence to guidelines ensuring good conduct is critical in AI studies. The Checklist for Artificial Intelligence in Medical Imaging (CLAIM) [22] and the guideline released by the National Health Service (NHS) for ""good practice for digital and data-driven health technologies"" [192] are examples of such recommendations. (2) Open communication of the complete source codes is indispensable for verifying the reproducibility of the results by testing them on external datasets. Nevertheless, among studies reviewed in this paper, only a few provided open-access codes [97–99, 102, 103, 133, 140, 157, 158]. (3) It is vital that AI studies conduct external validation as it is a key component in assessing performance of a model in the real-world setting. However, among studies included in this review, almost half of the studies did not have an independent external dataset to validate the model. (4) As an AI model can be only as good as the data used to train it, future investigations need to ensure using high-quality data in large quantities. This can be achieved by creating large-scale multimodal datasets containing data on demographic, clinical, laboratory, genomic, imaging, and lifestyle features of the patients. (5) Future studies require consideration of the potential risk of algorithm bias during model development, and they should include sufficient data points representing minorities to reduce the risk of bias. (6) AI algorithms can be further used to assess extra-articular involvement, such as skin and ocular manifestations, in patients with RA. (7) Furthermore, currently, most investigations have compared the performance of AI algorithms with human experts. However, evaluating the performance of the collaboration of AI algorithms and human experts versus human experts alone would provide more realistic and applicable results [174]. (8) Lastly, real-world, and wide application of AI algorithms would heavily rely on design of prospective trials, ideally multi-center and randomized, assessing the performance of these models. Of note, our study paved the way for future reviews focusing on applications of AI in other high-burden autoimmune and inflammatory rheumatological and musculoskeletal diseases, such as MS and systemic lupus erythematosus."
"Predicting 30-Day Readmission Risk for Patients With Chronic Obstructive Pulmonary Disease Through a Federated Machine Learning Architecture on Findable, Accessible, Interoperable, and Reusable (FAIR) Data: Development and Validation Study","Background: Owing to the nature of health data, their sharing and reuse for research are limited by legal, technical, and ethical implications. In this sense, to address that challenge and facilitate and promote the discovery of scientific knowledge, the Findable, Accessible, Interoperable, and Reusable (FAIR) principles help organizations to share research data in a secure, appropriate, and useful way for other researchers.
Objective: The objective of this study was the FAIRification of existing health research data sets and applying a federated machine learning architecture on top of the FAIRified data sets of different health research performing organizations. The entire FAIR4Health solution was validated through the assessment of a federated model for real-time prediction of 30-day readmission risk in patients with chronic obstructive pulmonary disease (COPD).
Methods: The application of the FAIR principles on health research data sets in 3 different health care settings enabled a retrospective multicenter study for the development of specific federated machine learning models for the early prediction of 30-day readmission risk in patients with COPD. This predictive model was generated upon the FAIR4Health platform. Finally, an observational prospective study with 30 days follow-up was conducted in 2 health care centers from different countries. The same inclusion and exclusion criteria were used in both retrospective and prospective studies.
Results: Clinical validation was demonstrated through the implementation of federated machine learning models on top of the FAIRified data sets from different health research performing organizations. The federated model for predicting the 30-day hospital readmission risk was trained using retrospective data from 4.944 patients with COPD. The assessment of the predictive model was performed using the data of 100 recruited (22 from Spain and 78 from Serbia) out of 2070 observed (records viewed) patients during the observational prospective study, which was executed from April 2021 to September 2021. Significant accuracy (0.98) and precision (0.25) of the predictive model generated upon the FAIR4Health platform were observed. Therefore, the generated prediction of 30-day readmission risk was confirmed in 87% (87/100) of cases.
Conclusions: Implementing a FAIR data policy in health research performing organizations to facilitate data sharing and reuse is relevant and needed, following the discovery, access, integration, and analysis of health research data. The FAIR4Health project proposes a technological solution in the health domain to facilitate alignment with the FAIR principles.","Principal Findings
The application of the FAIR principles in health research data sets of health research performing organizations from different countries allowed the federated data analysis to accelerate the discovery of scientific outputs. Therefore, the analysis of legal, technical, and ethical requirements of health research data were addressed during data FAIRification. Furthermore, a clinical decision support model for predicting 30-day readmission risk in patients with COPD at discharge based on the risk factors uncovered previously, using data mining approaches, was implemented, deployed, and validated. Finally, through a multicenter study in which the rate of readmission of patients with COPD within 30 days after hospital discharge was analyzed, clinical partners could reach use case objectives and obtain an early 30-day hospital readmission risk predictive model. Further details of the FAIR4Health pathfinder case studies can be found in the FAIR4Health public report on the demonstrators’ performance [43]. It is important to highlight that the FAIR4Health solution was implemented following a practical extensibility capacity, so that other research questions can be covered using the solution without the need to perform adaptations. Furthermore, to improve the reusability capacity of the study, using both the open-source code and the generated metadata freely available in GitHub [44], the study can be reproduced.

Limitations
First, significant cross-cutting data-related challenges were addressed during data collection. Data extraction from EHRs and other types of health care sources aligning this extraction with a FAIR4Health common data model was not trivial and required a lot of conceptual and technical efforts because of (1) the complexity of the raw data (the sources of EHRs are commonly very complex including the information in several tables in the source databases), (2) free text used in some fields in the raw data sources, and (3) differences between the types of raw data sources. To address the complexity of the raw data, each health research organization from different countries that participated in the data extraction involved colleagues who were experts in each source data model. To handle the information in free text fields, natural language processing techniques were assessed. Finally, in some cases, manual natural language processing to extract structured information from unstructured information was performed. To manage the differences between the nature of the raw data sources, each raw data set was analyzed in depth in a collaborative effort between each clinical partner and the technical partners to reach the required configuration in the FAIR4Health solution, achieving the FAIRification of all raw data and finally achieving the PPDDM models’ generation using all sources.
Second, concerning the predictive model generated in this study, it can be stated that it is possible to generate more efficient prediction parameters (with better accuracy, precision, and recall values) if the distribution of the readmission variable in the data sets is better adjusted. The readmission variable, which was the dependent variable, was not balanced in the data sets of the retrospective studies (data sets used to generate the predictive model for this prospective study), which resulted in the generated results being good but not perfect as desired. For more effective models, in the future, a better adjustment of the distribution of the readmission variable using data sets with more patients will be addressed to boost the application of predictive models in clinical practice. Most studies of predictive models based on machine learning show poor methodological quality and are at a high risk of bias. The small study size, poor management of missing data, and failure to address overfitting are factors that contribute to the risk of bias [45].
In contrast, it is crucial to add that this study was carried out while these 2 health care organizations were experiencing the consequences of the COVID-19 pandemic, and clinical researchers had to make significant efforts to properly conclude the prospective study:
IPBV as a health care institution was included in the national COVID-19 system of health care institutions caring for COVID-19 positive patients with severe clinical difficulties. Owing to this reorganization of the Serbian health care system, the likelihood of hospitalization of patients with COPD has been reduced since March 2020. Many of the researchers responsible for patient recruitment in the prospective study were engaged in COVID-19 departments, and the remaining researchers were overworked during the study period.
On the side of SAS, this health care institution was involved in the care of patients with suspicion of COVID-19 and COVID-19–positive patients with severe clinical difficulties. All health professionals in SAS had a higher workload in health care. In fact, different clinical researchers participating in this observational study were transferred during the project to the COVID-19 Emergency Hospital in Seville (Spain), relieving each other, with an essential health care priority and looking after patients who did not meet the inclusion criteria of this study and could not be recruited. The clinical researchers identified a low use of health care services (both urgencies and consultancies) by patients with COPD; presumably, the patients waited for more severe symptoms to go to the health care centers because of the fear of having contact with COVID-19–positive patients. In addition, hospitalizations of patients with COPD were restricted, similar to what has happened in other pathologies, to avoid patient flow through health care centers.
Next Steps
Considering the final version of the FAIR4Health solution and the main outcomes of this study, some future advances can be taken into account:
Both the FAIRification tools and the FAIR4Health platform were validated using the FAIR4Health common data model. The solution has been designed and developed by considering the extensive capacity of other data models, so it is appropriate to continue the validation and testing with other data models in future clinical validations.
The whole FAIR4Health solution covers alignment with relevant standards: HL7 FHIR, International Classification of Diseases, SNOMED Clinical Terms, Logical Observation Identifiers Names and Codes, and the Anatomical Therapeutic Chemical classification system. Other standards such as other HL7 standards, epidemiological standards, and W3C standards could be considered to be integrated if viable.
The FAIR4Health platform was validated using the following machine learning algorithms: frequent pattern growth, support vector machine, logistic regression, decision trees, random forest, and gradient-boosted trees. Deep learning algorithms such as neural networks can be considered in future studies to improve the capabilities of the FAIR4Health platform.
From a scientific point of view, some researchers of the FAIR4Health Consortium contribute to the application of the FAIR principles in the health research field, being involved in international working groups part of the European Open Science Cloud, the European Federation for Medical Informatics, the Research Data Alliance, the GO FAIR initiative, and HL7 International.

Conclusions
Despite the limitations mentioned above, the objective of this study was achieved: to validate the FAIR4Health solution through the assessment of a federated model that was generated by applying a federated machine learning architecture on top of the FAIRified data sets of different health research performing organizations for real-time prediction of 30-day readmission risk in patients with COPD.
The clinical, technical, and functional validation of the FAIR4Health solution was achieved through (1) the application of FAIR principles through the FAIR4Health FAIRification tools in health research data sets of different health research performing organizations and FAIRifying data from 4.944 patients with COPD; (2) development and use of federated machine learning architecture on top of the FAIRified data sets; and (3) clinical, technical, and functional development and assessment of a federated model for predicting 30-day readmission risk in patients with COPD, with an accuracy of 0.98, a precision of 0.25, and a confirmed prediction in 87% (87/100) of the cases.
In the retrospective study where 3 different organizations participated with their health care (hospital, primary care, and nursing homes) and health research data sets, the federated model was generated with an accuracy of 98.6% and a precision of 25%. In the observational prospective study in which 2 health care organizations participated, 100 patients were recruited for the federated model to predict their readmission risk to the hospital within 30 days because of COPD. Therefore, the accuracy of predictions generated by the model, and hence the FAIR4Health platform, was confirmed in 87% (87/100) of the cases.
Health research performing organizations are aware of the need to implement a FAIR data policy to facilitate data sharing and reuse following the discovery, access, integration, and analysis of health research data. One obvious example would be the COVID-19 pandemic, where international cooperation allowed the rapid sequencing and epidemiological studies to be carried out, thus demonstrating the need and importance of data sharing to accelerate health research [46,47]. For this purpose, organizations are usually making efforts to align themselves with the FAIR principles. This is the real and practical consequence of the FAIR4Health project in terms of patient management and health planning: to improve health research in specific pathologies through the findability-, accessibility-, interoperability-, and reusability-enhanced features in the case of health data.
The FAIR4Health project proposes a technological solution in the health domain to facilitate the use of larger and more heterogeneous data sets, thus increasing the variability of the data and the size of the data sets. Therefore, an increase in the scope of the research will be obtained and a significant improvement in the ability to generate more accurate predictive models."
Acute on chronic liver failure: prognostic models and artificial intelligence applications,"Critically ill patients presenting with acute on chronic liver failure (ACLF) represent a particularly vulnerable population due to various considerations surrounding the syndrome definition, lack of robust prospective evaluation of outcomes, and allocation of resources such as organs for transplantation. Ninety-day mortality related to ACLF is high and patients who do leave the hospital are frequently readmitted. Artificial intelligence (AI), which encompasses various classical and modern machine learning techniques, natural language processing, and other methods of predictive, prognostic, probabilistic, and simulation modeling, has emerged as an effective tool in various areas of healthcare. These methods are now being leveraged to potentially minimize physician and provider cognitive load and impact both short-term and long-term patient outcomes. However, the enthusiasm is tempered by ethical considerations and a current lack of proven benefits. In addition to prognostic applications, AI models can likely help improve the understanding of various mechanisms of morbidity and mortality in ACLF. Their overall impact on patient-centered outcomes and countless other aspects of patient care remains unclear. In this review, we discuss various AI approaches being utilized in healthcare and discuss the recent and expected future impact of AI on patients with ACLF through prognostic modeling and AI-based approaches.","Initially labeled by John McCarthy in 1955, AI is a nondescript term used to define a broad range of techniques that allow computers to perform tasks typically thought to require human reasoning and problem-solving skills.4,31 Modern AI applications typically involve a combination of multiple techniques to achieve a desired goal. Healthcare, given its complexity, is no exception to this. What follows is an overview of a selection of AI techniques relevant to healthcare (Figure ?1) Beneath the umbrella of AI applications, machine learning (ML) encompasses various techniques including supervised ML, unsupervised ML, semisupervised learning, reinforcement learning, neural networks, and deep learning (DL). Supervised ML involves labeled input and output data which the algorithm then uses to create a mathematical model from as it “learns” relationships between input and output data. Supervised ML is a technique frequently used in healthcare. Labeled input data represent various aspects of patient-level data and labeled output data represent various desired or expected clinical endpoints. The algorithm itself interprets input data and relationships within data. Unsupervised ML allows the algorithm to extract features from unlabeled input data without output data in an exploratory nature and effectively allowing the data to “speak for itself.”4,32,33 Semisupervised ML, combining both supervised and unsupervised ML, incorporates unlabeled and labeled input and output data and the model is then iteratively trained as it effectively labels previously unlabeled data. Reinforcement ML models incorporate a system of reward and penalty with the algorithm targeting maximum reward or avoiding penalty based on desired outputs. These models learn iteratively by observing prior actions, sometimes in a simulated fashion, and subsequently, create and apply algorithms that maximize the return of reward-associated outputs.34 For simplicity’s sake, both semisupervised and reinforcement ML can also be classified as supervised ML. DL is a ML technique involving artificial neural networks (ANNs) that is designed with a number of layers between input and output data. A series of algorithms interact and form “hidden layers,” usually in a feed-forward fashion through connections that resemble neurons in a mammalian brain. The model can be further trained with more input data and errors can be addressed using a principle known as backpropagation.31,35 These models are typically developed from large and complex databases with multiple hidden neural layers and without interpretability constraints. They, therefore, provide limited transparency to the users and are described as “black-box” models. The user of “black-box” AI knows inputs and understands the outcomes of the model but how the output value was generated remains unknown due to the complexity of the algorithms created.34Natural language processing is a technique that allows extraction of text from various data sources. In healthcare, this is typically used to extract text data and patient-level information which can then be utilized as input in other AI models.4 Knowledge engineering, more specifically expert systems, consist of designated domain experts contributing known or consensus-derived knowledge to better understand complex human decisions or outline complex human physiology or pathophysiology. Bayesian or probabilistic models, particularly in the form of directed-acyclic graphs (DAGs), use variables and their conditional dependencies to create a visually accessible way of representing expert systems and causal pathways. Multiple less complex DAGs can then be combined into larger, more complex hierarchical models.36 Digital twins are simulation models, created using DAGs using both data and expert knowledge, which can receive inputs from their real-world “twin” and return outputs that can enhance insight and assist in decision-making.34 Digital twins have been developed in various engineering and geopolitical arenas but are only recently being applied to healthcare.37–39 These models offer the benefit of being able to provide actionable knowledge whether in an educational environment, in research, or at the bedside.34



"
"Using machine learning to impact on long-term clinical care: principles, challenges, and practicalities","The rise of machine learning in healthcare has significant implications for paediatrics. Long-term conditions with significant disease heterogeneity comprise large portions of the routine work performed by paediatricians. Improving outcomes through discovery of disease and treatment prediction models, alongside novel subgroup clustering of patients, are some of the areas in which machine learning holds significant promise. While artificial intelligence has percolated into routine use in our day to day lives through advertising algorithms, song or movie selections and sifting of spam emails, the ability of machine learning to utilise highly complex and dimensional data has not yet reached its full potential in healthcare. In this review article, we discuss some of the foundations of machine learning, including some of the basic algorithms. We emphasise the importance of correct utilisation of machine learning, including adequate data preparation and external validation. Using nutrition in preterm infants and paediatric inflammatory bowel disease as examples, we discuss the evidence and potential utility of machine learning in paediatrics. Finally, we review some of the future applications, alongside challenges and ethical considerations related to application of artificial intelligence. IMPACT: Machine learning is a widely used term; however, understanding of the process and application to healthcare is lacking. This article uses clinical examples to explore complex machine learning terms and algorithms. We discuss limitations and potential future applications within paediatrics and neonatal medicine.","The integration of AI in clinical practice is rapidly increasing, and by relying on diagnostic and prognostic algorithms, clinicians are helped in the decision-making process to generate personalized treatments in many clinical settings [[10],[39]. However, there is still a debate on how AI assistance may affect medical performance; on the one hand, it can improve the sensitivity of clinical experts, while, on the other hand, it may lower their specificity. Studies showed that AI predictions based on explainable algorithms developed with a transparent model showed substantial benefits in settings such as liver transplantation (see dedicated section), antiviral therapy, and chemotherapy or in helping to anticipate strategic decisions to curb the local burden of pandemics such as COVID-19 [40, 41, 42, 43]. On the other hand, the potential benefits of AI using ML systems are hampered by their black-box nature, which poses new important ethical and legal challenges spanning from data quality to medical–legal questions arising from the incorporation of AI into clinical practice [[10],[39],[44]. Because of the uncertainty generated by the lack of scrutiny of the recommendations provided by AI algorithms, clinicians will be unable to take appropriate steps to mitigate their concern that algorithm inaccuracy could lead to patient injury and medical liability [44, 45, 46].
Substantially, AI transforms the traditional therapeutic relationship between physicians and patients into a new triadic doctor–machine–patient relationship [44, 45, 46, 47, 48]. This revolution complicates the attribution of responsibility in malpractice lawsuit experts attempting to define a new legal framework that considers the AI role in healthcare, reducing as much as possible the existing heterogeneity of approaches across countries regarding medical liability [44, 45, 46,[48],[49].
Although a major aim of AI is to help reduce the risk of potential medical errors, paradoxically, an overreliance on AI systems could become dangerous, particularly when clinicians do not have the sufficient technological knowledge to understand the proper functioning of AI systems and their limits and safety (see dedicated section). Problems arise when it is difficult to rely on alternative systems that, in parallel, could provide information on the reliability of any particular result provided by AI since any AI-helped action will never be faultless.
Experts extensively discussed the possibility of giving the AI systems a legal personhood so that they would become directly responsible for their own decisions and actions. However, if AI systems are recognized as a legal personhood with an active part in the decision-making process, it will be unacceptable to attribute any error to the human factor. The safety of the health care system relies on an organizational framework that warrants the well-functioning of all interdependent components and services: people, technology, and their interaction. Thus, the basic concept is that errors can derive from human behavior but also from malfunctioning of technologies, even though they are supposed to be “intelligent.” The Committee of Legal Affairs of the European Parliament stated that “AI-systems have neither legal personality nor human conscience, and that their sole task is to serve humanity” (https://commission.europa.eu/system/files/2022-09/1_1_197605_prop_dir_ai_en.pdf) [[44]]. To date, giving AI a legal personality is considered inadequate because even supposed intelligent technologies are not substantially different from any other non-AI-based sophisticated technology already used (https://commission.europa.eu/system/files/2022-09/1_1_197605_prop_dir_ai_en.pdf) [[44]].
However, since the experts’ opinions always conflict when dealing with the most advanced knowledge and technology, AI liability issues when applied to healthcare assistance remain open. Accordingly, introducing AI systems in clinical practice prompts to build infrastructures to deal with critical issues such as data, quality, privacy and security, and safe data sharing [50, 51, 52]. Special attention should be paid to mitigating bias throughout the whole cycle of medical AI, from data collection to after deployment, particularly when hurting marginalized groups [[53],[54]. As predictive models developed by ML algorithms are based on data on which the whole AI system is built, one major target of AI ethics will be to address the biases of AI models associated with the quality and quantity of the data used [[55]]. Regulation and governance of medical AI requires the implementation of standardized safe AI practices and the establishment of a transparent reporting of the performance of AI systems. This policy will make clinicians less skeptical and more reliant on their AI-assisted decision-making without losing control over their own care because of the potentially unexplained AI results.
Finally, since medical doctors are currently held liable when they deviate from the standard of care and patient injury occurs, a special concern is accountability, as it is not yet clear whether developers, sellers, or healthcare providers should be held accountable if a given AI system makes mistakes even after being clinically validated."
"Using machine learning to impact on long-term clinical care: principles, challenges, and practicalities","Although advancements in the field of inflammatory bowel disease (IBD) include effective therapies for many patients with Crohn's disease and ulcerative colitis, there remains a large unmet need, and there is a large number of investigational agents in the pipeline. Drug development through clinical trials is critical to understanding the safety and efficacy of new therapies in the affected human population, and the need for ethical trial design is of the utmost importance. This paper explores the ethical issues of clinical trials in IBD, focusing on placebo-controlled trials, vulnerable patients, exposure to monoclonal antibodies, globalization of trials, and surgical advances.","The present review summarizes an overview of 15 IBD specific HRQoL instruments with respect to their measurement properties and the methodological quality based on the COSMIN checklist. According to the results of the COSMIN checklist, most of the instruments did not include all the methodological quality. Only content validity was assessed properly in most of the included instruments. Most of the instruments scored “good” or “fair” for internal consistency, reliability, structural validity, hypotheses testing and criterion validity. The information regarding measurement error, responsiveness and cross-cultural validity was limited or was of poor measurement property because they did not reach the required criteria or because of insufficient information. Our results were consistent with other instruments appraised by the COSMIN criteria, such as irritable bowel syndrome specific QOL instruments [87]; rheumatoid arthritis specific QOL instruments [88]; and QOL instruments for infants, children and adolescents with eczema [89].
Most of the IBD-specific instruments did not show adequate methodological quality. One reason for this was that most of the IBD-specific HRQoL instruments were developed before 2010. However, COSMIN guidelines were developed approximately 2010 [12–14]. Therefore, older articles could not follow COSMIN guidelines, and their measurement properties might be underestimated. Based on the results of the measurement properties and translated versions of the included instruments, some instruments had good psychometric characteristics and were widely used. (1) For paediatric IBD-specific instruments,
most of the measurement properties were tested properly, especially the IMPACT-III [21]. The IMPACT-III had the same items as the IMPACT-II. However, The IMPACT-III was on a 0–4 Likert scale, which was easily understood by children. The IMPACT-III was translated into at least 4 translated versions [51–54].
The IMPACT-III was recommended to assess the HRQoL for paediatric IBD patients. (2) For the adult IBD instruments, the IBDQ-32 and SIBDQ (short version of IBDQ-32) had good measurement properties. The two
instruments had excellent content validity and proved to be valid, reliable and responsive. The two instruments contained symptoms, emotional and social domains. The two instruments were used widely. The IBDQ-32 has been translated and validated in 93 languages. The SIBDQ was used in the UK, the US, Germany and Spain [40–43]. The IBDQ-9, CGQL, SHS, EIBDQ and CUCQ were all short instruments, which had relatively high methodological quality. However, they had fewer translated versions. The IBDQ-36, CCQIBD, PIBDQL, CGQL and EIBDQ had the lowest measurement properties.
The PIBDQL and CGQL instruments were developed and assessed based on IBD patients receiving surgery, and they were translated into other languages. The EIBDQ had not been translated into other languages, which limited its use. Compared with reviews of IBD-specific instruments published by other authors [3–8], our review had the following advantages. (1) Our review included more eligible IBD-specific HRQoL instruments. For example, the review conducted by Alrubaiy et al. enrolled 10 instruments [8]. Among them, only five instruments were about HRQoL instruments, while others were burden or disability instruments, such as the Crohn’s disease burden questionnaire, the IBD disability score and the IBD disability index. (2) Our review fully evaluated the measurement properties, including content reliability, internal consistency, test-retest reliability, measurement error,
convergent/divergent, discriminant validity, criterion validity, cross-cultural validity and responsiveness. Previous reviews did not evaluate criterion validity, discriminant validity or cross-cultural validity for each instrument [8]. Criterion validity and discriminant validity are important features for the instrument. Criterion validity reflects the extent to which scores on a particular instrument relate to a gold standard. Discriminant validity refers to how well the scale can discriminate between different features of the participants.
All of the IBD-specific instruments were developed in North American and European countries. This is likely because the highest incidence and prevalence rates of IBD are in Europe and North America [90]. Another reason might be associated with the popularity of the QOL concepts and the standard procedure for QOL development [91, 92]. In developing countries, researchers mainly focused on translating and back-translating the IBD-specific instruments and used them to assess the QOL of IBD patients.
Although there was a lack of consensus regarding the specific domains among all of the instruments, the common domains measured in the instruments were identified: IBD-related symptoms, physical functioning or general wellbeing, emotional functioning and social functioning. These domains were consistent with the concepts of the common scales, such as the WHOQOL and FACT-G [92–94]. The typical manifestation of IBD included diarrhea with blood, fever, abdominal pain and malnutrition. These symptoms are the most frequently occurring, meaning that the domains contribute the most important information to the IBD specific instruments.
The limitations of this study were as follows: (1) Non-English articles were not enrolled because of language restrictions; thus, the restriction resulted in limited negative evidence for this study; (2) Articles about the original language were used to assess the measurement properties of the included instruments. The translated articles were not used for the assessment of measurement properties; and (3) Some articles about clinical trials may have been excluded in this review, which resulted in a limited ability to examine responsiveness."
Inflammatory bowel disease-specific health-related quality of life instruments: a systematic review of measurement properties,"Background: This review aims to critically appraise and compare the measurement properties of inflammatory bowel disease (IBD)-specific health-related quality of life instruments.
Methods: Medline, EMBASE and ISI Web of Knowledge were searched from their inception to May 2016. IBD-specific instruments for patients with Crohn's disease, ulcerative colitis or IBD were enrolled. The basic characteristics and domains of the instruments were collected. The methodological quality of measurement properties and measurement properties of the instruments were assessed.
Results: Fifteen IBD-specific instruments were included, which included twelve instruments for adult IBD patients and three for paediatric IBD patients. All of the instruments were developed in North American and European countries. The following common domains were identified: IBD-related symptoms, physical, emotional and social domain. The methodological quality was satisfactory for content validity; fair in internal consistency, reliability, structural validity, hypotheses testing and criterion validity; and poor in measurement error, cross-cultural validity and responsiveness. For adult IBD patients, the IBDQ-32 and its short version (SIBDQ) had good measurement properties and were the most widely used worldwide. For paediatric IBD patients, the IMPACT-III had good measurement properties and had more translated versions.
Conclusions: Most methodological quality should be promoted, especially measurement error, cross-cultural validity and responsiveness. The IBDQ-32 was the most widely used instrument with good reliability and validity, followed by the SIBDQ and IMPACT-III. Further validation studies are necessary to support the use of other instruments.","The present review summarizes an overview of 15 IBD-specific HRQoL instruments with respect to their measurement properties and the methodological quality based on the COSMIN checklist.
According to the results of the COSMIN checklist, most of the instruments did not include all the methodological quality. Only content validity was assessed properly in most of the included instruments. Most of the instruments scored “good” or “fair” for internal consistency, reliability, structural validity, hypotheses testing and criterion validity. The information regarding measurement error, responsiveness and cross-cultural validity was limited or was of poor measurement property because they did not reach the required criteria or because of insufficient information. Our results were consistent with other instruments appraised by the COSMIN criteria, such as irritable bowel syndrome-specific QOL instruments [87]; rheumatoid arthritis-specific QOL instruments [88]; and QOL instruments for infants, children and adolescents with eczema [89]. Most of the IBD-specific instruments did not show adequate methodological quality. One reason for this was that most of the IBD-specific HRQoL instruments were developed before 2010. However, COSMIN guidelines were developed approximately 2010 [12–14]. Therefore, older articles could not follow COSMIN guidelines, and their measurement properties might be underestimated.
Based on the results of the measurement properties and translated versions of the included instruments, some instruments had good psychometric characteristics and were widely used. (1) For paediatric IBD-specific instruments, most of the measurement properties were tested properly, especially the IMPACT-III [21]. The IMPACT-III had the same items as the IMPACT-II. However, The IMPACT-III was on a 0–4 Likert scale, which was easily understood by children. The IMPACT-III was translated into at least 4 translated versions [51–54]. The IMPACT-III was recommended to assess the HRQoL for paediatric IBD patients. (2) For the adult IBD instruments, the IBDQ-32 and SIBDQ (short version of IBDQ-32) had good measurement properties. The two instruments had excellent content validity and proved to be valid, reliable and responsive. The two instruments contained symptoms, emotional and social domains. The two instruments were used widely. The IBDQ-32 has been translated and validated in 93 languages. The SIBDQ was used in the UK, the US, Germany and Spain [40–43]. The IBDQ-9, CGQL, SHS, EIBDQ and CUCQ were all short instruments, which had relatively high methodological quality. However, they had fewer translated versions. The IBDQ-36, CCQIBD, PIBDQL, CGQL and EIBDQ had the lowest measurement properties. The PIBDQL and CGQL instruments were developed and assessed based on IBD patients receiving surgery, and they were translated into other languages. The EIBDQ had not been translated into other languages, which limited its use.
Compared with reviews of IBD-specific instruments published by other authors [3–8], our review had the following advantages. (1) Our review included more eligible IBD-specific HRQoL instruments. For example, the review conducted by Alrubaiy et al. enrolled 10 instruments [8]. Among them, only five instruments were about HRQoL instruments, while others were burden or disability instruments, such as the Crohn’s disease burden questionnaire, the IBD disability score and the IBD disability index. (2) Our review fully evaluated the measurement properties, including content reliability, internal consistency, test-retest reliability, measurement error, convergent/divergent, discriminant validity, criterion validity, cross-cultural validity and responsiveness. Previous reviews did not evaluate criterion validity, discriminant validity or cross-cultural validity for each instrument [8]. Criterion validity and discriminant validity are important features for the instrument. Criterion validity reflects the extent to which scores on a particular instrument relate to a gold standard. Discriminant validity refers to how well the scale can discriminate between different features of the participants.
All of the IBD-specific instruments were developed in North American and European countries. This is likely because the highest incidence and prevalence rates of IBD are in Europe and North America [90]. Another reason might be associated with the popularity of the QOL concepts and the standard procedure for QOL development [91, 92]. In developing countries, researchers mainly focused on translating and back-translating the IBD-specific instruments and used them to assess the QOL of IBD patients.
Although there was a lack of consensus regarding the specific domains among all of the instruments, the common domains measured in the instruments were identified: IBD-related symptoms, physical functioning or general wellbeing, emotional functioning and social functioning. These domains were consistent with the concepts of the common scales, such as the WHOQOL and FACT-G [92–94]. The typical manifestation of IBD included diarrhea with blood, fever, abdominal pain and malnutrition. These symptoms are the most frequently occurring, meaning that the domains contribute the most important information to the IBD-specific instruments.
The limitations of this study were as follows: (1) Non-English articles were not enrolled because of language restrictions; thus, the restriction resulted in limited negative evidence for this study; (2) Articles about the original language were used to assess the measurement properties of the included instruments. The translated articles were not used for the assessment of measurement properties; and (3) Some articles about clinical trials may have been excluded in this review, which resulted in a limited ability to examine responsiveness."
Challenges and opportunities in the application of artificial intelligence in gastroenterology and hepatology,"Artificial intelligence (AI) is an umbrella term used to describe a cluster of interrelated fields. Machine learning (ML) refers to a model that learns from past data to predict future data. Medicine and particularly gastroenterology and hepatology, are data-rich fields with extensive data repositories, and therefore fruitful ground for AI/ML-based software applications. In this study, we comprehensively review the current applications of AI/ML-based models in these fields and the opportunities that arise from their application. Specifically, we refer to the applications of AI/ML-based models in prevention, diagnosis, management, and prognosis of gastrointestinal bleeding, inflammatory bowel diseases, gastrointestinal premalignant and malignant lesions, other nonmalignant gastrointestinal lesions and diseases, hepatitis B and C infection, chronic liver diseases, hepatocellular carcinoma, cholangiocarcinoma, and primary sclerosing cholangitis. At the same time, we identify the major challenges that restrain the widespread use of these models in healthcare in an effort to explore ways to overcome them. Notably, we elaborate on the concerns regarding intrinsic biases, data protection, cybersecurity, intellectual property, liability, ethical challenges, and transparency. Even at a slower pace than anticipated, AI is infiltrating the healthcare industry. AI in healthcare will become a reality, and every physician will have to engage with it by necessity.","Inflammatory bowel disease (IBD) is a chronic, idiopathic condition. Current treatment options often are unable to achieve disease modification or control. Although there are some effective therapies currently available for Crohn’s disease (CD) and ulcerative colitis (UC), there remain substantial gaps in effective treatments for the many patients who do not respond to these therapies. Therefore, better treatments with novel mechanisms of action are needed. The development of these treatments requires clinical trials on human subjects.
The design of clinical trials for patients with IBD must adhere to uniformly accepted ethical standards of human subject research, as established by the Declaration of Helsinki, the Belmont Report, and the code of federal regulations on the protection of human subjects.1-3 These ethical principles include beneficence, nonma-leficence, respect for persons, and justice (Table). This review of the ethical implications of human subject research includes several concerns that are specifically pertinent to the IBD patient population. Because surgery is frequently necessary for patients with IBD, we also include a discussion of surgical innovations. We conclude with recommendations for the design of future, ethically sound clinical trials and surgical innovations in IBD. Placebo-Controlled Trials
A 1984 position paper from the American College of Gastroenterology (ACG) describes several circumstances in which placebos may be used in clinical trials.4 In this paper, the ACG asserts that placebo use is ethically appropriate when a standard therapy has yet to be established or when a standard therapy has previously been shown to be ineffective. Another ethical use of placebo is for a population that is intolerant to standard therapy and, therefore, unable to receive it. Furthermore, the position paper notes that placebo has been shown to be an effective treatment in many contexts, including induction and maintenance of remission in patients with active CD and UC.
Since the publication of this position paper, several effective standard-line therapies for CD and UC have been developed and clinically validated. Unless study participants are specifically intolerant of these therapies, the rule of clinical equipoise requires that these therapies not be withdrawn from any participant. Regarding the effectiveness of placebo treatment, this speaks mainly to the highly variable nature of placebo-controlled studies. A 2009 analysis by Sands identified multiple factors associated with the effectiveness or ineffectiveness of placebo treatment.5 These included regression to the mean during natural disease progression (depending on the severity of disease within the inclusion criteria), subjects’ purported knowledge of the effectiveness of the experimental treatment, the number of physician visits during the study, and even the attitude and tone of the physician while describing possible treatment outcomes. The most strictly controlled studies might aim to diminish the placebo effect by closely attending to these factors and increasing the risk to subjects, while not attending to these measures might diminish the value of a placebo-controlled trial in the first place.
Nevertheless, placebo-controlled studies remain the most statistically powerful method of elucidating the absolute effectiveness of a therapy. The issue, however, lies in the appropriate use of placebo. The primary reason for this obligation lies in the concept of clinical equipoise. The Declaration of Helsinki proposes that the only acceptable use of a placebo is when there is either no other effective therapy in existence or if no serious or irreversible harm to a subject would be risked by participating in a placebo study arm. The former case is not applicable to the majority of patients with CD or UC; the latter case, although not yet quantified across IBD clinical trials, remains of questionable relevance for any chronic illness. A review of clinical trials in asthma concluded that participants in placebo arms were significantly more likely than those in active treatment arms to withdraw from a study due to serious adverse events.6 Similarly, a meta-analysis of studies in hypertension found that any active treatment significantly reduced the risk of serious adverse events compared with placebo arms.7 Recognizing that IBD is also a chronic and progressive disorder with consequences from ineffective (or non) treatment, one might argue that the ethical range in which placebo-controlled trials would be justifiable is quite narrow. This may especially be true in the development of analog therapies to existing effective standard-of-care strategies, such as the use of aminosalicylates with novel or different delivery systems.
In addition, more recent work has demonstrated that efforts to reduce the placebo response may essentially eradicate equipoise. In other words, as we move toward more objective measures of disease response and remission, placebo responses and remissions are lower. Although there is a benefit to this from a clinical trial design and statistical power point of view, it raises critical questions about whether it remains ethical to perform placebo-controlled trials, especially for drugs within a class that has been previously (and repeatedly) shown to be superior to placebo.8
Vulnerable Patients
In establishing the principle of justice in clinical research, the Belmont Report asserts that some populations are more susceptible to harm than others in the context of clinical trials. Specifically, these populations include subjects unable to give informed consent and those especially vulnerable to coercion or undue influence. Within the IBD population, there are several identifiable areas of vulnerability.
Although the magnitude of uninsured or under-insured patients with IBD in the United States is not known, a 2009 study by Nguyen and colleagues analyzed hospitalization and insurance data from the Nationwide Inpatient Sample between 1999 and 2005.9 The study found that uninsured patients with IBD are more likely to be hospitalized than both insured patients with IBD and the general patient population. Furthermore, the rate of hospitalization among uninsured patients with IBD underwent a nearly 2-fold increase during the period analyzed, while the hospitalization rate of the general uninsured population remained steady. As the authors indicate, increased hospitalization in this context may be partly attributed to a decrease in outpatient care among the uninsured.10 Finally, the investigators found that uninsured patients with IBD were significantly more likely to leave the hospital against medical advice. Data regarding hospitalizations of patients with IBD enrolled in Medicaid are only preliminary thus far.11
Although these rates of hospitalization may have changed since 2005, the consideration of health insurance in the context of IBD clinical trials is important. It is clear that patients without insurance are at risk for failing to receive appropriate medical care under normal circumstances. In turn, participation in a clinical trial in which potentially effective therapy is provided without financial cost may seem very attractive to uninsured patients.12
These factors point to an overall vulnerability among uninsured patients with IBD, a group that is proportionally larger than the general uninsured population. Uninsured patients may be more likely to volunteer for clinical trials because of easy and payment-free access to any type of therapy or out of desperation to obtain care. Therefore, in enrolling uninsured patients, great care must be taken to ensure that the patient fully understands and consents to the medical risks involved in the trial and is freely acting in his or her own rational interests.
A second area of vulnerability lies in patients with acute or refractory IBD. Unfortunately, this is common among the UC and CD populations. These patients are vulnerable due to their healthcare experiences with ineffective therapies and subsequent poor quality of life, both of which may increase the propensity to make healthcare decisions based solely on desperation.13 It is conceivable that some, if not many, of these patients would actively seek out any new therapy via participation in clinical trials, regardless of the risks involved. The Declaration of Helsinki states that in cases in which no effective therapy has been found, the physician “may use an unproven intervention if, in the physicians judgment, it offers hope of saving life, reestablishing health, or alleviating suffering.” It is therefore apparent that in enrolling a patient with acute UC or CD in a clinical trial, the patient faces 2 risks: the risk of continued suffering and the risk of adverse events as a result of unproven therapies. The investigators guidance is important, both in helping to inform the decision in a rational manner and in explaining to the patient as clearly as possible the rationale of the study.
On a final note, discussions are currently taking place to revise the “Common Rule,” a set of regulations from the US Department of Health and Human Services (HHS) that specifically addresses human subject research.14 Of note within the proposed changes is a program to develop a national consent form template. Citing several studies, the HHS finds a trend toward longer, more complicated consent forms that may include legal jargon and a lack of structure between sections. Together, these factors may result in the inability of a patient to make a decision based on a clear understanding of risk. The proposed template would be used by all federally regulated institutional review boards (IRBs), thus giving clearer guidelines for informed consent, helping to remove disparities between multiple sites, and increasing the overall protection of the vulnerable patients in question.
Exposure to Monoclonal Antibodies
One of the most significant advancements in the treatment of CD and UC was the development of biologic therapies, the first of which have been the anti—tumor necrosis factor (anti-TNF) agents. These antibodies act by binding to the cytokine TNF-?and have been shown to be effective in patients with moderate to severe CD and UC. Additional monoclonal antibodies that target other cytokines and molecules are in development. Like any new drugs, these agents must undergo preliminary evaluations in small phase 2 studies before additional phase 3 studies are completed for subsequent review and potential approval by the US Food and Drug Administration (FDA). These phase 2 trials serve to establish the overall efficacy of the drug, but a great deal of time may pass before a subsequent phase 3 trial is completed and the drug becomes commercially available.
The use of biologic agents in clinical trials is associated with unique issues. After a transient exposure to a biologic agent through a single or short set of treatments, there is a risk of development of immune antibodies against the agent in the absence of continuing therapy. In an analysis of data from the ACCENT I trial, Hanauer and colleagues examined patients with CD who had received an initial infusion of infliximab (Remicade, Janssen) followed by a 46-week period of placebo infusions.15 At the end of the study period, antibodies to infliximab had developed in 30% of these patients. Furthermore, these antibodies were accompanied by an increased risk of severe hypersensitivity reactions and loss of response with future drug exposure.16
As more new biologic agents undergo clinical testing, investigators should be wary of this significant risk to subjects. Initial studies of infliximab, which led to FDA approval of the drug, did not extend this effective treatment for subjects beyond their conclusion; consequently, these patients had to wait months to a year until infliximab was marketed commercially.17 Although concomitant imu-nosuppressive agents or corticosteroids have been shown to reduce formation of anti-infliximab antibodies,18 this strategy is time-specific and may not be relevant to patients waiting for FDA approval for market distribution.
If enrollment and experimental treatment decrease or greatly decrease the possibility of a potent therapy when it becomes commercially available, such “sacrifice” by human subjects in clinical trials is unethical. In the case of biologic agents, the risk of disregarding this principle is especially great. Patients who are enrolled in an experimental treatment arm involving biologic agents and who demonstrate clear benefit from their treatment should be allowed access to this therapy as part of their continuing medical regimen. This should be explicitly defined in the protocol of any study involving an experimental agent with the potential to elicit a long-term antibody immune response.
Globalization
Due to difficulties in domestic recruiting of patients as well as efforts to capture more diverse patient populations, pharmaceutical companies and contract researchers have sought international locations for study recruitment. Indeed, globally outsourced clinical trials represent a significant portion of current research in IBD. A search at ClinicalTrials.gov for clinical trials in CD and UC found that approximately 21% of current trials are being conducted in developing nations (as defined by the Human Development Index).19 Each of these trials is sponsored by foreign research institutions, contract research organizations, and/or pharmaceutical companies. Factors involved in the trend of outsourcing clinical trial sites to developing nations include a general willingness to extend the global research community, global or local prevalence of particular conditions, lower cost of labor among healthcare professionals, and larger and more willing pools of potential subjects.
Although these factors may be tremendously advantageous in increasing the effectiveness of research investments and expediting the process of developing new therapies, there are several ethical concerns associated with outsourced clinical trials. The type of ethical oversight for clinical trials in developed countries, such as IRBs or even enforced laws regarding ethical research, may not be as comprehensive in developing nations. The burden then falls mainly to off-site investigators, in which case thorough ethical oversight may prove to be difficult.20,21 For example, a recent placebo-controlled trial of granulocyte macrophage colony-stimulating factor for treatment of CD drew enrollment from several Eastern Bloc nations, and the off-site investigators were found to have violated the inclusion criteria and study protocol, clearly skewing the results of the study (C. A. Siegel, oral communication, August 2013).
Specifically, what are the most salient ethical concerns in outsourced research? First, the differences between monetary compensation for both labor and subject participation may prompt a coercive or exploitative research environment. Compensation that is considered standard or modestly adjusted in the Western world may be much greater in a developing economy. Therefore, an excessive payment to overseas research professionals may inspire the pursuit of potentially unethical means to achieve a certain level of patient participation. In addition, excessive subject reward for participation is a definitively coercive method of enrollment; scrutiny of payment for outsourced research is required for proper ethical oversight.
Another concern is the lack of general healthcare access in developing nations. It is often the case in poorer nations that participants in clinical trials do not have access to any basic healthcare outside of the research setting. This is an acknowledged motivation for participation in clinical trials among foreign investigators and subjects.22
Currently, the best possible way to ensure that clinical trials conducted within this environment are ethically sound is to firmly maintain the system of informed consent and to have appropriate IRB review for patient protection. Although there is a clear benefit for poor participants from developing countries to have access to experimental therapies, the reality is that trial participation is often risky.23 This should be made absolutely clear to all participating subjects.
Finally, according to the principle of beneficence outlined in the Belmont Report, any research involving human subjects must benefit the patient population on which the research is being conducted. Accordingly, study participants should be guaranteed access to an effective therapy if one should be discovered. As Glickman and colleagues indicate, however, the primary markets for therapies for the most commonly known diseases are developed nations.20 This is a much broader concern to be addressed, and currently it is unclear to what extent outsourced research in IBD benefits the populations of subjects being tested, or if the increasing presence of clinical trials in developing nations is leading to better healthcare and easier access to new therapies in this area.
Surgical Advances
In surgical treatments for IBD, there has been a movement toward new, less invasive procedures, as well as more effective protocols, such as staging and differential timing of restorative ileoanal pouch surgery. Such advances have, in general, been performed in the hopes of reducing complications of IBD-related surgery, but as with most surgical innovations, they have been implemented without the use of existing standard practices.
Innovations in surgery differ from new drugs in several respects, most notable of which is the fact that patients undergoing surgery present unique profiles that, in many cases, require variation and improvisation on the part of the surgeon. In turn, the use of a novel procedure often arises outside of an explicit research context. The question for surgical innovators then is whether their novel methods should be conducted with the same type of ethical oversight mandated by drug investigators in the ways previously described.
In a 2008 position statement of the Society of University Surgeons, Biffl and colleagues state that any planned procedure that has unknown or less-understood outcomes or contains an apparent risk must not only be accompanied by informed consent, but also must be formally proposed to a local “surgical innovations committee” analogous to an IRB.24 Furthermore, after the procedure is performed, the positive or negative outcome must be made known to the field. This point is important in maintaining the standard of beneficence, as the work performed by the surgeon will then provide a benefit to the patient population at large.
Conclusions
In this review, we have explored the ethical issues of clinical trials in IBD, with a focus on several unique situations for patients with IBD and the healthcare community that cares for them. Although there is a clear need for additional therapies that offer more successful disease control and modification of outcomes, such therapies must be developed through ethically sound clinical trials with appropriate involvement of the global patient population. Patients who volunteer for early-phase clinical trials of biologic therapies should be given access to such therapies so that they do not risk loss of response due to immunogenicity. Surgical innovation in IBD does not always occur in standard IRB-approved clinical trial settings and, therefore, should be accompanied by careful informed consent and appropriate communication of such advances to the rest of the surgical and medical communities."
Selecting the best machine learning algorithm to support the diagnosis of Non-Alcoholic Fatty Liver Disease: A meta learner study,"Background & aims: Liver ultrasound scan (US) use in diagnosing Non-Alcoholic Fatty Liver Disease (NAFLD) causes costs and waiting lists overloads. We aimed to compare various Machine learning algorithms with a Meta learner approach to find the best of these as a predictor of NAFLD.
Methods: The study included 2970 subjects, 2920 constituting the training set and 50, randomly selected, used in the test phase, performing cross-validation. The best predictors were combined to create three models: 1) FLI plus GLUCOSE plus SEX plus AGE, 2) AVI plus GLUCOSE plus GGT plus SEX plus AGE, 3) BRI plus GLUCOSE plus GGT plus SEX plus AGE. Eight machine learning algorithms were trained with the predictors of each of the three models created. For these algorithms, the percent accuracy, variance and percent weight were compared.
Results: The SVM algorithm performed better with all models. Model 1 had 68% accuracy, with 1% variance and an algorithm weight of 27.35; Model 2 had 68% accuracy, with 1% variance and an algorithm weight of 33.62 and Model 3 had 77% accuracy, with 1% variance and an algorithm weight of 34.70. Model 2 was the most performing, composed of AVI plus GLUCOSE plus GGT plus SEX plus AGE, despite a lower percentage of accuracy.
Conclusion: A Machine Learning approach can support NAFLD diagnosis and reduce health costs. The SVM algorithm is easy to apply and the necessary parameters are easily retrieved in databases.","In this study, the search for the best algorithm to support NAFLD diagnosis was conducted by comparing the different Machine Learning algorithms for each model.
Specifically, the model with a high level of accuracy and the model with the lowest level of variance were identified in this research. The Machine Learning model presenting the lowest variance was selected.
Today, the search for non-invasive methods is very important, considered as an alternative to the expensive NAFLD diagnostic tools (MRI, Ultrasound). The reorganization of the National Health System requires closer consideration of aspects linked to the performance together with the factors linked to cost-reduction and waiting times. The aim of our study was to use new, modern Machine Learning techniques to support medical decisions during the diagnostic phase using easier and cheaper tools, thus reducing both the costs and waiting times inherent to the use of instrumental methods.
In view of the results of this study, it is possible to state that the most appropriate Machine Learning Algorithm is the Support Vector Machine in Python. In particular, the Support Vector Machine employing AVI plus Glucose plus GGT plus Sex plus Age, despite having an almost identical percentage of accuracy and weight to the other models, produced fewer prediction errors in the test step. We obtained in the test phase for the models composed of FLI plus Glucose plus Age plus Sex and AVI plus Glucose plus GGT plus Age plus Sex a percentage error equal to 32% while for the model composed of BRI plus Glucose plus GGT plus Age plus Sex an error of 23%. However, in the prediction phase, the model that made fewer errors was the one composed of AVI plus Glucose plus GGT plus Age plus Sex with an error of 20% while FLI plus Glucose plus Age plus Sex 26% and BRI plus Glucose plus GGT plus Age plus Sex 28%.
Therefore, AVI plus Glucose plus GGT plus Sex plus Age was the model that contributed most to reducing unnecessary ultrasound examinations. The good performance of ML Algorithms used to identify NAFLD, applying common anthropometric parameters and other variables, has shown them to be a valid alternative to classic Indexes [47, 48]. Moreover, the SVM was well able to identify subjects without NAFLD. From an ethical perspective, the model with the lowest variance is the best one, as it is characterized by a smaller number of false negatives, despite a lower percentage of accuracy.
This kind of study underlines the fact that this ML Algorithm can be used to find subjects at high risk of NAFLD, who need to undergo US. Furthermore, as low-risk subjects do not undergo US, 81.9% of unnecessary US examinations could be avoided (this value was calculated as the ratio of the total number of subjects in the test set divided by the total number of subjects in the test set plus the number of incorrect predictions.)
Some methodological issues need to be considered. A strength of this study is the population-based random sample from which the observations were drawn. The NAFLD prevalence in the sample is a good estimator of the population prevalence and its age-sex distribution. Limitations include both the limited number of observations and the method used to perform NAFLD diagnosis. It may be criticized the low sensibility of the NAFLD diagnostic methodology, as it fails to detect fatty liver content >25–90% [49]. However, this is a population-based study and subjects were chosen from the electoral register. They did not seek medical attention and participated on volunteer basis. Then, the diagnosis of NAFLD performed by US was the only diagnostic procedure we could propose to the participants. Ethical issues prevent us to propose biopsy or H-MRS. Moreover, and to lighten the waiting lists, our purpose was to find out a machine learning algorithm that permit us to avoid a number of USs which otherwise would have been prescribed. Then, this algorithm is useful to exclude NAFLD and as valid support diagnostic in the context of epidemiologic studies and not as replacement diagnostic tool.
In conclusion, this model, like others based on ML Algorithms, may be considered as a valid support for medical decision making as regards health policies, in epidemiological studies and screening."
Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension,"The CONSORT 2010 statement provides minimum guidelines for reporting randomised trials. Its widespread use has been instrumental in ensuring transparency in the evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate impact on health outcomes. The CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence) extension is a new reporting guideline for clinical trials evaluating interventions with an AI component. It was developed in parallel with its companion statement for clinical trial protocols: SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials-Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 29 candidate items, which were assessed by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a two-day consensus meeting (31 stakeholders), and refined through a checklist pilot (34 participants). The CONSORT-AI extension includes 14 new items that were considered sufficiently important for AI interventions that they should be routinely reported in addition to the core CONSORT 2010 items. CONSORT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention is integrated, the handling of inputs and outputs of the AI intervention, the human–AI interaction and provision of an analysis of error cases. CONSORT-AI will help promote transparency and completeness in reporting clinical trials for AI interventions. It will assist editors and peer reviewers, as well as the general readership, to understand, interpret, and critically appraise the quality of clinical trial design and risk of bias in the reported outcomes.","CONSORT-AI is a new reporting-guideline extension developed through international multi-stakeholder consensus. It aims to promote transparent reporting of AI intervention trials and is intended to facilitate critical appraisal and evidence synthesis. The extension items added in CONSORT-AI address a number of issues specific to the implementation and evaluation of AI interventions, which should be considered alongside the core CONSORT 2010 checklist and other CONSORT extensions.54 It is important to note that these are minimum requirements and there may be value in including additional items not included in the checklists (appendix pp 2–5) in the report or in supplementary materials.
In both CONSORT-AI and its companion project SPIRIT-AI, a major emphasis was the addition of several new items related to the intervention itself and its application in the clinical context. Items 5 (i)–5 (vi) were added to address AI-specific considerations in descriptions of the intervention. Specific recommendations were made pertinent to AI systems related to algorithm version, input and output data, integration into trial settings, expertise of the users, and protocol for acting upon the AI system’s recommendations. It was agreed that these details are critical for independent evaluation or replication of the trial. Journal editors reported that despite the importance of these items, they are currently often missing from trial reports at the time of submission for publication, which provides further weight for their inclusion as specifically listed extension items.
A recurrent focus of the Delphi comments and Consensus Group discussion was the safety of AI systems. This was in recognition that AI systems, unlike other health interventions, can unpredictably yield errors that are not easily detectable or explainable by human judgement. For example, changes to medical imaging that are invisible, or appear random, to the human eye may change the likelihood of the diagnostic output entirely.55,56 The concern is that given the theoretical ease with which AI systems could be deployed at scale, any unintended harmful consequences could be catastrophic.
CONSORT-AI item 19, which requires specification of any plans to analyse performance errors, was added to emphasise the importance of anticipating systematic errors made by the algorithm and their consequences.
Beyond this, investigators should also be encouraged to explore differences in performance and error rates across population subgroups. It has been shown that AI systems may be systematically biased toward different outputs, which may lead to different or even unfair treatment, on the basis of extant features.53,57–59 The topic of “continuously evolving” AI systems (also known as “continuously adapting” or “continuously
learning” AI systems) was discussed at length during the consensus meeting, but it was agreed that this be excluded from CONSORT-AI. These are AI systems with the ability to continuously train on new data, which may
cause changes in performance over time. The group noted that, while of interest, this field is relatively early in its development without tangible examples in health-care applications, and that it would not be appropriate for it to
be included in CONSORT-AI at this stage.60 This topic will be monitored and will be revisited in future iterations of CONSORT-AI. It is worth noting that incremental software changes, whether continuous or iterative, intentional
or unintentional, could have serious consequences on safety performance after deployment. It is therefore of vital importance that such changes be documented and identified by software version and that a robust postdeployment surveillance plan is in place.
This study is set in the current context of AI in health; therefore, several limitations should be noted. First, there are relatively few published interventional trials in the field of AI for health care; therefore, the discussions
and decisions made during this study were not always supported by existing examples of completed trials. This arises from our stated aim of addressing the issues of poor reporting in this field as early as possible, recognising
the strong drivers in the field and the specific challenges of study design and reporting for AI. As the science and study of AI evolves, we welcome collaboration with investigators to co-evolve these reporting standards to ensure their continued relevance. Second, the literature search for AI RCTs used terminology such as “artificial intelligence”, “machine learning”, and “deep learning”, but not terms such as “clinical decision support systems” or “expert systems”, which were more commonly used in the 1990s for technologies underpinned by AI systems and share risks similar to those of recent examples.61 It is likely that such systems, if published today, would be indexed under “artificial intelligence” or “machine learning”; however, clinical decision support systems were not actively discussed  during this consensus process. Third, the initial candidate-items list was generated by a relatively small group of experts consisting of Steering Group members and additional international experts; however, additional items from the wider Delphi group were taken forward for consideration by the Consensus Group, and no new items were suggested during the consensus meeting or post-meeting evaluation.
As with the CONSORT statement, the CONSORT-AI extension is intended as a minimum reporting guidance, and there are additional AI-specific considerations for trial reports that may warrant consideration (appendix pp 2–5). This extension is aimed particularly at investigators and readers reporting or appraising clinical trials; however, it may also serve as useful guidance for developers of AI interventions in earlier validation stages of an AI system. Investigators seeking to report studies developing and validating the diagnostic and predictive properties of AI models should refer to TRIPOD-ML (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis–Machine Learning) and STARD-AI (Standards for Reporting of Diagnostic Accuracy Studies–Artificial Intelligence), both of which are currently under development.32,62 Other potentially relevant guidelines, which are agnostic to study design, are registered with the EQUATOR Network.63 The CONSORT-AI extension is expected to encourage careful early planning of AI interventions for clinical trials and this, in conjunction with SPIRIT-AI, should help to improve the quality of trials for AI interventions. The development of the CONSORT-AI guidance does not include additional items within the discussion section of trial reports. The guidance provided by CONSORT 2010 on trial limitations, generalisability, and interpretation was deemed to be translatable to trials for AI interventions.
There is also recognition that AI is a rapidly evolving field, and there will be the need to update CONSORT-AI as the technology, and newer applications for it, develop. Currently, most applications of AI involve disease detection, diagnosis, and triage, and this is likely to have influenced the nature and prioritisation of items within CONSORT-AI. As wider applications that utilise “AI as therapy” emerge, it will be important to continue to evaluate CONSORT-AI in light of such studies.
Additionally, advances in computational techniques and the ability to integrate them into clinical workflows will bring new opportunities for innovation that benefits patients. However, they may be accompanied by new challenges around study design and reporting. In order to ensure transparency, minimise potential biases, and promote the trustworthiness of the results and the extent to which they may be generalisable, the SPIRIT-AI and
CONSORT-AI Steering Group will continue to monitor the need for updates."
What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use,"Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians’ trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the eld suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians’ trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.","This work documents the ongoing challenge of translation of clinical ML with a particular focus on explainability through the eyes of end users. We demonstrate how clinicians' views sometimes di?er from existing notions of explainability in ML, and propose strategies for enhancing buy-in and trust by focusing on these needs. In light of the objectives highlighted in Section 1, we survey clinicians with diverse specialties and identify when explainability methods can assist in enhancing clinicians' trust in ML models. Our research survey involved creating hypothetical scenarios of deploying a machine learning based predictive tool to carry out specific tasks in the ICU and the ED respectively. We demonstrate that by accounting for target stakeholders, even though the explainability task in clinical settings is significantly diverse, but it can be codi?ed into speci?c technical challenges. We further outline the need to evaluate clinical explainability methods rigorously under the proposed metrics in Section 3.3. To the best of our knowledge, this is the first attempt at involving ICU and ED stakeholders to identify targeted clinical needs and evaluating it against general machine learning literature in this field. Some of our non{technical observations, as highlighted in the qualitative synopsis in Section 3.1 and Appendix B are corroborated to some extent by the observations of Elish (2018) who followed the development and deployment of a machine learning based sepsis risk detection tool in a clinical setting. Our work however is far more general, as we concretely map conclusions from clinical surveys to prominent gaps in explainable ML literature as it pertains to elective clinical practice. Limitations of our methods are highlighted below."
Ethical limitations of algorithmic fairness solutions in health care machine learning,"Artificial intelligence has exposed pernicious bias within health data that constitutes substantial ethical threat to the use of machine learning in medicine.1,2 Solutions of algorithmic fairness have been developed to create neutral models: models designed to produce nondiscriminatory predictions by constraining bias with respect to predicted outcomes for protected identities, such as race or gender.3 These solutions can omit such variables from the model (widely regarded as ineffective and can increase discrimination), constrain it to ensure equal error rates across groups, derive outcomes that are independent of one’s identity after controlling for the estimated risk of that outcome, or mathematically balance benefit and harm to all groups. The temptation to engineer ethics into algorithm design is immense and industry is increasingly pushing these solutions. In the health-care space, where the stakes could be higher, clinicians will integrate these models into their care, trusting the issue of bias has been sufficiently managed within the model. However, even if well rec ognised technical challenges are set aside,3,4 framing fairness as a purely technical problem solvable by the inclusion of more data or accurate computations is ethically problematic. We highlight challenges to the ethical and empirical efficacy of solutions of algorithmic fairness that show risks of relying too heavily on the so called veneer of technical neutrality,5 which could exacerbate harms to vulnerable groups. Historically, algorithmic fairness has not accounted for complex causal relationships between biological, environmental, and social factors that give rise to differences in medical conditions across protected identities. Social determinants of health play an important role, particularly for risk models. Social and structural factors affect health across multiple intersecting identities,4 but the mechanism(s) by which social determinants affect health outcomes is not always well understood. Additional complications flow from the reality that difference does not always entail inequity. In some instances, it is appropriate to incorporate differences between identities because there is a reasonable presumption of causation. For example, biological differences between genders can affect the efficacy of pharmacological compounds; incorporating these differences into prescribing practices does not make those prescriptions unjust. However, incorporating noncausative factors into recommendations can propagate unequal treatment by reifying extant inequities and exacerbating their effects. We should not allow models to promote different standards of care according to protected identities that do not have a causative association with the outcome. Nevertheless, in many cases it is difficult to distinguish between acknowledging difference and propagating discrimination. Given the epistemic uncertainty surrounding the association between protected identities and health outcomes, the use of fairness solutions can create empirical challenges. Consider the case of heart attack symptoms among women.6 The under-representation of women (particularly women of colour) in research of heart health is now well recognised as problematic and directly affected uneven improvements in treatment of heart attacks between women and men. By tailoring health solutions to the majority (ie, referent) group, we inevitably fall short of helping all patients. Many algorithmic fairness solutions, in effect, replicate this problem by trying to fit the non-referent groups to that of the referent,7,8 ignoring heterogeneity and assuming that the latter represents a true underlying pattern. Another concern is disconnection between the patient’s clinical trajectory and the fair prediction. Consider the implications at the point-of-care, a model, corrected for fairness, will predict that a patient will respond to a treatment as a patient in the referent class would. What happens when that patient does not have the predicted response? This difference between an idealised model and non-ideal, real-world behaviour affects metrics of model performance (eg, specificity, sensitivity) and clinical utility in practice. Moreover, the model has made an ineffective recommendation that could have obscured more relevant interventions to help that patient. If clinicians and patients believe that the mode has been rendered neutral, then any discrepancies between model prediction and the patient’s true clinical state might be impossible to interpret. The result would be to camouflage persistent health inequalities. As such, fairness, operationalised by output metrics alone, is insufficient; real-world consequences should be carefully considered.","Bias and ineffective solutions of algorithmic fairness threaten the ethical obligation to avoid or minimise harms to patients (non-maleficence; panel). Nonmaleficence demands that any new clinical tool should be assessed for patient safety. For health-care machine learning, safety should include awareness of model limitations with respect to protected identities and social determinants of health. Considerations of justice requires that implemented models do not exacerbate pernicious bias. There is a movement toward developing guidelines of standardised reporting for machine learning models of health care9 and their prospective appraisal through clinical trials.10 Appraisal is particularly important in determining the real-world implications for vulnerable patients when machine learning models are integrated into clinical decision making. Clinical trials are essential to providing a sense of the model’s performance for clinicians to make informed decisions at the point-of-care through awareness of identityrelated model limitations. Some computations can promote justice through revealing unfairness and refining problem formulation. Obermeyer and colleagues2 show how calibration can reveal unfairness in a seemingly neutral task through which choice of label can dictate how heavily bias is incorporated into predictions. It might be that no way exists to define a purely neutral problem; some clinical prediction tasks might be more susceptible to bias than others. Transparency at multiple points in the pipeline of machine learning including development, testing, and implementation stages can support interpretation of model outputs by relevant stakeholders (eg, researchers, clinicians, patients, and auditors). Combined with adequate documentation of outputs and ensuing decisions, these steps support a strong accountability framework for point-of-care machine learning tools with respect to safety and fairness to patients. Problem formulation with respect to bias will often be value-laden and ethically charged. Ethical decision making highlights the importance of converging knowledge sources to inform a given choice. Important stakeholders could include affected communities, cultural anthropologists, social scientists, and race and gender theorists. Computations alone clearly cannot solve the bias problem, but they could be offered a place within a broader approach to addressing fairness aims in healthcare. Algorithmic fairness could be necessary to fix statistical limitations reflective of perniciously biased data, and we encourage this work. The worry is that suggesting these as solutions risks unintended harms.5 Bias is not new; however, machine learning has potential to reveal bias, motivate change, and support ethical analysis while bringing this crucial conversation to a new audience. We are at a watershed moment in health care. Ethical considerations have rarely been so integral and essential to maximising success of a technology both empirically and clinically. The time is right to partake in thoughtful and collaborative engagement on the challenge of bias to bring about lasting change."
Patient safety and quality improvement: Ethical principles for a regulatory approach to bias in healthcare machine learning,"Accumulating evidence demonstrates the impact of bias that reflects social inequality on the performance of machine learning (ML) models in health care. Given their intended placement within healthcare decision making more broadly, ML tools require attention to adequately quantify the impact of bias and reduce its potential to exacerbate inequalities. We suggest that taking a patient safety and quality improvement approach to bias can support the quantification of bias-related effects on ML. Drawing from the ethical principles underpinning these approaches, we argue that patient safety and quality improvement lenses support the quantification of relevant performance metrics, in order to minimize harm while promoting accountability, justice, and transparency. We identify specific methods for operationalizing these principles with the goal of attending to bias to support better decision making in light of controllable and uncontrollable factors.","To operationalize patient safety and quality improvement, the following practices can be adopted by healthcare institutions and regulators who embrace these ethical principles for the delivery of ML-assisted health care.
Data collection:  Whether bias is believed to influence health outcomes within a particular model or not, developers should maintain statistics on the characteristics of the population on which the model was developed. These variables should, at minimum, include the so-called protected characteristics under civil and human rights legislation (ie, gender, race, ethnicity, age, socioeconomic status).24 These variables may or may not be included in the model itself but can support a post hoc evaluation of systematic differences in predictions. This information can be used to help researchers determine whether the model can generalize appropriately, whether a distinct model may be needed for some subgroups of patients, or to explore unappreciated causal factors that relate to subgroup differences.
Auditing and prospective evaluation: Model auditing is consistent with a focus on continuous quality improvement, and should collect and retain evidence that the ML-based decision-making tool is safe to use in the intended population. Audits should include (1) information on the reliability and validity of the target label, (2) evidence of sufficient representation of subgroups in the population for which the model is intended, (3) data collection errors stratified by subgroup, and (4) assessment of potentially confounding factors. In the event that such benefits are restricted to certain groups, recommendations regarding the subpopulations in which a tool may be safely used are essential. At a systemic level, care should be taken to ensure that benefits of predictive models do not unfairly accrue to privileged populations. Any reported disparity in outcome or treatment path determined by such models should accompany logged clinical justifications. 
Models approved at the regulatory stage must also be evaluated locally. Performance of ML models is well recognized to vary across sites due to a number of factors25; thus, the need for local validation through a prospective, noninterventional silent period is apparent.26 Techniques to investigate hidden stratification effects can reveal noncausative but correlative features that result in notable differences in performance accuracy27—such techniques may support identification of bias-related effects. Reporting of relevant subgroup differences in model performance is especially important for clinical trials involving AI. This information aids healthcare decision-makers in determining the suitability of a model for the population served by their institution, and for the clinician determining how much to rely on a model's output with respect to an individual patient.17
Practice guidance: Careful consideration must be given to determine how information about potential bias is included in the point-of-care interpretation of model outputs. Physicians have a fiduciary duty to continually act in their patient’s best interests and obtain informed consent from capable patients or their surrogate decision makers, which includes offering them all relevant information to support decision making. Incorporating understanding of potential bias in communicating the model outputs to the patients can enhance trustworthiness. In collaboration with stakeholders, ML developers should consider relevant differences in model performance and identify users’ needs in deciding what information to present to human decision makers.
Oversight: Oversight is central to any quality improvement effort. Model evaluations may be performed regularly to prevent the risk of model decay or the influence of feedback loops, which may worsen errors for specific populations over time.28 Continued quality improvement efforts independent of bias alone should be conducted to ensure the performance of a model is maintained. These evaluations are particularly important as populations shift, practice changes occur, and new policies are implemented. Ensuring these evaluations keep track of subgroup-specific effects can continue to inform the model’s ongoing use.
One of the key remaining questions facing ML is where oversight should occur, and by whom. At the present time, conversations about a feasible, long-term oversight strategy are in flux. In the long term, as these tools become ubiquitous across health care, the need for robust, consistent standards for local review will become more pronounced. In the interim, it is incumbent on ML researchers to retain records of model performance metrics and conduct evaluations. This means that there will have to be expertise on staff in hospitals where the tools are deployed that will attend to the fairness, efficacy, and safety of these algorithms. Clinicians can and should demand these evaluations when considering the adoption and use of models in clinical populations. Hospital decision makers should be critically evaluating these statistics to determine applicability to the population they serve."
Clinical research underlies ethical integration of healthcare artificial intelligence,"Despite the promise of machine learning (ML), only a small fraction of developed models are successfully implemented at the point of care, a phenomenon called the ‘AI chasm’1. Crossing this chasm requires careful consideration of the most appropriate method of evaluation and oversight. The traditional method of translating the insights of basic science into medical practice — clinical research — has received little attention in the context of healthcare ML due, in part, to genuine uncertainty about whether its application in medicine requires validation via a clinical-research framework. It is now clear that in silico (computational) validation is not sufficient for successful clinical deployment. Accurate predictions within a retrospective, static database do not translate into accurate predictions of health events in a non-stationary clinical context that includes shifting patient trends, operational and/or procedural changes, and practice updates2,3. There is now widespread agreement within the healthcare ML community that additional validation is needed. We propose a three-stage process for the evaluation and validation of ML models into clinical care, subject to ethics review and oversight (Table 1).","With novel technologies, there is often a question of whether they require entire new oversight mechanisms. Since access to data within traditional research ethics and privacy paradigms must be carefully justified with respect to the question or hypotheses under study, ML researchers have struggled to justify their data requirements and research methods to ethics review boards. This has been a cause for delay and frustration. Further controversy is driven by features of ML applications that appear to fall in a ‘gray zone’ between research and quality improvement11. Since quality improvement is typically exempt from research ethics oversight, this view has led to minimal oversight and variable levels of review within institutions11. But quality-improvement studies are appropriate only when the benefits and risks of the interventions under study are already known; however, before prospective evaluation, the risks and benefits of an ML model are unknown. Grounding ML in the research-ethics pathway achieves two crucial goals: a pathway for progress in clinical ML becomes readily available; and trust is fostered by alignment with established legal and ethical standards for clinical research. This pipeline has been implemented at scale at The Hospital for Sick Children, with the changes required for ML work described in Table 1. Another key component of scientific (and ethical) value is consistent and fulsome reporting. The recently published guidelines of CONSORT-AI (Consolidated Standards of Reporting Trials-AI)12 and SPIRIT-AI (Standard Protocol Items: Reporting for Interventional Trials-AI)13 aim to enhance the scientific reproducibility of healthcare AI research. Enhancing the standardization and reliability of these trials can promote effective and patient-focused clinical translation. Closing the AI chasm is not just a technical matter. If the goal of implementing ML tools at the point of care is to enhance patient care, then it is essential to maintain a high standard of empirical validation and ethical review to protect the rights and interests of patients."
"Evidence, ethics and the promise of artificial intelligence in psychiatry","Researchers are studying how artificial intelligence (AI) can be used to better detect, prognosticate and subgroup diseases. The idea that AI might advance medicine’s understanding of biological categories of psychiatric disorders, as well as provide better treatments, is appealing given the historical challenges with prediction, diagnosis and treatment in psychiatry. Given the power of AI to analyse vast amounts of information, some clinicians may feel obligated to align their clinical judgements with the outputs of the AI system. However, a potential epistemic privileging of AI in clinical judgements may lead to unintended consequences that could negatively affect patient treatment, well-being and rights. The implications are also relevant to precision medicine, digital twin technologies and predictive analytics generally. We propose that a commitment to epistemic humility can help promote judicious clinical decision-making at the interface of big data and AI in psychiatry.","In this paper, we argued that a potential epistemic privileging of AI in clinical judgements may lead to unintended consequences. The key to clinical decision-making grounded in epistemic humility requires clinicians to critically consider what goals are trying to be achieved by relying on the AI output before potentially relevant and legitimate perspectives offered by patients are deprioritised in clinical judgements. It is imperative that health systems that adopt AI-based predictions do not prioritise these outputs to the exclusion of SDM and incorporation of patient
experiential knowledge. In making our arguments, we are not privileging human clinical judgement over AI, claiming that AI is superior to clinical decision-making in psychiatry, or arguing categorically that AI does not have a role in augmenting clinical decision-making in psychiatry. Rather, we are concerned with AI’s potential place on the epistemic hierarchy in clinical decision-making. We argue that an uncritical acceptance of AI as being superior to humans in terms of accuracy, reliability and knowledge risks entrenching many of the inequities people living with mental illnesses have experienced for centuries. AI developers ought to be aware of the potential unintended consequences of their algorithms,74 and together with clinicians should work collaboratively with people with mental illness to develop–and access–the resources to understand and communicate their experiences of mental illness in the context of AI. This will help support health systems and clinicians commiting to epistemic humility in practice."
Tackling bias in AI health datasets through the STANDING Together initiative,"As of June 2022, a wide range of Artificial Intelligence (AI) as a Medical Device (AIaMDs) have received regulatory clearance internationally, with at least 343 devices cleared by the US Food and Drug Administration (FDA)1. Despite the
enormous potential of AIaMDs, their rapid growth in healthcare has been accompanied by concerns that AI models may learn biases engrained in medical practice and exacerbate health inequalities. This has been exemplified by several AI systems that have shown the ability of algorithms to systematically misrepresent and exacerbate health problems in minority groups2,3. This raises concerns that, without appropriate safeguarding, AI models may perpetuate existing health inequality and mistrust.
Tackling bias in AI requires a multifaceted approach. A recent report by the US National Institute of Standards and Technology on bias in AI emphasized that algorithmic development
does not occur by engineering decisions alone, but embeds a myriad of values and behaviors within the data and the humans who interact with them. The report calls for a sociotechnical approach that considers how different biases interact and the social contexts within which AI systems are built and used4. Although there is an expanding field of research dedicated to fairness in machine learning, many AIaMDs that receive regulatory clearance have not appropriately accounted for biases that disadvantage certain populations. There are also ethical challenges around algorithmic fairness methods (computational techniques that seek to ensure outputs are not unjustifiably influenced by bias), given that these methods are aimed at making predictions fair, rather than enabling the fair treatment of individuals5. Furthermore, current approaches to satisfy regulatory requirements are focused on aggregate-level performance, which can mask stratification across subpopulations.
One major source of bias is the data that underpin AI systems. It is often necessary to train models with large quantities of data, which means datasets are often sourced to prioritize sample size. There are concerns that many health datasets do not adequately represent minority groups; however, the extent of this problem is unknown because many datasets do not provide demographic information, such as on ethnicity and race. Publicly available datasets for skin cancer and eye imaging have shown inconsistent and incomplete demographic reporting, and are disproportionately collected from a small number of high-income countries6,7. For skin cancer datasets, the reporting of key demographic information, such as ethnicity and skin tone, even when clinically relevant, was only present in 2% of datasets7. Under-representation in datasets can affect the fairness of AI systems by two principal means. During AI development, under-representation within training datasets can negatively affect model performance for under-represented groups3. A lack of diversity within the training data risks poor generalizability of model performance after deployment. During evaluation, under-representation within test datasets increases the uncertainty of performance in that group due to small sample sizes, which reduces the likelihood of detecting under-performance. Therefore, under-representation not only creates models that under-perform within minority populations, but also hampers the ability to detect this bias. Furthermore, under-representation in datasets may result in exclusion of populations from the intended use altogether, thereby creating AI systems licensed for only certain groups within society. Even when datasets are inclusive, additional issues can compound bias. Structural inequities can manifest in datasets through the actions of clinical and data curation teams, who are responsible for recording, selecting, labelling and aggregating data, based on assumptions that reflect hegemonic social attitudes. Addressing the consequences of structural biases requires a wider consideration of the dataset: how and why it was created; the setting in which the data was collected and by whom; the extent to which the data reflect broader structural biases and axes of injustice; the inclusion and exclusion criteria; and how measurements, observations and labels were constructed. These concerns have motivated calls for better documentation practices and the creation of tools such as ‘Datasheets for Datasets’ and ‘Healthsheets’8,9.
","The aforementioned problems are becoming increasingly recognized by regulators of medical devices. In October 2021, The US FDA, Health Canada and the UK Medicines and Healthcare products Regulatory Agency (MHRA) jointly published 10 guiding principles for good machine learning practice. This specifically states that data should be representative of the intended population in order to “manage any bias, promote appropriate and generalizable performance across the intended patient population, assess usability and identify circumstances where the model may underperform”10. Commitment to identify and mitigate bias by medical regulators is an important step in the right direction; however, there is a lack of evidence that these principles are adopted by AIaMD manufacturers. Without specific consensus on how to assess the appropriateness of datasets, it is unclear what constitutes best practice regarding the use of health data in AI to promote fairness and equity.
To tackle this problem, we are announcing the development of the STANDING Together (standards for data diversity, inclusivity and generalisability) initiative. This is an international, consensus-based initiative that aims to develop recommendations for the composition (who is represented) and reporting (how they are represented) of datasets that underpin medical AI systems. We will engage patients and the public, clinicians and academic researchers across biomedical, computational and social sciences, industry experts, regulators and policy-makers. The standards will represent the culmination of a multiphase evidence generation process, which consists of: dataset mapping reviews to assess limitations in health datasets across different diseases with regard to diversity and inclusivity; interviews with dataset curators to explore the barriers and challenges to ensuring diversity and inclusivity within health datasets; a modified Delphi consensus study to finalize the content that will feature in these recommendations; and an extensive multi-stakeholder piloting phase.
The resulting standards will support informed decision-making for those who strive to engineer and implement fair and safe AI systems in healthcare. STANDING Together will be the first in a line of work through which stakeholders can determine what demographic data is collected and how it is represented in datasets. The findings will motivate curators of health datasets to prioritize diversity and inclusiveness as we all seek to build and invest in health datasets of the future. We hope that this initiative will enable the availability of more inclusive data to promote responsible AI in healthcare, and in the long term, better health outcomes for all. The modified Delphi consensus study will begin in September 2022 and the final standards will be published in 2023. We welcome those with expertise in AI, health data science and health inequalities to participate (through https://www.datadiversity.org/delphi or by contacting contact@datadiversity.org)."
Patient and caregiver perspectives on early identification for advance care planning in primary healthcare settings,"Background

As part of a broader study to improve the capacity for advance care planning (ACP) in primary healthcare settings, the research team set out to develop and validate a computerized algorithm to help primary care physicians identify individuals at risk of death, and also carried out focus groups and interviews with relevant stakeholder groups. Interviews with patients and family caregivers were carried out in parallel to algorithm development and validation to examine (1) views on early identification of individuals at risk of deteriorating health or dying; (2) views on the use of a computerized algorithm for early identification; and (3) preferences and challenges for ACP.

Methods

Fourteen participants were recruited from two Canadian provinces. Participants included individuals aged 65 and older with declining health and self-identified caregivers of individuals aged 65 and older with declining health. Semi-structured interviews were conducted via telephone. A qualitative descriptive analytic approach was employed, which focused on summarizing and describing the informational contents of the data.

Results

Participants supported the early identification of patients at risk of deteriorating health or dying. Early identification was viewed as conducive to planning not only for death, but for the remainder of life. Participants were also supportive of the use of a computerized algorithm to assist with early identification, although limitations were recognized. While participants felt that having family physicians assume responsibility for early identification and ACP was appropriate, questions arose around feasibility, including whether family physicians have sufficient time for ACP. Preferences related to the content of and approach to ACP discussions were highly individualized. Required supports during ACP include informational and emotional supports.

Conclusions

This work supports the role of primary care providers in the early identification of individuals at risk of deteriorating health or death and the process of ACP. To improve ACP capacity in primary healthcare settings, compensation systems for primary care providers should be adjusted to ensure appropriate compensation and to accommodate longer ACP appointments. Additional resources and more established links to community organizations and services will also be required to facilitate referrals to relevant community services as part of the ACP process.","This study explored patient and caregiver perspectives with regard to the identification of individuals at risk of deteriorating health or death earlier on in their disease trajectory in PHC settings, and the initiation of ACP by PCPs. Participants in our study regarded ACP favorably and agreed that the early identification of at-risk individuals for the purposes of ACP initiation was appropriate. They were also supportive of the use of a computerized algorithm to assist with early identification, although limitations were recognized. PHC was generally perceived as a suitable setting for both early identification and ACP initiation to occur.
In order for ACP to be meaningful, individuals at risk of dying must be identified earlier in their disease trajectory, so that discussions can take place to support the development and implementation of advance care plans that align with the needs and wishes of the patient. Unfortunately, physicians have consistently reported hesitancy to initiate conversations related to planning for EOL based on the perception it may be too upsetting or stressful for patients [44–46] or cause them to lose hope [33]. While some research has shown that some patients are in fact reluctant to discuss death and/or engage in ACP [33, 47], this is not always the case [48]. The majority of participants in our study viewed early identification in PHC settings positively, as a means of reducing stress for patients and families and allowing patients to focus on what matters most to them, although there was also recognition that discomfort with discussing death may be a barrier to ACP for some individuals. These findings, together with existing literature, suggest that physicians may be overestimating the negative impacts of raising the topic of ACP with patients.
One of the reported barriers to ACP is that providers have difficulty identifying patients who are at risk of dying in the near future [23, 24]. As such, the identification of those approaching the EOL subsequently occurs too late for proactive needs and desired care plans to be put into place and carried out [24]. To facilitate early identification, we explored the use of an algorithm based on EMR data to routinely and systematically identify patients who are at risk of deteriorating health or death in order to trigger the initiation of ACP. While the feasibility of this approach has been demonstrated elsewhere by Mason et al. [49], similar efforts have faced issues related to public perception. Specifically, efforts in the United Kingdom to assist PCPs in identifying patients expected to die within 1 year [36] were met with claims from the media that ‘death lists’ were being created and for the purposes of rationing health care, which caused unnecessary distress among patients and families [50, 51]. The participants in our study did not express similar concerns, which may be indicative of different societal attitudes about the use of EMR data, but may also be related to the fact that they received substantial information about the study and its aims prior to completing the interview, and therefore understood that the intended aim of the algorithm was to improve patient care. In other words, the interview itself may have served as an intervention and the views expressed may not accurately reflect how individuals would react to the use of an EMR-based algorithm and/or being identified as at risk of death. In practice, the implementation of an EMR-based algorithm to identify individuals as being at risk of death in PHC settings must be accompanied by a communication strategy aimed at the general public that clearly communicates the objectives of the routine and systematic identification of individuals at risk of death: to facilitate ACP initiation and ensure patient-centered care as individuals approach the end of life. Such a campaign would be integral to maintaining trust between patients and physicians, and mitigating negative media attention.
Regarding ACP conversations, our findings show that individual preferences vary greatly with respect to the style of approach, explicit mention of death or dying, involvement of family members, and the type of supports required. This is consistent with other studies [47, 52] that have reported highly personalized ACP preferences, and highlights the need for an approach that is tailored to each individual patient. This also points to the value of having ACP led by someone who is familiar with the patient and has established a rapport with them. Participants were generally in agreement that having their PCPs initiate ACP would be appropriate, but questioned the feasibility of having them take on this role given physicians’ workload and time constraints—challenges that have been echoed by physicians themselves [40, 53]. In a companion study by the authors [40], stakeholders (including 29 health care providers) felt that while ACP is incredibly important, physicians do not have the time to engage in the multiple, in-depth discussions required. This issue may be related to the fact that many family physicians work in solo practices or physician-only practices without other professionals (e.g., nurses, nurse practitioners, social workers, etc.) to help support this work. Perceived lack of time may also be related to how physicians are compensated for the care they provide. The majority of family physicians in Canada are paid through a fee-for-service model [54], which promotes shorter visits. Moreover, not all provincial/territorial fee schedules include codes for ACP, meaning physicians in some jurisdictions cannot be compensated for ACP. Time availability and funding/reimbursement have also been identified as barriers to advance care planning in primary care settings in other jurisdictions [55, 56]. An important step to increasing ACP, particularly in primary care settings, is ensuring that funding mechanisms are in place to encourage providers to the take the necessary time to meaningfully engage in ACP with patients and family/friend caregivers [7, 57].
There are several limitations to the work presented here. First, this study included patients and family/friend caregivers from only two provinces (Nova Scotia and Ontario) in Canada. Secondly, not all participants had personal experience with or a detailed understanding of ACP prior to their participation in the study. For some individuals, this study was the first time they had heard of ACP or been asked to reflect on their preferences; thus, their responses may have been influenced by the definition of ACP that the research team provided. Similarly, the EMR-based algorithm was not developed at the time of this study, so participants were asked to respond based on a description of the algorithm provided by the research team. Nonetheless, the findings reported here provide unique insights for improving the capacity of PCPs to initiate ACP with patients who may benefit from it."
Prediction of unplanned 30-day readmission for ICU patients with heart failure,"Background: Intensive Care Unit (ICU) readmissions in patients with heart failure (HF) result in a significant risk of death and financial burden for patients and healthcare systems. Prediction of at-risk patients for readmission allows for targeted interventions that reduce morbidity and mortality.
Methods and results: We presented a process mining/deep learning approach for the prediction of unplanned 30-day readmission of ICU patients with HF. A patient’s health records can be understood as a sequence of observations called event logs; used to discover a process model. Time information was extracted using the DREAM (Decay Replay Mining) algorithm. Demographic information and severity scores upon admission were then combined with the time information and fed to a neural network (NN) model to further enhance the prediction efficiency. Additionally, several machine learning (ML) algorithms were developed to be used as the baseline models for the comparison of the results.
Results: By using the Medical Information Mart for Intensive Care III (MIMIC-III) dataset of 3411 ICU patients with HF, our proposed model yielded an area under the receiver operating characteristics (AUROC) of 0.930, 95% confidence interval of [0.898–0.960], the precision of 0.886, sensitivity of 0.805, accuracy of 0.841, and F-score of 0.800 which were far better than the results of the best baseline model and the existing literature.
Conclusions: The proposed approach was capable of modeling the time-related variables and incorporating the medical history of patients from prior hospital visits for prediction. Thus, our approach significantly improved the outcome prediction compared to that of other ML-based models and health calculators.","Existing model compilation summary
Several methods have been concurrently developed to predict unplanned 30-day readmission of the ICU patients with HF aiming to benefit both health care providers and the patients. Table 6 shows the existing models that have been used MIMIC-III dataset to predict unplanned 30-day readmission of ICU patients with HF.In this study, a process mining/deep learning technique was investigated for predicting unplanned 30-day readmission of ICU patients with HF, in which time information associated with the events, severity scores, and demographics were fed into a NN model.
The effectiveness of our developed approach outperformed the best results of the existing literature in terms of the AUROC value proposed by Lin et al. [11]. The efficacy of our approach was demonstrated by a substantial improvement of?+?10% on AUROC.
In addition, the presented results indicated?+?6% and?+?7% improvements in sensitivity and F-score metrics, respectively, compared to the best sensitivity and F-score values reported in the literature by Huang et al. [39].
Although the existing proposed methodologies in the literature were successful in predicting unplanned 30-day readmission of ICU patients with HF, they possessed several drawbacks. First, most of the existing models did not use the time-series features, and to the best of our knowledge, none of them incorporated time information associated with the variables in their predictive modeling that could lead to significant information loss and poor performance accordingly [40, 41].
Furthermore, the proposed approach was a process mining/deep learning approach that illustrated the careflows of patients through a process model. As a result, our framework was more interpretable compared to the existing methods, which is significant for clinical applications [42].
Moreover, the health calculators that computed outcomes based on the severity scores ignored the past medical history of the patients which could have a significant impact on the likelihood of unplanned readmission.
Our proposed approach had several advantages over prior research papers which are as follows: (a) Process mining approach yielded a comprehensive analysis of careflows of patients through a process model which is understandable and can easily be interpreted compared to ML techniques. The process model provided a map that represented the possible diagnosis, procedures, performed services, laboratory measurements, and more, that happened to a patient. Additionally, it eased the interpretability of a model prediction. An example of a process model can be found in the existing research paper [20, 43] (b) The EHR can be directly used as inputs to our proposed approach without any computationally expensive preprocessing steps. (c) The process mining/deep learning framework was capable of modeling the time-related variables and incorporating the medical history of the patients from the previous hospital visits in the prediction algorithm unlike ML-based models and health calculators.
Study limitations: The proposed approach had some limitations. Even though MIMIC-III is a comprehensive database and many recent research projects have been using the same database for their experiments [21, 44], the data is almost 18 years old. Thus, we suggest that a newer multihospital database such as the Nationwide Readmission Database (NRD) [23, 45] should be used in the future to externally validate our proposed model and its results. Also, MIMIC-III readmission information is limited to several facilities, and for the cases that the patients are admitted to other facilities, the readmission information is not available, hence, it may bias the results. Since this approach was a process mining/deep learning approach, the availability of the past hospital visits of the patients was essential. This approach was not useful for patients whose admission histories were not available. However, this limitation can be overcome if the history of patients could be exchanged through a network system between health care providers. Application Program Interfaces (APIs) and similar innovations hold promise that soon these drawbacks can seemingly be curtailed.
Moreover, in our model development, the train and validation datasets were used to build the model. The test dataset was set aside from the beginning and only used to evaluate the performance of the model. The train, validation, and test sets were coming from the MIMIC-III dataset. However, using an independent dataset from a different system would be beneficial to test the performance of the model [46], which provides room for future work.
Conclusions: A process mining/deep learning approach to model EHR data of ICU patients with HF to predict unplanned 30-day readmission provided significant improvement in outcome prediction observed and compared to the results of the baseline ML models and existing literature. This improvement could be due to the capability of the process mining approach of modeling time information related to the variables and incorporating past hospital visits of the patients for prediction. Our framework can assist clinicians in identifying patients with a higher risk of unplanned 30-day readmission. Discharge planners may find this prediction tool useful in determining when a patient is safe to be discharged from the hospital and to guide post-discharge outpatient management. Future studies may validate the proposed approach using datasets from other healthcare systems or investigate its use for different diseases and outcomes. Moreover, the MIMIC-III dataset contains useful information such as clinical notes, and images, which can be fed to the models as inputs. Therefore, it potentially makes room for further research."
"Leveraging Responsible, Explainable, and Local Artificial Intelligence Solutions for Clinical Public Health in the Global South","In the present paper, we will explore how artificial intelligence (AI) and big data analytics (BDA) can help address clinical public and global health needs in the Global South, leveraging and capitalizing on our experience with the ""Africa-Canada Artificial Intelligence and Data Innovation Consortium"" (ACADIC) Project in the Global South, and focusing on the ethical and regulatory challenges we had to face. ""Clinical public health"" can be defined as an interdisciplinary field, at the intersection of clinical medicine and public health, whilst ""clinical global health"" is the practice of clinical public health with a special focus on health issue management in resource-limited settings and contexts, including the Global South. As such, clinical public and global health represent vital approaches, instrumental in (i) applying a community/population perspective to clinical practice as well as a clinical lens to community/population health, (ii) identifying health needs both at the individual and community/population levels, (iii) systematically addressing the determinants of health, including the social and structural ones, (iv) reaching the goals of population's health and well-being, especially of socially vulnerable, underserved communities, (v) better coordinating and integrating the delivery of healthcare provisions, (vi) strengthening health promotion, health protection, and health equity, and (vii) closing gender inequality and other (ethnic and socio-economic) disparities and gaps. Clinical public and global health are called to respond to the more pressing healthcare needs and challenges of our contemporary society, for which AI and BDA can help unlock new options and perspectives. In the aftermath of the still ongoing COVID-19 pandemic, the future trend of AI and BDA in the healthcare field will be devoted to building a more healthy, resilient society, able to face several challenges arising from globally networked hyper-risks, including ageing, multimorbidity, chronic disease accumulation, and climate change.","In the present paper, we explored how AI and BDA can help address clinical public and global health needs in the Global South, leveraging and capitalizing on our experience with the ACADIC Project in the Global South, and focusing on the ethical and regulatory challenges we had to face. Whilst clinical public health is at the intersection of clinical medicine and public health, clinical global health is the practice of clinical public health in the Global South. As such, clinical public and global health represent vital approaches, instrumental in combining a community/population perspective with clinical practice, identifying health needs, systematically addressing the determinants of health, better coordinating and integrating the delivery of healthcare provisions, reaching the goals of population’s health and well-being, strengthening health promotion, health protection, and health equity, and closing disparities and gaps. Clinical public and global health are called to respond to the more pressing healthcare needs and challenges of our contemporary society, for which AI and BDA can help unlock new options and perspectives. In the aftermath of the still ongoing COVID-19 pandemic, the future trend of AI and BDA in the healthcare field will be devoted to building a more healthy, resilient society, able to face several challenges arising from globally networked hyper-risks, including aging, multimorbidity, chronic disease accumulation, and climate change. Based on the existing literature and our experience, we identified the following lessons (i) strengthening local research and healthcare capacity in the Global South, (ii) strengthening local epidemic/pandemic management planning, including the establishment and development of networks with simulator users in the Global South, (iii) a need for locally informed models in the Global South, (iv) a need for flexible modeling frameworks to respond rapidly to future emergencies in the Global South, (v) limitations and shortcomings of modeling should be communicated clearly and consistently to end users in the Global South, (vi) systematically monitoring the use and implementation of models in the process of decision-making in the Global South, (vii) a need for strengthening AI- and Big Data-related funding in the Global South, and (viii) a need for strengthening AI- and BDA-modeling capacity in the Global South. This is in line with other experiences of authors from the Global South, which reaffirm that equity in health is paramount, being “the core value of health for all” [99], as advocated by the Alma-Ata declaration of 1978. This goal is even more ambitious in a world and society where inequities are prevailing and poverty is increasingly widening, with the emergence/re-emergence of new illnesses [100] and the accumulation of chronic diseases [101], which put a significant strain on already weakened healthcare systems [99]. As such, carrying out health research is vital, even though, in some resource-limited areas and environments, it may be particularly complex and difficult, with a limited capacity to undertake and translate research into practice [102]. There, it is fundamental to strengthen and develop sustainable health research capacity in the Global South [102]. Key enablers were identified in (i) ensuring adequate, vigorous funding, (ii) building effective stewardship and establishing equitable and sustainable research collaborations and partnerships, (iii) mentoring and training next-generation researchers, and (iv) effectively linking research-related outcomes to policies and practices [102]. Researchers and scholars from the Global South are terribly under-represented in research [103,104,105,106,107,108,109,110], project leadership and management, authorship, and funding allocations [106], paradoxically even in research concerning the Global South itself [103,104]. Therefore, decolonizing global health research, partnerships, and outreach could mean counteract and mitigate against global health iniquities [105,106,107,108,109]. This “paradigm shift” [106] implies (i) redefining equitable and fair models and best practices of collaboration and (ii) implementing mechanisms to monitor and track progress [106], as well as (iii) recognizing “non-western forms of knowledge and authority”, acknowledging discrimination and disrupting “colonial structures and legacies that influence access to healthcare” [108]. The present study has some limitations, that should be acknowledged. The literature review was narrative rather than systematic and the qualitative part of the study was preliminary. As such, further research is needed. Moreover, only a limited number of countries from the Global South could be included and the findings of the current review may not be generalizable to other realities and contexts from the Global South. Through a narrative literature review and our experience with the ACADIC project, we have learned many lessons that will inform the execution of future initiatives in the Global South. If all these lessons are properly addressed, it will be possible to carry out responsible, inclusive, and impactful AI- and BDA-based research in the healthcare sector that will provide global benefits However, our literature review is narrative and our qualitative analysis preliminary. As such, further research in the field is warranted. More in detail, future studies should conduct high-quality systematic reviews on some (sub-)topics such as clinical public and global health, as well as on digital health in the Global South. Moreover, new interactive, web-based trackers and indicators, such as the “Global Digital Health Index” (GDHI), should be developed and validated to evaluate and monitor the progress toward inclusive, equitable, responsible, and locally championed AI and digital health throughout the world, with a focus on the Global South. "
"Developing the Total Health Profile, a 
Generalizable Unified Set of Multimorbidity Risk Scores Derived From 
Machine Learning for Broad Patient Populations: Retrospective Cohort 
Study","Background: Multimorbidity clinical risk scores allow clinicians to quickly assess their patients' health for decision making, often for recommendation to care management
 programs. However, these scores are limited by several issues: existing multimorbidity scores (1) are generally limited to one data group (eg, 
diagnoses, labs) and may be missing vital information, (2) are usually 
limited to specific demographic groups (eg, age), and (3) do not 
formally provide any granularity in the form of more nuanced 
multimorbidity risk scores to direct clinician attention.ObjectiveUsing
 diagnosis, lab, prescription, procedure, and demographic data from 
electronic health records (EHRs), we developed a physiologically diverse
 and generalizable set of multimorbidity risk scores.MethodsUsing
 EHR data from a nationwide cohort of patients, we developed the total 
health profile, a set of six integrated risk scores reflecting five 
distinct organ systems and overall health. We selected the occurrence of
 an inpatient hospital visitation over a 2-year follow-up window, 
attributable to specific organ systems, as our risk endpoint. Using a 
physician-curated set of features, we trained six machine learning 
models on 794,294 patients to predict the calibrated probability of the 
aforementioned endpoint, producing risk scores for heart, lung, neuro, 
kidney, and digestive functions and a sixth score for combined risk. We 
evaluated the scores using a held-out test cohort of 198,574 patients.ResultsStudy
 patients closely matched national census averages, with a median age of
 41 years, a median income of $66,829, and racial averages by zip code 
of 73.8% White, 5.9% Asian, and 11.9% African American. All models were 
well calibrated and demonstrated strong performance with areas under the
 receiver operating curve (AUROCs) of 0.83 for the total health score 
(THS), 0.89 for heart, 0.86 for lung, 0.84 for neuro, 0.90 for kidney, 
and 0.83 for digestive functions. There was consistent performance of 
this scoring system across sexes, diverse patient ages, and zip code 
income levels. Each model learned to generate predictions by focusing on
 appropriate clinically relevant patient features, such as heart-related
 hospitalizations and chronic hypertension diagnosis for the heart 
model. The THS outperformed the other commonly used multimorbidity 
scoring systems, specifically the Charlson Comorbidity Index (CCI) and 
the Elixhauser Comorbidity Index (ECI) overall (AUROCs: THS=0.823, 
CCI=0.735, ECI=0.649) as well as for every age, sex, and income bracket.
 Performance improvements were most pronounced for middle-aged and 
lower-income subgroups. Ablation tests using only diagnosis, 
prescription, social determinants of health, and lab feature groups, 
while retaining procedure-related features, showed that the combination 
of feature groups has the best predictive performance, though only 
marginally better than the diagnosis-only model on at-risk groups.ConclusionsMassive
 retrospective EHR data sets have made it possible to use machine 
learning to build practical multimorbidity risk scores that are highly 
predictive, personalizable, intuitive to explain, and generalizable 
across diverse patient populations.","There is a continued need for an updated clinical score that profiles 
patients based on multimorbidities that are equitable across populations
 and nuanced enough to facilitate precision medicine. To facilitate 
clinical decision making across patient populations, we created an 
automated, generalizable, integrated, multimorbidity risk profile across
 several clinical domains. The THP is composed of cardiovascular, 
respiratory, neuropsychiatric, renal, and gastrointestinal clinical risk
 subprofiles, as well as a sixth score, the THS, representing the 
overall combinatorial risk of the five organ-specific scores. We 
followed ML best practices to train six integrated models on large-scale
 EHR data with the forecasted probability of a risk endpoint, 
organ-specific IP hospital visits, over a 2-year window as the target. 
We chose IP hospital stays as our risk endpoint because reductions in 
overall health, whether due to multiple health conditions [6,25] or aging [26], are associated with increased hospital visits [27,28].
 The primary contribution of this work goes beyond the models themselves
 by matching clinical knowledge to data that are available at scale, 
across a diverse cohort of patients. In our experiments, we found that 
the profile demonstrates high performance in terms of the AUROC on the 
aggregate held-out testing set. Importantly, there was consistent 
performance of this scoring system across sexes, diverse patient ages, 
and income levels. The THS model and each of the component models 
learned to generate predictions by focusing on appropriate, clinically 
relevant patient features. The THP is personalized based on individual 
organ system risk drivers, and visualizations, such as radar plots, can 
be used to facilitate explainability and encourage confidence of 
clinical decision making, providing meaningful feature importance. The 
THS outperformed relevant baselines, specifically the other commonly 
used multimorbidity scoring systems CCI and ECI, for every age, sex, and
 income bracket. Finally, we conducted multiple ablation tests, while 
retaining procedure features, to determine the relative contribution of 
feature groups to the THP. In this experiment, we found that while the 
combined feature set predictive performance outperformed the 
prescription, lab, and SDoH ablations, it was largely similar to the 
diagnosis ablation. However, we hypothesized that we would find larger 
differences in performance among at-risk populations and found, in 
patients with multiple comorbidities and on certain prescriptions, minor
 but consistent increases in predictive power using the combined feature
 set versus the diagnosis feature set, implying that risk prediction is 
improved on more complex patients, given more complex data. As more 
features, including more labs, diagnosis, and prescriptions, are added 
to the THP, future work will more closely examine which demographics 
benefit from it.The THP’s multimorbidity scores can be 
distinguished from traditional multimorbidity scores in three ways: 
First, they are derived from a comprehensive set of diagnostic, 
prescription, lab, and medical procedure data. This is in contrast to 
other multimorbidity scores that use only one set of information, such 
as diagnoses (as is the case with the ECI [10] and the CCI [9]) or prescriptions (such as Rx-Risk [29]).
 Second, these scores were derived from a large and diverse cohort of 
794,294 patients with medical data spanning decades. Third, the THP was 
calculated from patients of both sexes and from across the age spectrum 
(3-90 years), rather than focusing on mostly geriatric populations as 
with traditional multimorbidity. To the best of our knowledge, this is 
the first time that ML was used to integrate multiple types of 
physician-curated clinical information from a large, diverse population 
and produce a multimorbidity score that can help guide patient care 
irrespective of sex and age.As part of the 
overall multimorbidity score in the THP, we calculated robust, 
organ-system-specific scores that provide a more granular picture of 
health. We believe that these organ-specific multimorbidity scores can 
complement existing condition-specific scores in clinical practice by 
providing additional validation for treatment decisions for 
cardiovascular, respiratory, neuropsychiatric, gastrointestinal, and 
renal domains. Along these lines, we note that these disease-specific 
scores often use patient-reported outcomes as part of their input [14], with some even using them exclusively [3].
 Although EHR software systems may have health-based modules to 
automatically compute such scores at the population level, these 
self-reported data are frequently unavailable or unreliable [30],
 making it difficult to scale these scores to the population level with a
 high degree of efficacy. Although the THP cannot be directly compared 
against these alternative risk scores, as they typically focus on 
diagnoses versus emergency events, the fact that the THP consistently 
achieved relatively high AUROCs is nevertheless promising with regard to
 its ability to complement these more specific risk scores. 
Specifically, it says something well established in multimorbidity 
scores but understudied in more specific risk scores: foregoing patient 
input (which typically contains useful information) entirely, in 
exchange for more scalable data, can still lead to strong results. 
Moreover, these alternate risk scores are also typically hyperspecific, 
limiting their clinical utility to a subset of patients—likely due to 
them being built on similarly restricted cohorts (eg the American Heart 
Association pooled cohort equations for atherosclerotic cardiovascular 
disease derived from cohorts exclusively in the age range of 40-79 
years). As our approach has no constraints upon individual patients’ age
 or sex, and are built using a similarly diverse cohort, risk profiles 
that are applicable to a far larger population can be easily derived. Of
 course, we assessed generalizability only among three well-known 
dimensions (age, sex, and income), and there are far more subtle biases 
that have been observed even among established risk scores, such as the 
CHADS2VASC stroke score underestimating risk in patients with chronic 
renal disease [31].
 Further study will be needed to fully examine these sorts of biases in 
our proposed risk models, but even in this case, the scalability of our 
approach will only make this research simpler to perform.LimitationsData-related
 limitations of this study include unmeasured variables and incomplete 
observations. Regarding the former, in this study, we did not include 
lifestyle behavioral data, such as nutrition, smoking, and physical 
activity. Although reporting of these factors is known to be 
inconsistent and unreliable [32],
 especially in healthy populations (which typically lack recent 
EHR/claims medical history), they play an important role in clinical 
outcomes. We believe this would be most addressable through the 
collection of passive data from wearable sensors, which future work will
 include. On a similar note, although we were able to use aggregate 
statistics for race and economic status based on zip-code-derived census
 data, we were unable to track them at an individual level. Though this 
form of zip code aggregation has been shown to be useful in clinical 
risk assessment [33],
 individual SDoH data could increase the precision and accuracy of THP 
multimorbidity scores. Future studies of the THP will examine the impact
 of longer observation and follow-up windows on strategies for clinical 
intervention. Finally, we note the unreliability of claims data at 
large, as they are typically produced with financial incentives that are
 not necessarily aligned with patient care, though they are still often 
used for risk assessment problems [34,35].ConclusionIn
 summary, we combined practical clinical knowledge with modern ML on 
large-scale data to produce THP multimorbidity scores to aid in decision
 making across generalizable patient populations. We believe that the 
THP will allow for more targeted prioritization of care-gap closure, the
 assessment of comprehensive risk profiles for a greater number of 
patients, and facilitation of better physician-patient interactions and 
joint decision making via feature explainability. Although prospective 
studies will be required to measure the utility of this approach, our 
intention is that the THS may be used as a preliminary risk stratifier 
to rapidly prioritize patients for care from a population health 
management perspective [36].
 Once a patient is engaged with a care provider, the organ-specific 
scores can be used to guide, and explain, individualized clinical 
interventions based on existing best practices. This would provide the 
foundation for an integrated continuum between population health and 
personalized medicine. Finally, we also note the promise that the THP 
has for clinical research at large, reflecting the rare opportunity to 
study holistic clinical risk at an extreme scale, potentially unveiling 
clinically valuable insights."
"Incident atrial fibrillation and its risk prediction in patients developing
COVID-19: A machine learning based algorithm approach","Background: The elderly multi-morbid patient is at high risk of adverse outcomes with COVID-19 complications,
and in the general population, the development of incident AF is associated with worse outcomes in such pa-
tients. There is therefore the need to identify those patients with COVID-19 who are at highest risk of developing
incident AF. We therefore investigated incident AF risks in a large prospective population of elderly patients
with/without incident COVID-19 cases and baseline cardiovascular/non-cardiovascular multi-morbidities. We
used two approaches: main effect modeling and secondly, a machine-learning (ML) approach, accounting for the
complex dynamic relationships among comorbidity variables.
Methods: We studied a prospective elderly US cohort of 280,592 patients from medical databases in an 8-month
investigation of with/without newly incident COVID19 cases. Incident AF outcomes were examined in rela-
tionship to diverse multi-morbid conditions, COVID-19 status and demographic variables, with ML accounting
for the dynamic nature of changing multimorbidity risk factors.
Results: Multi-morbidity contributed to the onset of confirmed COVID-19 cases with cognitive impairment (OR
1.69; 95%CI 1.52–1.88), anemia (OR 1.41; 95%CI 1.32–1.50), diabetes mellitus (OR 1.35; 95%CI 1.27–1.44) and
vascular disease (OR 1.30; 95%CI 1.21–1.39) having the highest associations. A main effect model (C-index value
0.718) showed that COVID-19 had the highest association with incident AF cases (OR 3.12; 95%CI 2.61–3.710,
followed by congestive heart failure (1.72; 95%CI 1.50–1.96), then coronary artery disease (OR 1.43; 95%CI
1.27–1.60) and valvular disease (1.42; 95%CI 1.26–1.60). The ML algorithm demonstrated improved discrimi-
natory validity incrementally over the statistical main effect model (training: C-index 0.729, 95%CI 0.718–0.740;
validation: C-index 0.704, 95%CI 0.687–0.72). Calibration of the ML based formulation was satisfactory and
better than the main-effect model. Decision curve analysis demonstrated that the clinical utility for the ML based
formulation was better than the ‘treat all’ strategy and the main effect model.
Conclusion: COVID-19 status has major implications for incident AF in a cohort with diverse cardiovascular/non-
cardiovascular multi-morbidities. Our ML approach accounting for dynamic multimorbidity changes had good
prediction for new onset AF amongst incident COVID19 cases.","In this large analysis of elderly patients free of AF and COVID-19 at
baseline, but followed up for new COVID-19 cases, we developed a ML
based logistic regression algorithm for predicting incident AF account-
ing for dynamic changes in risk including newly acquired risk factors.
Second, DCA showed the ML-based logistic regression algorithm had
better clinical utility in terms of net benefit than the two treatment
strategies (i.e., treat all or none).
The ML analyses demonstrated that COVID-19 status had the strongest independent association with incident AF relative to the traditional
cardiovascular co-morbidities including congestive heart failure and
coronary artery disease. This was also evident in the main effect analyses. In the absence of COVID-19, the presence of congestive heart
failure and coronary disease are independent cardiovascular risk factors
leading to incident AF conditions; however, the presence of incident
COVID-19 infection changed the importance of classic cardiovascular
risk factors feeding into the development of new-onset AF. There were
also significant and dynamic interactions between the presence of
incident COVID-19 infections and co-morbid history including anemia,
chronic obstructive pulmonary disease and vascular disease.
In the main effect model, cardiovascular and non-cardiovascular multi-morbidities had significant roles in the spectrum of AF disease
complexity in addition to the emergent COVID-19 as a risk factor. As
expected, multi-morbidity played an important role in increasing the
risk of COVID-19 infection[3–5]. Demographic variables continued to
demonstrate their importance as risk factors associated with the inci-
dence of AF. Age implicated its effects in non-linear terms using both (a)
quadratic effects when modelling age as a continuous variables; and (b)
interactive terms (with coronary artery disease and chronic kidney
disease) upon the use of age as a categorical variable. Gender showed its
influence in interactive terms with the co-morbid history (chronic
obstructive pulmonary disease, major bleeding).
Our findings are important given the worse prognosis amongst
COVID-19 patients with AF, with a higher risk of thrombosis and mor-
tality when compared to AF patients without COVID-19 patients [11].
Our ML prediction could be incorporated into telehealth approaches to
monitor patients following their COVID-19 diagnosis, for the onset of
incident AF [12]; an important consideration given that many COVID-19
infections could be asymptomatic [13]. Given the increasing focus on
integrated care management of patients with AF [14], and the need for
thromboprophylaxis in such patients [15], novel ML approaches could
facilitate structured management and follow-up, especially since risk
profiles change in a dynamic manner over time [16–18]. Such a struc-
tured approach to holistic AF care, including proactive risk evaluation,
has been shown to be associated with improved clinical outcomes,
especially with a reduction in hospitalisations and bleeding events
[19–21].
4.1. Limitations
Our study is limited by its observational design and shorter follow-up
period. As with observational cohorts the possibility of residual con-
founding remains. One should keep in mind the potential bias emerging
due to healthcare services concentrating on the treatment of COVID-19
cases and possibly leading to the cancellation of routine services, such as
office visits for established chronic conditions. This extent of possible
bias is not known but should be kept in mind. Additional research would be required to assess the implications of these results on integrated care
management for such AF patients [8]. Finally, the results of this study
are only applicable for the incident AF cases for which the prior history
of anticoagulants and heart rhythm control were applied as exclusion
criteria for this purpose. Therefore, the effects of prior use of anticoag-
ulants cannot be ascertained from this study.
5. Conclusions
COVID-19 status had the strongest independent association with
incident AF, compared to the traditional cardiovascular co-morbidities
including congestive heart failure and coronary artery disease. An ML
approach elicited the complex dynamic relations which lead to the
incidence of AF and in general showed better performance than the
statistical main-effect model in terms of discriminatory validity, clinical
utility as well as model calibration."
"Improving Stroke Risk Prediction in the 
General Population: A Comparative Assessment of Common Clinical Rules, a
 New Multimorbid Index, and Machine-Learning-Based Algorithms","Background There are few large studies examining and predicting the diversified
cardiovascular/noncardiovascular comorbidity relationships with stroke. We investigated
stroke risks in a very large prospective cohort of patients with multimorbidity, using two
common clinical rules, a clinical multimorbid index and a machine-learning (ML) approach,
accounting for the complex relationships among variables, including the dynamic nature of
changing risk factors.
Methods We studied a prospective U.S. cohort of 3,435,224 patients from medical
databases in a 2-year investigation. Stroke outcomes were examined in relationship to
diverse multimorbid conditions, demographic variables, and other inputs, with ML
accounting for the dynamic nature of changing multimorbidity risk factors, two clinical
risk scores, and a clinical multimorbid index.
Results Common clinical risk scores had moderate and comparable c indices with stroke
outcomes in the training and external validation samples (validation—CHADS2: c index
0.812, 95% confidence interval [CI] 0.808–0.815; CHA2DS2-VASc: c index 0.809, 95% CI
0.805–0.812). A clinical multimorbid index had higher discriminant validity values for both
the training/external validation samples (validation: c index 0.850, 95% CI 0.847–0.853). The
ML-based algorithms yielded the highest discriminant validity values for the gradient
boosting/neural network logistic regression formulations with no significant differences
among the ML approaches (validation for logistic regression: c index 0.866, 95% CI
0.856–0.876). Calibration of the ML-based formulation was satisfactory across a wide range
of predicted probabilities. Decision curve analysis demonstrated that clinical utility for the
ML-based formulation was better than that for the two current clinical rules and the newly
developed multimorbid tool. Also, ML models and clinical stroke risk scores were more
clinically useful than the “treat all” strategy.
Conclusion Complex relationships of various comorbidities uncovered using a ML
approach for diverse (and dynamic) multimorbidity changes have major consequences for stroke risk prediction. This approach may facilitate automated approaches for
dynamic risk stratification in the significant presence of multimorbidity, helping in the
decision-making process for risk assessment and integrated/holistic management.","In this analysis, first, subjects without AF were enrolled at baseline (day 0) and over the first 12 months of enrolment, we accounted for incident comorbidities, including AF, in the first 12 months of enrollment. The first incidence of stroke “no treatment” strategies. However, calibration was only fair in
the 0 to 10% range of predicted probabilities. Beyond this range,
these tools are not well calibrated, clearly demonstrating the
need for additional predictors to improve stroke prediction.
The newly developed multimorbid index showed signifi-
cantly higher c index values (0.85) than the CHADS 2 and
CHA 2 DS 2 -VASc tools and improved clinical utility in terms of
net true-positive stroke events, but the external calibration 
beyond the first 12 months was determined, and ML-based
algorithms yielded the highest discriminant validity values.
Second, decision curve analysis demonstrated that clinical
utility for the ML-based formulation was better than that for
the clinical rules and a clinical multimorbid index. Multimorbidity is closely related to stroke. In a systematic
review, Gallacher et al13 maintained that multimorbidity in
stroke patients is a growing health care issue, with a preva-
lence ranging from 43 to 94%, with implications for increased
mortality. However, the majority of prior studies had a small
sample size. In essence, the present study is a very large cohort
addressing the impact of multimorbidity on stroke, accounting
for the dynamic nature of risk factors changing with aging and
incident comorbidities using ML. This approach is important
since traditionally, stroke prediction has been linked to
individual cardiovascular diseases such as AF and common
comorbidities such as diabetes mellitus and hypertension,
which are determined at baseline, and outcomes determined
many years later when risk would be altered by age and
incident risks. In recent studies examining the dynamic nature
of stroke risk, follow-up risk profile or changes in risk profile
had the best prediction of ischemic stroke in patients with AF.6
Multimorbidity clusters and their trajectories may help iden-
tifying homogeneous patient groups with similar prognosis,
assisting clinicians and health care systems in personalization
of clinical interventions and preventive strategies.8,14
In this study, we have addressed a concept showing how
ML improves on clinical risk assessment over clinical scores,
accounting for the dynamic nature of risk factors, by aging and
incident comorbidities. Generalizability is ensured by examin-
ing a very large cohort covering different geographical areas in
the U.S. continent and including a diverse social array of house-
holds. This very large cohort was typified by a diverse cardio-
vascular and noncardiovascular multimorbid history across a
wide spectrum of age from 18 to 90 years old. We also compared
stroke prediction with two common clinical stroke prediction
rules, a newly developed multimorbid index and ML-based
algorithms, allowing us to uncover the complex interrelation-
ships among the prior comorbid history leading to stroke.
The two common simple stroke risk scores (CHADS2,
CHA2DS2-VASc) demonstrated moderate (0.81) discriminant
validity in the training and validation samples. They also
demonstrated better clinical utility than the “treat all” and  of the multimorbid index was suboptimal across the wide
spectrum of predicted probabilities.
The ML-based logistic regression algorithm provided the
best performance in terms of discriminant validity,
calibration, clinical utility, and cumulative lifts. The vari-
ables embedded in this ML algorithm include not only the
multimorbid index but also significant interactions of
comorbid history, stroke risk indices, demographic, and other types of variables. Quadratic terms were also
included. Collectively, these complex interrelationships
confirm the importance of diverse cardiovascular/noncar-
diovascular comorbid history in stroke prediction as well
as the importance of inclusion of age groups below 65 years
and greater than 45 years. One may argue against the complexity brought forward by
the ML approach into clinical practice. However, this can be
alleviated by advancing the use of digital strategies aimed at
enhancing the participation of patients and health care pro-
viders in integrated stroke care management.15,16 This is par-
ticularly crucial in the presence of diverse and significant multimorbidity enabling and connecting different medical
providers in pursuing the optimal care for stroke patients.
Limitations
Our study is limited by its observational design, but this
represents the largest prospective cohort of “real-world”
patients for the assessment of stroke risk incorporating the
dynamic nature of risk factors and aging. As with observational
cohorts the possibility of residual confounding remains. Given
the nature of conditions that are risk factors for stroke, MI, etc., a
1-year run-in period to define comorbid condition risks may
underclassify patients but data may not be available in many
cases to account for the longer time periods, especially if
insurance providers change. Indeed, exclusion of healthy (less
than two claims) patients may lead to higher than expected
event rates. Clinical risk indices also tend to overestimate the
risk of future strokes, especially at the low- and high-risk
groups. Finally, statistical significance is shown (as expected
with such a sample size), but we fully recognize that statistical
significance is not the same as clinical significance. However, we
clearly illustrate and prove a concept on how risk prediction can
be improved by ML models.
Conclusion
Complex relationships of various comorbidities uncovered
using a ML approach for diverse (and dynamic) cardiovascu-
lar/noncardiovascular multimorbidity changes have major
consequences for stroke risk prediction. This approach may
facilitate automated approaches for dynamic risk stratification in the significant presence of multimorbidity, helping in the
decision-making process for risk assessment and integrated/
holistic management. The results show that the ML approach
is superior to conventional risk models, and open up the
potential for this approach being used for more refined preci-
sion medicine and health insurance support."
"Natural language processing system for rapid detection and
intervention of mental health crisis chat messages","Patients experiencing mental health crises often seek help through messaging-based platforms, but may face long wait times
due to limited message triage capacity. Here we build and deploy a machine-learning-enabled system to improve response
times to crisis messages in a large, national telehealth provider network. We train a two-stage natural language processing
(NLP) system with key word filtering followed by logistic regression on 721 electronic medical record chat messages, of which
32% are potential crises (suicidal/homicidal ideation, domestic violence, or non-suicidal self-injury). Model performance is
evaluated on a retrospective test set (4/1/21–4/1/22, N = 481) and a prospective test set (10/1/22–10/31/22, N = 102,471). In
the retrospective test set, the model has an AUC of 0.82 (95% CI: 0.78–0.86), sensitivity of 0.99 (95% CI: 0.96–1.00), and PPV of
0.35 (95% CI: 0.309–0.4). In the prospective test set, the model has an AUC of 0.98 (95% CI: 0.966–0.984), sensitivity of 0.98 (95%
CI: 0.96–0.99), and PPV of 0.66 (95% CI: 0.626–0.692). The daily median time from message receipt to crisis specialist triage
ranges from 8 to 13 min, compared to 9 h before the deployment of the system. We demonstrate that a NLP-based machine
learning model can reliably identify potential crisis chat messages in a telehealth setting. Our system integrates into existing
clinical workflows, suggesting that with appropriate training, humans can successfully leverage ML systems to facilitate triage
of crisis messages.","While crisis helplines provide much needed support to individuals
experiencing mental health emergencies, they can be limited by
human capacity 12 and are typically not integrated within
healthcare systems. With increasing volumes of requests for help 5
that are triaged on a first-come-first-serve basis, time-sensitive
crisis messages from high-risk individuals may wait in a queue
behind less urgent messages. We built CMD-1, an NLP-enabled
system that detects and surfaces crisis messages to enable faster
triage from a crisis response team. We deployed CMD-1 within a
large, national telehealth provider platform and performed a
prospective validation on over 120k messages coming from over
30k distinct patients. We found that CMD-1 was able to detect
high-risk messages with high accuracy (97.5% sensitivity and
97.0% specificity) and enabled crisis specialists to triage crisis
messages within 10 min (median) of message receipt.
The speed at which a high-risk individual in crisis is contacted is
especially important because immediate intervention can ulti-
mately divert them away from a suicide attempt (Kelly et al., 2008;
McClatchey et al., 2019). CMD-1 reduced response times to crisis
chat messages by nearly two orders of magnitude, from over 9 h
on average to 9 min (median). This supports the findings of
previous work using ML to enhance a clinical team’s capability to
satisfactorily address the crises of high-risk individuals (Crisis Text
Line, and Xu et al., 2021). For example, Crisis Text Line used a
machine learning model to reduce wait times for high risk texters
from 8 min (median, 75th percentile: 35 min) down to 3 min
(median, 75th percentile: 11 min). One key advantage of CMD-1 is
that it was integrated within the patient’s clinical workflow,
allowing clinicians to be notified of patient crises as they happen.
Indeed, we have shown that notifying clinicians of patient crises can lead to more timely follow-up care13 . Crisis response systems
that are integrated within provider organizations have the
potential to impact key aspects of patient care. Future work
might investigate the impact of CMD-1 and response times on
downstream health outcomes such as hospitalization or ED
utilization.
Despite growing interest in ML applications in healthcare in
recent years, ML models are rarely deployed in health systems,
with most studies presenting model development without
translation to clinical care 14 . Deploying a ML model into clinical
workflows requires technical considerations (e.g., model accuracy),
operational considerations (e.g., model interpretability, user experience), and technological infrastructure (e.g., data storage
capabilities, ability to generate predictions in near-real time) 15,16 .
In developing CMD-1, a cross-functional team of clinicians worked
together to address these considerations. Clinicians and data
scientists collaborated to define a meaningful outcome and select
an appropriate probability threshold for classification. Clinicians
and data engineers collaborated to design a simple, effective user
interface for the CMD-1 Slack posts. Data scientists and data
engineers collaborated to embed the ML model within a robust
data infrastructure that enabled near real-time predictions and
data capture.
The superior performance of CMD-1 in the prospective test set
compared to the retrospective test set warranted further
investigation. We hypothesized that the higher AUC (0.98 vs.
0.82) and higher specificity (0.97 vs. 0.12) in the prospective test
set compared to the retrospective test set was due to differences
in both the data sampling methodology as well as class
imbalance. To increase the prevalence of true crises in the training
and retrospective test sets, we included all messages sent in the
seven days prior to a true crisis, as recorded in a crisis event
tracker. While this enabled us to enrich our training set with true
crisis messages, the included non-crisis messages were not
representative of non-crisis messages in the deployment setting.
We hypothesized that the non-crisis messages preceding crisis
events were more difficult to distinguish from crisis messages than
non-crisis messages not associated with crisis events, and that this
explained the lower AUC and specificity during retrospective
evaluation. For example, the message “I want nothing. I used to
want things, now I don’t want anything… I sense a danger”, which
preceded a crisis, was likely more difficult to accurately classify as
a non-crisis than the message “Thanks for helping me meet with
someone,” which did not precede a crisis. The fact that non-crisis
messages in the prospective test set were shorter than those in
the training and retrospective test sets lends further support to
this idea.
To confirm this hypothesis, we show that the median predicted
probability for non-crisis messages in the retrospective test set
was 0.079 [IQR: 0.026–0.170], while the median predicted
probability for non-crisis messages in the prospective test set was 0.019 [IQR: 0.0027–0.069], which is much closer to the model’s
decision boundary of 0.01 (SI Table 5).
In addition, the difference between retrospective and prospec-
tive test set values caused by data sampling methodology was
exacerbated by the increased proportion of non-crisis messages in
the prospective test set. To further confirm this hypothesis, we
conducted a sensitivity analysis where we randomly down-
sampled the proportion of non-crisis messages in the prospective
test set so that the event rate matched that of the retrospective
test set. Over 100 iterations, the median AUC on the modified
prospective test set with down-sampled true negatives was 0.82
[IQR: 0.82–0.82], which nearly exactly matches the AUC of 0.82 in
the retrospective test set (SI Fig. 3). This suggests that the
performance of CMD-1 was comparable in both retrospective and
prospective test sets, and that differences in AUC could largely be
attributed to differences in the proportion of and type of non-
crisis messages. Other contributing factors to the differences in
model performance may include changes in the patient popula-
tion between April and October 2022. For example, compared to
patients in the retrospective test set, patients in the prospective
test set were more likely to be male, to have generalized anxiety
disorder, and to have been in treatment longer.
One limitation of our approach is our use of the crisis terms
filter. As discussed above, the terms filter excluded messages from
being evaluated by the NLP model if they did not contain at least
one phrase from the filter. While our filter was broad and curated
manually by experts, some messages that may otherwise be
flagged as crises could be filtered out by the terms filter. One way
to quantify this would be an ablative study—removing one term
from the terms list at a time and quantify changes in our
experimental outcomes. Using this, one can estimate the number
of messages that would be flagged as crises after adding more
terms to the filter.
Another limitation is the size of our training set and the fact that
we artificially enriched the training set for true crises. As described
in “Methods” section, due to the low prevalence of crisis messages
(<1%), we enriched the training set and retrospective test set for
crisis messages for efficiency of data labeling. The training set was
721 messages, and the prevalence of crisis messages in the
prospective test set was 0.6% compared to 32% in the training
and retrospective test sets. Applying a prediction model to a
dataset with a different event rate than the dataset used to train
the model can result in mis-calibrated predicted risks, thus
impacting downstream classification17 . Further, class imbalance
corrections like random under-sampling of non-events without subsequent recalibration has been shown to lead to miscalibra-
tion. There are two reasons why these risks of miscalibration were
mitigated when deploying CMD-1. First, CMD-1 is a classifier and
not a continuous risk prediction model, meaning that the impact
of miscalibration on model discrimination is limited to predicted
risks close to the classification threshold. This is reinforced by the
strong sensitivity and specificity of the model on the prospective
test set. Second, deploying CMD-1 in a population where the
event rate is lower than that of the training dataset would be
expected to lead to inflated predicted risks and therefore more
false positives, not more false negatives. This is acceptable given
our 20 to 1 preference for false positives over false negatives for
this use case.
The moderate PPV of CMD-1 could also be considered a
limitation. With approximately four out of every 10 messages
surfaced being a false positive, there is ample opportunity to
improve the accuracy of CMD-1 to decrease wasted human effort
of triaging false positives. Exploring the performance of other
word embeddings like word2vec or other prediction functions like
support vector machines, random forest, or deep learning
architectures such as large language models could improve the
PPV of CMD-1 without sacrificing sensitivity.
Overall, CMD-1 serves as a promising model for ML-enabled
solutions to drive improvements in mental healthcare delivery. By
using technology to automate a manual task, CMD-1 increases
operational efficiency and reduces wait times for patients. With
demand for mental health services far exceeding supply, providers
must leverage technology and data to increase access to care and
make the best use of available human capacity."
"AI should focus on equity in pandemic preparedness

            
                
                
                    
                
            ","Equity issues must be considered when artificial 
intelligence (AI) becomes a tool for predicting viral evolution, 
designing vaccines and enhancing the response to future pandemics (see Nature 622, 440–441; 2023). All populations must benefit, regardless of their socio-economic status or location.Over-reliance
 on AI could inadvertently prioritize certain viruses or populations, 
leading to inequities in vaccine and disease research. Selective 
targeting of microorganisms using AI could also disrupt the delicate 
balance of the planet’s ecosystems and weaken society’s ability to 
control pandemics.The prevalence of diseases such as dengue and 
multi-drug-resistant tuberculosis is disproportionately high in 
low-income countries, which have limited resources and health-care 
infrastructure. Global diseases receive considerably more attention and 
investment. Without careful implementation of AI-assisted vaccine 
development, there is a risk of perpetuating inequities. Vaccines for 
diseases that mainly affect marginalized populations might not be 
adequately researched, developed or distributed.A more inclusive 
approach to pandemic preparedness should use AI to facilitate 
collaboration between computer scientists, governments, health-care 
providers, epidemiologists and the public.
                ",
"Cardiovascular disease (CVD) outcomes and associated risk factors
in a medicare population without prior CVD history: an analysis using
statistical and machine learning algorithms","There is limited information on predicting incident cardiovascular outcomes among high- to very high-risk populations
such as the elderly (? 65 years) in the absence of prior cardiovascular disease and the presence of non-cardiovascular multi-
morbidity. We hypothesized that statistical/machine learning modeling can improve risk prediction, thus helping inform
care management strategies. We defined a population from the Medicare health plan, a US government-funded program
mostly for the elderly and varied levels of non-cardiovascular multi-morbidity. Participants were screened for cardiovascular
disease (CVD), coronary or peripheral artery disease (CAD or PAD), heart failure (HF), atrial fibrillation (AF), ischemic
stroke (IS), transient ischemic attack (TIA), and myocardial infarction (MI) for a 3-yr period in the comorbid history. They
were followed up for up to 45.2 months. Analyses included descriptive approaches in terms of incidence rates and density
ratios, and inferential in terms of main effect statistical/complex machine learning modeling. The contemporary risk factors
of interest spanned across the domains of comorbidity, lifestyle, and healthcare utilization history. The cohort consisted
of 154,551 individuals (mean age 68.8 years; 62.2% female). The overall crude incidence rate of CVD events was 9.9 new
cases per 100 person-years. The highest rates among its component outcomes were obtained for CAD or PAD (3.6 for each),
followed by HF (2.2) and AF (1.8), then IS (1.3), and finally TIA (1.0) and MI (0.9).
Model performance was modest in terms of discriminatory power (C index: 0.67, 95%CI 0.667–0.674 for training; and 0.668,
95%CI 0.663–0.673 for validation data), equal agreement between predicted and observed events for calibration purposes,
and good clinical utility in terms of a net benefit of 15 true positives per 100 patients relative to the All-patient treatment
strategy. Complex models based on machine learning algorithms yielded incrementally better discriminatory power and
much improved goodness-of-fitness tests from those based on main effect statistical modeling. This Medicare population
represents a highly vulnerable group for incident CVD events. This population would benefit from an integrated approach to
their care and management, including attention to their comorbidities and lifestyle factors, as well as medication adherence.","The main findings from this study are as follows: (1) the
overall crude incidence rate for CVD events was high
amounting to 9.9 new cases/100 person-years and an inci-
dence density equaling 29%; (2) the highest incidence
rates and density ratios were obtained for CAD and PAD;
(3) the model developed to predict incident CVD events
demonstrated good performance in terms of discrimi-
nation, calibration, and clinical utility; (4) a number of
important contemporary individual risk factors emerged
from the comorbid (e.g., COPD), lifestyle/personal (e.g.,
tobacco use/dependency) and healthcare utilization (e.g.,
ER visit counts) history; and (5) the more complex ML-
based algorithms yielded slightly better discriminatory
performance than those based on traditional main effect
statistical modeling, yet it produced a better model with a
goodness-of-fit test thus reducing misspecification errors. The crude overall incidence density ratio of 29% was
much higher than those recently reported in 2019 for the
general population in the US [20]. For the adult population
aged 18 years and above, the prevalence rate of self-reported
heart disease (i.e., coronary heart disease, angina or angina
pectoris, and myocardial infarction) was 6.4% [18]. For an
age group equal to 65 years and above, the prevalence rate
was 18.3%. Based on an American Heart Association report,
[21] the prevalence of CVD (defined as comprised of CAD,
HF and stroke) was equal to 9.3% overall in adults ? 20 years
of age. The 29% incidence ratio was much higher than the
prevalence rate of 7.9% for CVD for the general population
we previously reported [11] and was slightly lower than that
of 35.1% for the age ? 65-year group.
In this study, the crude overall incidence rate was 9.9
cases/100 person-years, a value that is higher than the over-
all rates of 0.66 and 0.95 cases/100 person-years previously
reported for the UK women and men, respectively [2]. Fur-
thermore, the 85–94 year age group studied in the present
study had an incidence rate that is almost double the overall
rate (18.3); indeed, almost 1 in 2 end up having an incident
CVD event based on the incidence density ratio of 48%. This
age bracket has limited prior data and demonstrates that it is
extremely vulnerable for incident CVD events.
The overall incidence rate obtained for MI was 0.9
events/100 person-years, which was higher than our prior
report for the general population across three health plans
(i.e., Medicare, Medicaid, Commercial) and a comparable
age spectrum [11]. Hence, the MI incidence rate for this
Medicare cohort without CVD in the comorbid history is
almost 38% to 104% of the incidence rate for the general
population. The overall incidence rate of AF was 1.8 new
events/100 person-years, much higher than that reported by
Lip et al. [12] for a Medicaid cohort typified by a lower
socio-economic status (relative to those participating in this
study), high disability status, presence of cardiovascular
conditions in comorbid history, and a much younger aver-
age age. In our prior study [12], the incidence rate was 0.49
cases/100 person-years. Furthermore, the overall rate in the
present study was much higher than that reported by Lip
et al. [13] for the general population (incidence rate = 0.33
cases/100 person-years) with the presence of cardiovascular
conditions in the comorbid history.
Lip et al. [14] reported that the incidence rate for
stroke (i.e., ischemic stroke, transient ischemic attack, and
thrombo-embolic events) is 0.95 cases/100 person-years in
a general population across three health plans (Commercial,
Medicare, and Medicaid). The population had 3.4 million
participants and had cardiovascular conditions in the comor-
bid history including stroke. Yet, the value obtained in the
present study for ischemic stroke was higher and equal to
1.3 new events/100 person-years, although the incidence
rate for ischemic stroke alone is lower than that for stroke at large. Indeed, adding TIA events to those for ischemic stroke
would yield even higher incidence rates [15].
The incidence statistics in the present study suggest that
the cohort may represent a high to very high CVD risk
population. Therefore, because of the lack of information in
the published literature, a simple model predictive of inci-
dent CVD events and consisting of individual risk factors
was devised, demonstrating good performance metrics and
good model calibration. The clinical utility of the model
also showed good results by truly detecting 15 true posi-
tive events (after accounting for the false positives) per 100
patients over and above those produced by the all-patient
treatment strategy at a probability threshold of 20%.
Collectively, this predominantly elderly population with a
varied mix of disability and non-cardiovascular multi-mor-
bidity and the absence of cardiovascular disease in comorbid
history therefore represents a high- to very high-risk group
for CVD events. This suggests that significant non-cardio-
vascular illnesses put excessive overload on the cardiovascu-
lar system and weaken the general immune system. Indeed,
these effects would make these cohorts vulnerable for any
kind of cardiovascular illness as well as the easier path for
the effects of external factors on the cardiovascular system
due to reduced immunity (e.g., COVID-19).
In light of the above, it appears that these subpopula-
tions would benefit from an integrated approach to their care
management, including attention to their comorbidities and
lifestyle factors. From a practicing clinician standpoint, such
an integrated care management approach can be devised to
manage comorbid history symptoms or disorders [22–24].
This would be in a way similar to (a) the ABC (Atrial fibril-
lation Better Care) strategy to manage AF [25], which has
been associated with improved outcomes in AF patients [26]
and has been recommended in guidelines [27] and (b) other
integrated approaches reported by researchers and clinicians
in the medical literature for chronic long term conditions
[24, 28]. The lifestyle/personal variables can be modified to
reduce the future CVD risk in a comprehensive manner con-
sistent with the guidelines [29]. For example, the American
Heart Association introduced “Life’s Simple 7 initiative”
including three cardiovascular risk factors (glucose, blood
pressure, and cholesterol) and four lifestyle behaviors (body
mass index, smoking, physical activity, and diet), and the
majority of these factors have been associated with longevity
in prospective observational studies [30–33]. In addition to
an integrated care management approach, it will be essen-
tial to find ways to improve medication adherence given the
likely polypharmacy in such patients [34]. With respect to the statistical methods and ML algorithms
utilized in this study, it appears that ML algorithms provide
better solutions than the traditional statistical techniques,
given that the ML formulations provide complex non-linear
equations, thereby, exploiting the detailed interactions and non-linear effects within and across the classes of clinical
and non-clinical parameters utilized in the built-in models.
Parametric ML techniques were utilized with the aim to pro-
vide detailed equations for use by clinicians, and neural net-
work algorithms provided comparable results to the complex
logistic regressions equations. Therefore, the latter solution
was utilized due to their explicit mathematical formulations.
Indeed, there is a potential economic value of integrating
ML in usual care, for detecting patients at higher risk for an
integrated, personalized approach. This is particularly vital
for the patients at risk of serious CVD (e.g., ischemic heart
disease, stroke, and heart failure) with important co-morbid-
ities. For example, Szymanski et al. [35] examined the use
of an AF risk prediction algorithm in improving AF detec-
tion compared with regular screening in primary care and
assessed the associated budget impact, potentially saving
millions in the UK healthcare system. Other examples are
also reported for other cardiovascular conditions [36–39].
Limitations
The findings of this study were based on observational
research derived from administrative databases with poten-
tial subject and methodological biases compared to the well-
controlled clinical trials. The use of observational studies
using administrative data may be subject to confounding bias
by unadjusted factors (e.g., disease severity, blood pressure
control, exact estimated glomerular filtration rate, adverse
drug effect, and reasons for ceasing medication) or by a
residual channeling bias. Finally, residual bias is still pos-
sible, especially with regard to unmeasured variables related
to disease severity and clinical data.
Despite the above-mentioned limitations, the methodo-
logical procedures deployed in this investigation are based
on best available practices. Additionally, the potential biases
may have been lessened by the truly diversified population
utilized in this study with large numbers. Despite the biases
to which observational studies are subject, these stud-
ies complement clinical trial via generalization of results
through the use of real world data.
Conclusions
This Medicare population represents a high- to very high-
risk group for CVD events and would benefit from an inte-
grated care approach to their management, including atten-
tion to their comorbidities, lifestyle factors, and healthcare
utilization patterns."
"Systemic Multimorbidity Clusters
in People with Periodontitis","This study aimed to identify systemic multimorbidity clusters in people with periodontitis via a novel artificial intelligence–based network
analysis and to explore the effect of associated factors. This study utilized cross-sectional data of 3,736 participants across 3 cycles of
the National Health and Nutrition Examination Survey (2009 to 2014). Periodontal examination was carried out by trained dentists
for participants aged ?30 y. The extent of periodontitis was represented by the proportion of sites with clinical attachment loss (CAL)
? 3 mm, split into 4 equal quartiles. A range of systemic diseases reported during the survey were also extracted. Hypergraph network
analysis with eigenvector centralities was applied to identify systemic multimorbidity clusters and single-disease influence in the overall
population and when stratified by CAL quartile. Individual factors that could affect the systemic multimorbidity clusters were also
explored by CAL quartile. In the study population, the top 3 prevalent diseases were hypertension (63.9%), arthritis (47.6%), and obesity
(45.9%). A total of 106 unique systemic multimorbidity clusters were identified across the study population. Hypertension was the most
centralized disease in the overall population (centrality [C]: 0.50), followed closely by arthritis (C: 0.45) and obesity (C: 0.42). Diabetes
had higher centrality in the highest CAL quartile (C: 0.31) than the lowest (C: 0.26). “Hypertension, obesity” was the largest weighted
multimorbidity cluster across CAL quartiles. This study has revealed a range of common systemic multimorbidity clusters in people
with periodontitis. People with periodontitis are more likely to present with hypertension and obesity together, and diabetes is more
influential to multimorbidity clusters in people with severe periodontitis. Factors such as ethnicity, deprivation, and smoking status may
also influence the pattern of multimorbidity clusters.","In this study, we applied hypergraph analysis to a large cross-
sectional cohort to identify the systemic multimorbidity clus-
ters in people with periodontitis. Hypertension, arthritis, and
obesity were the single diseases most central/influential to
multimorbidity clusters for people with periodontitis; the most
frequent systemic multimorbidity cluster was “hypertension,
obesity.” Diabetes was more central in participants with a
higher degree of CAL, and systemic multimorbidity clusters
were more evenly weighted in these participants as compared
with those who had lower levels of CAL. Subgroup analysis
showed that diabetes and stroke were more central to systemic
multimorbidity clusters in nonsmoking participants with higher
CAL as compared with smokers, and this difference was not
observed in the lower quartile of CAL. The focus of this study was to identify systemic multimorbid-
ity clusters that present in people with periodontitis by using
the proportion of CAL ? 3 mm and its quartiles as an indicator
for the extent of periodontitis. Previous studies mainly focused
on the association between periodontitis and only 1 of the sys-
temic conditions, such as cardiovascular disease (Larvin et al.
2021a), hypertension (Czesnikiewicz-Guzik et al. 2019),
diabetes (Winning et al. 2017), and COVID-19 outcomes
(Larvin et al. 2020; Larvin, Wilmott, et al. 2021). Some studies demonstrated that periodontitis is coprevalent with several sys-
temic conditions, such as obesity, diabetes, and cardiovascular
disease, but these were predefined at study inclusion (Kang
et al. 2019; Bilgin Çetin et al. 2020; Takeda et al. 2021).
Through use of hypergraph analysis as a data-driven technique,
our study has identified some common systemic multimorbid-
ity clusters in people with periodontitis and the most influential
single diseases to multimorbid presentation. For example, we
identified “hypertension, obesity” as the most frequent multi-
morbidity cluster in people with periodontitis. Hypertension
and obesity are 2 common conditions known to usually precede development of further systemic disease due to the
inflammatory stresses that they trigger on the body (Liu and
Nikolajczyk 2019; Ruan and Gao 2019). Past findings based
on cluster analysis also showed that the prevalence of hyper-
tension is common in systemic multimorbidity clusters of the
general population (Formiga et al. 2013; Zemedikun et al.
2018). Cluster analyses are historically limited, as they group
conditions by how closely related they are, rather than identify
tangible systemic multimorbidity clusters as in hypergraph
analyses. Our study highlighted that hypertension and obesity
are central to multimorbidity in people with periodontitis, and
this could be important to develop more targeted treatment
intervention for this particular population.
Furthermore, our study has demonstrated that diabetes is
more central in multimorbidity clusters for people with severe
periodontitis versus people with mild periodontitis. This find-
ing aligns with suggestions of a bidirectional relationship
between periodontitis and diabetes (Casanova et al. 2014;
Stöhr et al. 2021). The systemic inflammation associated with
severe periodontitis likely disrupts immune function and glu-
cose handling associated with diabetes and vice versa (Donath
et al. 2019; Berbudi et al. 2020). The European Federation of Periodontology and the International Diabetes Federation have
developed a roadmap for health care professionals in managing
these patients with collaborative care pathways (Sanz et al.
2018). Our findings advocate consideration of the wider sys-
temic multimorbidity clusters in which these patients may also
present; specifically, such clusters should be reflected in the
recent classification of periodontal disease as separate diag-
nostic entities.
No previous studies have systematically investigated the
systemic multimorbidity clusters in people with periodontitis,
particularly the impact of individual factors on clustering pat-
terns. To account for the high correlation between multimor-
bidity and age and sex, we performed 1:1 matching across
CAL quartiles to ensure that identified multimorbidity clusters
were independent of age and sex. In addition, we assessed the
impact of ethnicity, smoking status, and deprivation on multi-
morbidity clusters. We found that ethnicity and deprivation
affect the clustering pattern of arthritis and obesity more in
people with mild periodontitis, while smoking status and
deprivation affect the clustering pattern of diabetes and cancer
in people with severe periodontitis. These findings highlight
the difference in multimorbidity clusters by patient factors, and such difference should be taken into account when designing
and planning health care provision and strategies for disease
prevention.
Strengths and Limitations
As the first to use hypergraph analysis in oral health research,
our study has notable strengths. The hypergraph technique is in
its relative infancy within the health research sector and is
showing great promise in using cross-sectional data as a nonin-
trusive information source and to move beyond descriptive
analysis, revealing more about patient multimorbid presenta-
tion. Hypergraph analysis for multimorbidity research is dis-
tinct from traditional regression methods as it uses the data to
identify multimorbidity clusters of several diseases simultane-
ously. Previous regression analyses are limited to single
predefined outcomes and pairwise methods to surmise multi-
morbidity. Our study has demonstrated the utility of this tech-
nique in oral health research, as a means of using big data to
identify and compare tangible systemic multimorbidity clus-
ters in populations with periodontitis. As awareness of multi-
morbidity in dental patients is coming to the forefront of oral
health research (Watt and Serban 2020), our findings improve
understanding of multimorbidity in people periodontitis that
may present to the dental clinic. Another strength of our study
is the clinical periodontal examination data including CAL,
which enables exploration of periodontitis severity with clini-
cal validation and high intraclass correlation (Dye et al. 2019).
Furthermore, as a representative national survey across multi-
ple years, the results from NHANES data can provide improved
generalizability than typical epidemiologic studies that can be
limited by selection bias.
Our study was limited to 3 NHANES cycles due to the
availability of periodontal examination data, which limited our
sample size; future NHANES cycles with periodontal exami-
nation could supplement our findings with more data and more
robust findings. The NHANES data set does not supply linked
electronic health records; therefore, our analysis relied on self-
reported multimorbidity. As NHANES was a cross-sectional
national survey, we could not ascertain whether periodontitis
occurred before or after the multimorbidity clusters, because
the timing of disease diagnosis was not available. We used
quartiles of CAL ? 3 mm as a surrogate for a periodontitis case
definition. While this ensured equal sample sizes across
groups, it should be noted that clinical guidelines recommend
CAL and probing depth for periodontitis classification (Caton
et al. 2018).
Conclusion
This study has revealed a range of common systemic multi-
morbidity clusters in people with periodontitis. People with
periodontitis are more likely to present with hypertension and
obesity together, and these conditions are highly influential to
the presence of other multimorbidity. In addition, diabetes is
more influential to multimorbidity clusters in people with severe periodontitis. Patient factors such as deprivation and
smoking status could influence the pattern of multimorbidity
clusters."
"An inflammatory aging clock (iAge) based on deep
learning tracks multimorbidity, immunosenescence, frailty and
cardiovascular aging","While many diseases of aging have been linked to the immunological system, immune metrics
capable of identifying the most at-risk individuals are lacking. From the blood immunome of
1,001 individuals aged 8–96 years, we developed a deep-learning method based on patterns of
systemic age-related inflammation. The resulting inflammatory clock of aging (iAge) tracked
with multimorbidity, immunosenescence, frailty and cardiovascular aging, and is also associated
with exceptional longevity in centenarians. The strongest contributor to iAge was the chemokine
CXCL9, which was involved in cardiac aging, adverse cardiac remodeling and poor vascular
function. Furthermore, aging endothelial cells in human and mice show loss of function, cellular
senescence and hallmark phenotypes of arterial stiffness, all of which are reversed by silencing
CXCL9. In conclusion, we identify a key role of CXCL9 in age-related chronic inflammation and
derive a metric for multimorbidity that can be utilized for the early detection of age-related clinical
phenotypes.","In this study, we conducted extensive immune monitoring in a large cohort of 1,001
individuals to identify immune biomarkers of aging and establish reference values for age-
related systemic chronic inflammation. We used artificial intelligence to create a compact
representation of these biomarkers and derived an ‘inflammatory clock’ of aging, which
takes into account the nonlinear relationship and redundancy of the cytokine network. This
metric tracked with multiple aging phenotypes in the general population and thus, has strong
potential for translational medicine, as it could be used as a diagnostic tool for identifying
those at risk for both noncommunicable and infectious diseases.
Our nonlinear GAE method was optimal for the identification of iAge and its contributors.
As with other deep-learning methods, GAE is capable of capturing complex relationships
between analytes. Similar methods striving to extract signatures of aging have been
described in different systems ranging from genome-wide association studies to proteomics.
We summarize a few notable aging clocks in Supplementary Table 3. In brief, an epigenetic
clock using markers measuring DNA methylation on CpG sites has been used to calculate
an epigenetic age that was able to predict all-cause mortality74,75. It has also been associated
with age-related diseases such as frailty, Alzheimer’s disease, Parkinson’s disease and
cancer. Other clocks such as transcriptomic and microRNA clocks have also been shown to successfully capture aspects of the aging process that are different from epigenetic clocks.
Instead of being associated with all-cause mortality or disease, transcriptomic clocks are
associated with IL-6, albumin, lipids and glucose levels76. There have also been attempts
to derive proteomic clocks and metabolomic clocks77–82 of clinical relevance; however,
iAge allows for new discoveries in the immune system. iAge derived from immunological
cytokines gives us an insight into the salient cytokines that are related to aging and
disease. A notable difference compared to other clocks is that iAge is clearly actionable
as shown by our experiments in CXCL9 where we can reverse aging phenotypes. More
practical approaches range from altering a person’s exposomes (lifestyle) and or the use of
interventions to target CXCL9 and other biomarkers described here.
Recent advancements in deep learning beyond traditional machine-learning methods have
provided enormous opportunities to model biological age. Some of the most popular deep-
learning architectures used to estimate biological age have been recurrent neural networks
(RNNs), convolutional neural networks (CNNs), generative adversarial networks (GANs)
and deep artificial neural networks (ANNs). RNNs have been used on face attributes and
physical activities to estimate biological age83. Although the modality is not in the realm
of biological markers, RNNs have potential to garner results in biological data that require
positional relationships such as epigenetic age. CNNs and GANs have both been used to
abstract facial attributes to predict chronological age84,85. GANs and CNNs are exceptional
in abstracting images to distill useful information. Future applications of GANs and CNNs
can be applied in other biological images such as magnetic resonance imaging; however,
for now, these models are proof of concepts that they can accurately estimate cAge; they
might not necessarily predict the health or lifespan of individuals. The deep-learning models
that have been applied to modality used in this paper are the deep ANNs. ANNs have been
applied to blood biochemistry markers and cell counts to derive biological age86,87. The
results showed that such clocks are able to predict all-cause mortality, potentially finding
biomarkers to intervene and steer individuals toward a healthier life.
Some of the limitations of biological clocks in general is that they do not directly provide
the mechanism by which they work. While it is possible to infer causality between aging
and molecular biomarkers especially in the context of longitudinal or time-series data,
individual biomarkers selected from biological aging clocks need to be experimentally tested
to elucidate the underlying mechanism, as we have done in this study. Our GAE algorithm,
a deep-learning method that efficiently deals with the network structure and nonlinear
behavior of the inflammatory response, can extract high-level complex abstractions as
‘data representations’ using nonlinear functions and is well suited for the analysis of
complex systems where most behaviors are nonlinear, context-dependent and organized in a
distributed hierarchical fashion88. In our case, this method outperformed other commonly
used linear modeling methods such as Elastic Net and PCA and also other nonlinear
approaches such as plain auto-encoder89 (Extended Data Fig. 3b). The correlation between
chronological age and iAge was 0.78 (P < 10?16) (Fig. 1a), which is lower than that of the
recently reported ‘proteomic age’ metric (
R = 0.92)90. However, in contrast with proteomic
age, which did not report disease associations, we find that iAge tracks with multiple
diseases and immunosenescence. In particular, we find a strong association between elevated
iAge and poor acute ex vivo immune responses, which is consistent with previous reports showing that high levels of baseline inflammatory markers correlate with weaker responses
to hepatitis B and herpes zoster vaccine formulations15,91. Similarly, inflammatory markers
have been shown to be, at least in part, responsible for a reduced JAK–STAT response to
cytokine stimulations in various leukocyte populations in our previous studies of aging28.
Despite the proven utility of cytokine stimulation assays used in our study with respect
to an individual’s overall immune competence5,24,25, one limitation of the assay relates to
the stimuli used here which may not completely mirror the physiological stimuli that act
on specific immune cell subsets in vivo. For example, while the stimuli we used strongly
activate the memory compartment of bulk CD8+ and CD4+ T cells, these act relatively
weakly on naive T cells. Additional cell subsets that are poorly activated by the cytokines
used in our study are type 1 helper T cells CD4+ T cells that can be activated by IL-12 and
IL-18 or type 17 helper CD4+ T cells, which respond to other cytokine stimulations such as
IL-1? or IL-18 in concert with IL-23 to produce type 17 helper T-cell-associated cytokines.
Recent findings from our group16,28 placed the immune system in the center of aging
phenotypes. Similar to our previous findings, our inflammatory clock metric specifically
hones in on the crucial role that the immune system and SCI play in the accumulation of
diseases of aging, with a focus on cardiovascular aging. Unlike other metrics of ‘biological’
age, which do not offer a clinically relevant metric92, we demonstrate that iAge predicts
multimorbidity and mortality and therefore can be used as a biological surrogate of age-
related health versus disease. iAge is directly associated with multiple disease phenotypes,
including cardiovascular aging, frailty, immune decline and exceptional longevity. In our
recent work16, we combined cellular phenotypes to describe subject- and population-level
immune aging phenotypes (IMM-AGE), which correlated with iAge. This suggests that
future research should leverage both immune-age scores to propose a unified metric that
reflects multiple aspects of immune aging, thus potentially providing a better clinical
predictive value.
A major contributor to the inflammatory clock, CXCL9, was validated as an indicator
of cardiovascular pathology independently of age. CXCL9 is a T-cell chemoattractant
induced by IFN-? and is mostly produced by neutrophils, macrophages and ECs. Despite
previous data showing that CXCL9 and other CXCR3 ligands are significantly elevated
in hypertension and in patients with left ventricular dysfunction41, we find that CXCL9
is mainly produced by aged endothelium and predicts subclinical levels of cardiovascular
aging in nominally healthy individuals. Some studies in humans have found CXCL9 to
increase with age93–98 and an age-dependent profile has also been observed in Chagas
disease99 and atopic dermatitis100. Notably, CXCL9 has also been shown to be associated
with falls in the older population101,102, which parallels our results predicting frailty.
At least two sources of CXCL9-mediated inflammation can ensue with aging based
on our findings; one that is age-intrinsic and observed in aging ECs and one that is
independent of age (likely as a response to cumulative exposure to environmental insults)
and found in the validation cohort of 97 apparently healthy adults. Notably, we did not
find any significant correlation between known disease risk factors reported in the study
(BMI, smoking, dyslipidemia) and levels of CXCL9 gene or protein expression. We thus
hypothesize that one root cause of CXCL9 overproduction is cellular aging per se, which
can trigger metabolic dysfunction (as shown in many previous studies of aging) with production of DAMPs. Examples of these include adenosine, adenine and N4-acetylcytidine
as demonstrated in our previous longitudinal studies of aging5. These DAMPs can then act
through the inflammasome machinery, such as NLRC4, to regulate multiple inflammatory
signals, including IL-1? and CXCL9 (ref. 103).
Our data also place the endothelium as a central player in cardiovascular aging, consistent
with previous findings104 and they also suggest that ECs may be one source of
inflammation, but it is also possible that cardiomyocytes play a role as in models of acute
myocardial infarction there is activation of the inflammasome NLRP3 in these cells105,106.
As ECs but not cardiomyocytes expressed the CXCL9 receptor, CXCR3 (Extended Data
Fig. 9), we hypothesize that this chemokine acts both in a paracrine fashion (when it
is produced by macrophages to attract T cells to the site of injury) and in an autocrine
fashion (when it is produced by the endothelium) creating a positive feedback loop. In this
model, increasing doses of CXCL9 and expression of its receptor in these cells leads to
cumulative deterioration of endothelial function in aging. Moreover, silencing of CXCL9
in ECs resulted in a reversal of the high inflammation/low proliferation early senescence
phenotype, which suggests by tackling CXCL9 it may be possible to delay onset of EC
senescence. It is also notable that IFN-?, a direct agonist to CXCL9, did not increase in
expression in our cellular aging RNA-seq experiment, suggesting that there are triggers of
CXCL9 (other than IFN-?) that play a role in cellular senescence in the endothelium that
are currently unknown. However, in our 1KIP study, IFN-? was in fact the second-most
important negative contributor to iAge, which could be explained by the cell-priming effect
of cytokines, where the effect of a first cytokine alters the response to a different one107–109.
In a more recent and refined version of this model (the high baseline-low output model for
chronic inflammation and the acute response) we show that sustained levels of inflammatory
mediators lead to nonfunctional constitutive phosphorylation of signaling pathways with
saturation of phosphorylation sites in signaling proteins (such as the JAK–STAT system),
which results in a lowered ? phosphorylation in response to acute stimuli and subsequent
dampening of the immune response to infections or vaccination28.
In conclusion, by applying artificial intelligence methods to deep immune monitoring
of human blood we generate an inflammatory clock of aging, which can be used as a
companion diagnostic to inform physicians about patient’s inflammatory burden and overall
health status, especially in those with chronic diseases. Furthermore, our immune metric
for human health can identify within healthy older adults with no clinical or laboratory
evidence of cardiovascular disease, those at risk for early cardiovascular aging. Lastly, we
demonstrate that CXCL9 is a master regulator of vascular function and cellular senescence,
which indicates that therapies targeting CXCL9 could be used to prevent age-related
deterioration of the vascular system and other physiological systems as well."
"An inflammatory aging clock (iAge) based on 
deep learning tracks multimorbidity, immunosenescence, frailty and 
cardiovascular aging","While many diseases of aging have been linked to the immunological 
system, immune metrics capable of identifying the most at-risk 
individuals are lacking. From the blood immunome of 1,001 individuals 
aged 8–96 years, we developed a deep-learning method based on patterns 
of systemic age-related inflammation. The resulting inflammatory clock 
of aging (iAge) tracked with multimorbidity, immunosenescence, frailty 
and cardiovascular aging, and is also associated with exceptional 
longevity in centenarians. The strongest contributor to iAge was the 
chemokine CXCL9, which was involved in cardiac aging, adverse cardiac 
remodeling and poor vascular function. Furthermore, aging endothelial 
cells in human and mice show loss of function, cellular senescence and 
hallmark phenotypes of arterial stiffness, all of which are reversed by 
silencing CXCL9. In conclusion, we identify a key role of CXCL9 in 
age-related chronic inflammation and derive a metric for multimorbidity 
that can be utilized for the early detection of age-related clinical 
phenotypes.","In this study, we conducted extensive 
immune monitoring in a large cohort of 1,001 individuals to identify 
immune biomarkers of aging and establish reference values for 
age-related systemic chronic inflammation. We used artificial 
intelligence to create a compact representation of these biomarkers and 
derived an ‘inflammatory clock’ of aging, which takes into account the 
nonlinear relationship and redundancy of the cytokine network. This 
metric tracked with multiple aging phenotypes in the general population 
and thus, has strong potential for translational medicine, as it could 
be used as a diagnostic tool for identifying those at risk for both 
noncommunicable and infectious diseases.Our nonlinear 
GAE method was optimal for the identification of iAge and its 
contributors. As with other deep-learning methods, GAE is capable of 
capturing complex relationships between analytes. Similar methods 
striving to extract signatures of aging have been described in different
 systems ranging from genome-wide association studies to proteomics. We 
summarize a few notable aging clocks in Supplementary Table 3.
 In brief, an epigenetic clock using markers measuring DNA methylation 
on CpG sites has been used to calculate an epigenetic age that was able 
to predict all-cause mortality74,75.
 It has also been associated with age-related diseases such as frailty, 
Alzheimer’s disease, Parkinson’s disease and cancer. Other clocks such 
as transcriptomic and microRNA clocks have also been shown to 
successfully capture aspects of the aging process that are different 
from epigenetic clocks. Instead of being associated with all-cause 
mortality or disease, transcriptomic clocks are associated with IL-6, 
albumin, lipids and glucose levels76. There have also been attempts to derive proteomic clocks and metabolomic clocks77–82
 of clinical relevance; however, iAge allows for new discoveries in the 
immune system. iAge derived from immunological cytokines gives us an 
insight into the salient cytokines that are related to aging and 
disease. A notable difference compared to other clocks is that iAge is 
clearly actionable as shown by our experiments in CXCL9 where we can 
reverse aging phenotypes. More practical approaches range from altering a
 person’s exposomes (lifestyle) and or the use of interventions to 
target CXCL9 and other biomarkers described here.Recent 
advancements in deep learning beyond traditional machine-learning 
methods have provided enormous opportunities to model biological age. 
Some of the most popular deep-learning architectures used to estimate 
biological age have been recurrent neural networks (RNNs), convolutional
 neural networks (CNNs), generative adversarial networks (GANs) and deep
 artificial neural networks (ANNs). RNNs have been used on face 
attributes and physical activities to estimate biological age83.
 Although the modality is not in the realm of biological markers, RNNs 
have potential to garner results in biological data that require 
positional relationships such as epigenetic age. CNNs and GANs have both
 been used to abstract facial attributes to predict chronological age84,85.
 GANs and CNNs are exceptional in abstracting images to distill useful 
information. Future applications of GANs and CNNs can be applied in 
other biological images such as magnetic resonance imaging; however, for
 now, these models are proof of concepts that they can accurately 
estimate cAge; they might not necessarily predict the health or lifespan
 of individuals. The deep-learning models that have been applied to 
modality used in this paper are the deep ANNs. ANNs have been applied to
 blood biochemistry markers and cell counts to derive biological age86,87.
 The results showed that such clocks are able to predict all-cause 
mortality, potentially finding biomarkers to intervene and steer 
individuals toward a healthier life.Some of the 
limitations of biological clocks in general is that they do not directly
 provide the mechanism by which they work. While it is possible to infer
 causality between aging and molecular biomarkers especially in the 
context of longitudinal or time-series data, individual biomarkers 
selected from biological aging clocks need to be experimentally tested 
to elucidate the underlying mechanism, as we have done in this study. 
Our GAE algorithm, a deep-learning method that efficiently deals with 
the network structure and nonlinear behavior of the inflammatory 
response, can extract high-level complex abstractions as ‘data 
representations’ using nonlinear functions and is well suited for the 
analysis of complex systems where most behaviors are nonlinear, 
context-dependent and organized in a distributed hierarchical fashion88.
 In our case, this method outperformed other commonly used linear 
modeling methods such as Elastic Net and PCA and also other nonlinear 
approaches such as plain auto-encoder89 (Extended Data Fig. 3b). The correlation between chronological age and iAge was 0.78 (P < 10?16) (Fig. 1a), which is lower than that of the recently reported ‘proteomic age’ metric (R = 0.92)90.
 However, in contrast with proteomic age, which did not report disease 
associations, we find that iAge tracks with multiple diseases and 
immunosenescence. In particular, we find a strong association between 
elevated iAge and poor acute ex vivo immune responses, which is 
consistent with previous reports showing that high levels of baseline 
inflammatory markers correlate with weaker responses to hepatitis B and 
herpes zoster vaccine formulations15,91.
 Similarly, inflammatory markers have been shown to be, at least in 
part, responsible for a reduced JAK–STAT response to cytokine 
stimulations in various leukocyte populations in our previous studies of
 aging28.
 Despite the proven utility of cytokine stimulation assays used in our 
study with respect to an individual’s overall immune competence5,24,25,
 one limitation of the assay relates to the stimuli used here which may 
not completely mirror the physiological stimuli that act on specific 
immune cell subsets in vivo. For example, while the stimuli we used 
strongly activate the memory compartment of bulk CD8+ and CD4+
 T cells, these act relatively weakly on naive T cells. Additional cell 
subsets that are poorly activated by the cytokines used in our study are
 type 1 helper T cells CD4+ T cells that can be activated by IL-12 and IL-18 or type 17 helper CD4+
 T cells, which respond to other cytokine stimulations such as IL-1? or 
IL-18 in concert with IL-23 to produce type 17 helper T-cell-associated 
cytokines.Recent findings from our group16,28
 placed the immune system in the center of aging phenotypes. Similar to 
our previous findings, our inflammatory clock metric specifically hones 
in on the crucial role that the immune system and SCI play in the 
accumulation of diseases of aging, with a focus on cardiovascular aging.
 Unlike other metrics of ‘biological’ age, which do not offer a 
clinically relevant metric92,
 we demonstrate that iAge predicts multimorbidity and mortality and 
therefore can be used as a biological surrogate of age-related health 
versus disease. iAge is directly associated with multiple disease 
phenotypes, including cardiovascular aging, frailty, immune decline and 
exceptional longevity. In our recent work16,
 we combined cellular phenotypes to describe subject- and 
population-level immune aging phenotypes (IMM-AGE), which correlated 
with iAge. This suggests that future research should leverage both 
immune-age scores to propose a unified metric that reflects multiple 
aspects of immune aging, thus potentially providing a better clinical 
predictive value.A major contributor to the inflammatory
 clock, CXCL9, was validated as an indicator of cardiovascular pathology
 independently of age. CXCL9 is a T-cell chemoattractant induced by 
IFN-? and is mostly produced by neutrophils, macrophages and ECs. 
Despite previous data showing that CXCL9 and other CXCR3 ligands are 
significantly elevated in hypertension and in patients with left 
ventricular dysfunction41,
 we find that CXCL9 is mainly produced by aged endothelium and predicts 
subclinical levels of cardiovascular aging in nominally healthy 
individuals. Some studies in humans have found CXCL9 to increase with 
age93–98 and an age-dependent profile has also been observed in Chagas disease99 and atopic dermatitis100. Notably, CXCL9 has also been shown to be associated with falls in the older population101,102,
 which parallels our results predicting frailty. At least two sources of
 CXCL9-mediated inflammation can ensue with aging based on our findings;
 one that is age-intrinsic and observed in aging ECs and one that is 
independent of age (likely as a response to cumulative exposure to 
environmental insults) and found in the validation cohort of 97 
apparently healthy adults. Notably, we did not find any significant 
correlation between known disease risk factors reported in the study 
(BMI, smoking, dyslipidemia) and levels of CXCL9 gene or protein 
expression. We thus hypothesize that one root cause of CXCL9 
overproduction is cellular aging per se, which can trigger metabolic 
dysfunction (as shown in many previous studies of aging) with production
 of DAMPs. Examples of these include adenosine, adenine and 
N4-acetylcytidine as demonstrated in our previous longitudinal studies 
of aging5.
 These DAMPs can then act through the inflammasome machinery, such as 
NLRC4, to regulate multiple inflammatory signals, including IL-1? and 
CXCL9 (ref. 103).Our data also place the endothelium as a central player in cardiovascular aging, consistent with previous findings104
 and they also suggest that ECs may be one source of inflammation, but 
it is also possible that cardiomyocytes play a role as in models of 
acute myocardial infarction there is activation of the inflammasome 
NLRP3 in these cells105,106. As ECs but not cardiomyocytes expressed the CXCL9 receptor, CXCR3 (Extended Data Fig. 9),
 we hypothesize that this chemokine acts both in a paracrine fashion 
(when it is produced by macrophages to attract T cells to the site of 
injury) and in an autocrine fashion (when it is produced by the 
endothelium) creating a positive feedback loop. In this model, 
increasing doses of CXCL9 and expression of its receptor in these cells 
leads to cumulative deterioration of endothelial function in aging. 
Moreover, silencing of CXCL9 in ECs resulted in a reversal of the high 
inflammation/low proliferation early senescence phenotype, which 
suggests by tackling CXCL9 it may be possible to delay onset of EC 
senescence. It is also notable that IFN-?, a direct agonist to CXCL9, 
did not increase in expression in our cellular aging RNA-seq experiment,
 suggesting that there are triggers of CXCL9 (other than IFN-?) that 
play a role in cellular senescence in the endothelium that are currently
 unknown. However, in our 1KIP study, IFN-? was in fact the second-most 
important negative contributor to iAge, which could be explained by the 
cell-priming effect of cytokines, where the effect of a first cytokine 
alters the response to a different one107–109.
 In a more recent and refined version of this model (the high 
baseline-low output model for chronic inflammation and the acute 
response) we show that sustained levels of inflammatory mediators lead 
to nonfunctional constitutive phosphorylation of signaling pathways with
 saturation of phosphorylation sites in signaling proteins (such as the 
JAK–STAT system), which results in a lowered ? phosphorylation in 
response to acute stimuli and subsequent dampening of the immune 
response to infections or vaccination28.In
 conclusion, by applying artificial intelligence methods to deep immune 
monitoring of human blood we generate an inflammatory clock of aging, 
which can be used as a companion diagnostic to inform physicians about 
patient’s inflammatory burden and overall health status, especially in 
those with chronic diseases. Furthermore, our immune metric for human 
health can identify within healthy older adults with no clinical or 
laboratory evidence of cardiovascular disease, those at risk for early 
cardiovascular aging. Lastly, we demonstrate that CXCL9 is a master 
regulator of vascular function and cellular senescence, which indicates 
that therapies targeting CXCL9 could be used to prevent age-related 
deterioration of the vascular system and other physiological systems as 
well."
Development of a prediction model of postpartum hospital use using an equity-focused approach,"Background
Racial inequities in maternal morbidity and mortality persist into the postpartum
         period, leading to a higher rate of postpartum hospital use among Black and Hispanic
         people. Delivery hospitalizations provide an opportunity to screen and identify people
         at high risk to prevent adverse postpartum outcomes. Current models do not adequately
         incorporate social and structural determinants of health, and some include race, which
         may result in biased risk stratification.Objective
      This study aimed to develop a risk prediction model of postpartum hospital use while
         incorporating social and structural determinants of health and using an equity approach.
      Study Design
      We conducted a retrospective cohort study using 2016–2018 linked birth certificate
         and hospital discharge data for live-born infants in New York City. We included deliveries
         from 2016 to 2017 in model development, randomly assigning 70%/30% of deliveries as
         training/test data. We used deliveries in 2018 for temporal model validation. We defined
         “Composite postpartum hospital use” as at least 1 readmission or emergency department
         visit within 30 days of the delivery discharge. We categorized diagnosis at first
         hospital use into 14 categories based on International Classification of Diseases-Tenth
         Revision diagnosis codes. We tested 72 candidate variables, including social determinants
         of health, demographics, comorbidities, obstetrical complications, and severe maternal
         morbidity. Structural determinants of health were the Index of Concentration at the
         Extremes, which is an indicator of racial-economic segregation at the zip code level,
         and publicly available indices of the neighborhood built/natural and social/economic
         environment of the Child Opportunity Index. We used 4 statistical and machine learning
         algorithms to predict “Composite postpartum hospital use”, and an ensemble approach
         to predict “Cause-specific postpartum hospital use”. We simulated the impact of each
         risk stratification method paired with an effective intervention on race-ethnic equity
         in postpartum hospital use.
      Results
      The overall incidence of postpartum hospital use was 5.7%; the incidences among Black,
         Hispanic, and White people were 8.8%, 7.4%, and 3.3%, respectively. The most common
         diagnoses for hospital use were general perinatal complications (17.5%), hypertension/eclampsia
         (12.0%), nongynecologic infections (10.7%), and wound infections (8.4%). Logistic
         regression with least absolute shrinkage and selection operator selection retained
         22 predictor variables and achieved an area under the receiver operating curve of
         0.69 in the training, 0.69 in test, and 0.69 in validation data. Other machine learning
         algorithms performed similarly. Selected social and structural determinants of health
         features included the Index of Concentration at the Extremes, insurance payor, depressive
         symptoms, and trimester entering prenatal care. The “Cause-specific postpartum hospital
         use” model selected 6 of the 14 outcome diagnoses (acute cardiovascular disease, gastrointestinal
         disease, hypertension/eclampsia, psychiatric disease, sepsis, and wound infection),
         achieving an area under the receiver operating curve of 0.75 in training, 0.77 in
         test, and 0.75 in validation data using a cross-validation approach. Models had slightly
         lower performance in Black and Hispanic subgroups. When simulating use of the risk
         stratification models with a postpartum intervention, identifying high-risk individuals
         with the “Composite postpartum hospital use” model resulted in the greatest reduction
         in racial-ethnic disparities in postpartum hospital use, compared with the “Cause-specific
         postpartum hospital use” model or a standard approach to identifying high-risk individuals
         with common pregnancy complications.
      Conclusion
      The “Composite postpartum hospital use” prediction model incorporating social and
         structural determinants of health can be used at delivery discharge to identify persons
         at risk for postpartum hospital use.","Our findings can be considered in the
context of existing literature on predic-
tion of PHU. A study using California
linked birth and hospital data to predict
60-day PHU had a slightly lower auROC
than ours (0.67 vs 0.69 for Composite
PHU model and 0.75 for Cause-specific
model).25 Our analysis considered a
greater number of candidate variables,
including SSDOH variables. Hospital
readmissions and ED visits are often
difficult to predict,26 in part because of
the heterogeneity of the outcome; thus, it
is logical that limiting the ensemble
model to a subset of reasons for PHU
yielded a higher auROC. Our overall
PHU model performed slightly worse in
Black and Hispanic people. Nonetheless,
because Black and Hispanic people were
more likely to have high predicted
probabilities, when we tested the poten-
tial equity impact of our model, we found
that coupled with an effective interven-
tion, our Composite PHU model had the
greatest impact on postpartum health
inequities. Our findings have implications
regarding the current concern with
obstetrical prediction algorithms that
include race as a covariate because they algorithm. 27 In obstetrics, algorithms
using race, including the VBAC
(Vaginal Birth After Cesarean) calcu-
lator 28,29 and anemia treatment guide-
lines, 30 have recently been revealed as
resulting in disparities in care. Because
the ultimate goal of identifying people
at risk of PHU is to intervene, predictive
models including race may result in
health care bias. Meanwhile, structural
racism and neighborhood deprivation
are increasingly recognized as impor-
tant determinants of maternal health
equity and postpartum read-
missions. 31,32 We found that the ICE, a
proxy for structural racism, was selected
in our final model. The SE index, likely a
downstream manifestation of structural
racism, performed similarly, and adding
race-ethnicity to our model did not
improve performance. These results
indicate that incorporating structural
determinants into clinical prediction
models may help identify those at risk of
poor health due to structural racism and
disadvantage.
could “embed” racial bias into the Postpartum interventions currently un-
der investigation often target groups
considered high-risk based on certain
morbidities, such as hypertension or
SMM. Although potentially effective in Postpartum interventions currently un-
der investigation often target groups
considered high-risk based on certain
morbidities, such as hypertension or
SMM. Although potentially effective in improving postpartum health for these
subsets, they only represent a minority of
the birthing population. Our whole-
population approach enabled us to cap-
ture birthing people who might not be
targeted for high-risk postpartum sup-
port. Indeed, our Composite PHU
model, when compared with a “stan-
dard” risk stratification approach, per-
formed better in terms of racial-ethnic
disparity reduction. Future research,
however, is needed to determine the
most effective way to implement our
postpartum prediction model to
improve postpartum care, also taking
into consideration feasibility and cost.
Potential multipronged interventions could address system- and hospital-level
SSDOH while providing wraparound
services to address clinical and
individual-level SSDOH and empower
patients. In addition, our model should
be externally validated in other geo-
graphic regions that may differ in the
distribution of SSDOH measures, po-
tentially using electronic medical record
data. Moreover, alternate geographic
definitions of area-based measures, such
as census tract or buffer zones, could be
tested. A primary strength of our analysis is our
equity approach. We used a large data set
with a rich array of candidate variables.
We chose area-based measures of SSDOH
that are publicly available nationally. We
went beyond reporting model perfor-
mance to investigate the potential of the
risk stratification model to affect post-
partum health equity. A primary limita-
tion is the lack of primary collected
patient data, such as blood pressure
measures or other biomarkers. The
Cause-specific model is limited by the
finding that the cause of readmission was
most frequently indicated by nonspecific
ICD-10 codes, hindering understanding
of the types of clinical situations that this
category represents. We did not have data
on stillbirths, which result in higher rates
of maternal readmission than live bi-
rths.33 We were not able to ascertain
obstetrical triage visits. Further, many ED
visits had a general ICD-10 code for
general perinatal reasons; outcome data
with more specific diagnosis codes may
have altered model performance. We did
not ascertain gender; thus, it is unclear if
our findings are generalizable to all
pregnancy-capable genders. In addition,
our predictive model has yet to be paired
with a real-life intervention to evaluate the
actual clinical impact of targeting post-
partum care using a prediction algorithm.
Conclusion
A Composite PHU model incorporating
SSDOH can be used to identify persons
at risk for PHU, with the potential to
reduce racial-ethnic inequities in post-
partum health. "
Community- and data-driven homelessness prevention and service delivery: optimizing for equity,"Objective: The study tests a community- and data-driven approach to homelessness prevention. Federal poli-
cies call for efficient and equitable local responses to homelessness. However, the overwhelming demand for
limited homeless assistance is challenging without empirically supported decision-making tools and raises
questions of whom to serve with scarce resources.
Materials and Methods: System-wide administrative records capture the delivery of an array of homeless serv-
ices (prevention, shelter, short-term housing, supportive housing) and whether households reenter the system
within 2 years. Counterfactual machine learning identifies which service most likely prevents reentry for each
household. Based on community input, predictions are aggregated for subpopulations of interest (race/ethnic-
ity, gender, families, youth, and health conditions) to generate transparent prioritization rules for whom to serve
first. Simulations of households entering the system during the study period evaluate whether reallocating
services based on prioritization rules compared with services-as-usual.
Results: Homelessness prevention benefited households who could access it, while differential effects exist for
homeless households that partially align with community interests. Households with comorbid health condi-
tions avoid homelessness most when provided longer-term supportive housing, and families with children fare
best in short-term rentals. No additional differential effects existed for intersectional subgroups. Prioritization
rules reduce community-wide homelessness in simulations. Moreover, prioritization mitigated observed reen-
try disparities for female and unaccompanied youth without excluding Black and families with children.
Discussion: Leveraging administrative records with machine learning supplements local decision-making and
enables ongoing evaluation of data- and equity-driven homeless services.
Conclusions: Community- and data-driven prioritization rules more equitably target scarce homeless resources.","The study demonstrates the feasibility of an iterative community-
and data-driven approach for effective and equitable homeless serv-
ice delivery. Findings address an essential gap between policy and
practice. 3,8,9,20 Federal guidelines require communities to allocate
scarce homeless assistance based on system-wide assessments of
household risk; 1 yet, little evidence supports the accuracy and cul-
tural validity of existing tools currently used for coordinated
entry.4,5 Furthermore, the scarcity of homeless services inherently
requires homeless providers to make continual moral preferences on
whom to serve first, with little ability to evaluate individual
decision-making and system goals.5
Our approach elicits feedback from key stakeholders to define
subpopulations of interest and relevant intersectional identities. In
this pilot, target households initially included those with comorbid
conditions, families with children, unaccompanied youth, African
Americans, and female-headed. Leveraging historical administrative
records, counterfactual machine learning shows transitional housing
reduces reentries the most for households with comorbidities, and
families with children and no comorbidities do best with rapid
rehousing during the study period, regardless of race/ethnicity, gen-
der, and age. The evidence informs transparent, easily implement-
able, and evaluable prioritization rules for targeting services that
minimize system reentries. Simulations of prioritization rules demonstrate larger reductions for subpopulations of interest (ie,
comorbidities and families) without perpetuating disparities by race/
ethnicity, gender, age, and unaccompanied youth. Results demon-
strate promise for efficient and equitable homeless service delivery
incorporating community- and data-driven insights.
The use of historical data in the feasibility study raises interesting
questions regarding generalizability and implementing community-
and data-driven homeless services. Evidence from the Great Reces-
sion supports targeting individuals with comorbidities for transi-
tional housing and families without comorbidities for rapid
rehousing to reduce reentry into homeless services. However, local
and temporal variations in economic conditions, policy priorities,
service capacities, and other conditions could produce different
results—a testable hypothesis. Likewise, prioritization depends on
the goals of service delivery and the information available; rules
could vary if optimizing on healthcare utilization, employment,
criminal involvement, social connectedness, etc., as opposed to
homelessness needs. Successful implementation requires meaningful
engagement with diverse community stakeholders to evaluate the
current service and data systems and develop goals for equity-driven
decision-making. Engagement also provides an opportunity to fore-
see and plan for potential unintended consequences from new serv-
ice processes that could perpetuate and exacerbate inequities.
Findings must be interpreted in the context of several conceptual
and methodological limitations. First, targeting homeless services
fails to address the lack of affordable housing that drives the over-
whelming demand for housing assistance. Although we demonstrate
an approach for articulating and evaluating ethical preferences for
scarce resource allocation, reforms that make safe and affordable
housing accessible to low-income households remain critical for just
homeless service delivery. Second, we need to consider the imple-
mentation challenges of introducing data-driven decision supports
into social services to demonstrate the approach’s feasibility. Simu-
lations automate resource allocation based on predicted success, but
decision-making systems must incorporate caseworker insights that
fail to appear in HMIS.38–40 Moreover, introducing data-driven
decision supports into homeless services introduces nontrivial
dynamics on how information is interpreted and used that could
generate unexpected outcomes.21,41 Rigorous research must con-
sider the intended and unintended consequences of prioritizing
scarce resources.
Finally, a series of technical issues limit insights from the model-
ing. Noteworthy, the data predate federal initiatives around coordi-
nated entry and housing first, and instead, allocations functioned
primarily as first-come-first-serve. Generating unconfounded treat-
ment effects could prove difficult with current data that includes
changing system preferences. Likewise, federal system performance
measures now focus on service receipt, not need. In contrast, our
outcome considers all re-requests for assistance regardless of avail-
ability that might be less prone to systematic exclusion from serv-
ices. Lastly, the model building requires considerable local tailoring
that meets community interests and the statistical assumptions nec-
essary for counterfactual estimation, such as measurement quality,
sample size, statistical power, etc. HMIS collects selected household
features that might not capture the highly dimensional mechanisms
underlying service delivery and intersectionalities of interest. Likewise, the local availability of linked predictors (e.g., Census)
and outcomes (eg, health outcomes) allow for tailored modeling
that could expand upon service effects on homeless reentry. The iter-
ative approach requires deep collaboration on the technical and sub-
stantive elements of model building."
"Chronic Care for All? The Intersecting Roles 
of Race and Immigration in Shaping Multimorbidity, Primary Care 
Coordination, and Unmet Health Care Needs Among Older Canadians


    
  ","Objectives: Despite the predominance of chronic disease clustering, primary care delivery for multimorbid patients tends
to be less effective and often uncoordinated. This study aims to quantify racial–nativity inequalities in multimorbidity prev-
alence ?3 chronic conditions), access to primary care, and relations to past-year subjective unmet health care needs (SUN)
among older Canadians.
Methods: Population-based data were drawn from the Canadian Community Health Survey (2015–2018). Multivariable
logistic regression was performed to estimate the likelihood of multimorbidity, sites of usual source of primary care (USOC),
primary care coordination, and multidimensional aspects of SUN. The Classification and Regression Tree (CART) was ap-
plied to identify intersecting determinants of SUN.
Results: The overall sample (n = 19,020) were predominantly (69.4%) Canadian-born (CB) Whites (1% CB non-Whites,
18.1% White immigrants, and 11.5% racialized immigrants). Compared with CB Whites, racialized immigrants were more
likely to have multimorbidity (adjusted odds ratio [AOR] = 1.35, 99% confidence interval [CI]: 1.13–1.61), lack a USOC
(AOR = 1.41, 99% CI: 1.07–1.84), and report higher SUN (AOR = 1.47, 99% CI: 1.02–2.11). Racialized immigrants’
greater SUN was driven by heightened affordability barriers (AOR = 4.31, 99% CI: 2.02–9.16), acceptability barriers
(AOR = 3.11, 99% CI: 1.90–5.10), and unmet needs for chronic care (AOR = 2.71, 99% CI: 1.53–4.80) than CB Whites.
The CART analysis found that the racial–nativity gap in SUN perception was still evident even among those who had access
to nonpoorly coordinated care.
Discussion: To achieve an equitable chronic care system, efforts need to tackle affordability barriers, improve service ac-
ceptability, minimize service fragmentation, and reallocate treatment resources to underserved older racialized immigrants
in Canada.","The current study estimated that close to 82% of older
Canadians (aged ?65 years) had at least one chronic con-
dition (greater than equal to three diseases: 29.6%) and
3.5% of them reported SUN in the previous year; how-
ever, despite a universal health system in Canada, racialized
immigrants had a disproportionately higher prevalence
of SUN (4.4%) and the highest rate of not having a usual
source of care (8.2%). The current investigation examined
whether and how racial–nativity disparities in health care
needs and access to care could exacerbate or attenuate the
unique SUN gap experienced by racialized immigrant older
adults under universal health coverage. The findings reveal
that, compared with CB Whites, racialized immigrants’ dis-
advantage in certain chronic illnesses and access-to-care
indicators could not explain racialized immigrants’ vulner-
ability to higher SUN perception. Alternatively, racialized
immigrants’ excess SUN perception was driven by their in-
creased exposure to affordability barriers, acceptability bar-
riers and possible undertreatment of physician-diagnosed
chronic physical disease. The overall finding that health
disparities between CB Whites and racialized immigrants
are larger once confounders are adjusted for is a crucial ob-
servation. As indicated in the descriptive profiles, racialized
migrants were somehow advantaged than CB Whites on
some of these sociodemographics in later life—they tend to
report higher income, higher educational level, be in marital status and have healthier lifestyles—and although these relative advantages were accounted for in the multivariable
models, the racial–nativity gaps in health and health care
needs were further enlarged, suggesting that there are other
more fundamental causes (e.g., systemic discrimination,
racism, nativism) in shaping the vulnerability for racialized
immigrants at old age (Krieger, 1999, 2014).
Inequities in Need: Chronic Disease
Multimorbidity
To the best of our knowledge, this is the first study to assess
the joint impact of race and nativity on multimorbidity prev-
alence among older adults in Canada. The multimorbidity
disadvantage observed in this study challenges previous re-
search that may overgeneralize the “monolithic healthy im-
migrant phenomenon” to racialized immigrants in later life.
On the contrary, prior Canadian immigrant research reported
lower odds of multimorbidity among immigrants (Geda
et al., 2021; Mondor et al., 2018) or no nativity disparities
(McDonald & Kennedy, 2004; Newbold, K. B. & Filice,
2006), although their chronic illness prevalence increased
upon arrival (Newbold, B., 2005). One explanation of mixed
findings is that previous studies typically homogenized immi-
grant populations across all races and life stages as a whole,
whereas this study distinguished racialized immigrants in
comparison to CB Whites with a specific focus on older age.
Through the racial equity lens, it would not be surprising
to observe White/non-White inequalities that are con-
sistent with racial health literature in Canada (Chiu et al.,
2015; Ramraj et al., 2016; Veenstra & Patterson, 2016),
which found that racial minorities were more prone to res-
piratory and circulatory system disorders. For example,
studies suggest that long-term racialized immigrants have
doubled the odds of developing diabetes compared with CB
Whites (Adjei et al., 2020), and they also had earlier onset
of diabetes than immigrants from the United Kingdom
(Tenkorang, 2017). The finding of multimorbidity disad-
vantage among racialized immigrants was in sharp contrast
with their profiles of healthier behaviors (the lowest rate of
regular drinking, smoking, and obesity) than CB Whites in
this sample, suggesting that (un) lifestyle factors were very
unlikely to be the fundamental causes. Alternatively, our
study implies that, for racialized immigrants, any FB health
advantage may be offset by cumulative exposure to various
societal stressors, such as racism, xenophobia, and unequal
life chances in the postmigration period (Brown, 2018;
Kobayashi & Prus, 2012; Lin, 2022b), that would lead to
the accumulation of chronic conditions at later life stages
(Bailey et al., 2017).
Inequities in Care: Usual Source of Care and Care
Coordination
This study represents one of the first attempts to quantify
racial–nativity disparities in USOC and care coordination
between HCP for older Canadians. Racial segregation in USOC has been well documented in the American health
care system (Arnett et al., 2016; Corbie-Smith et al., 2002;
Gaskin et al., 2007), whereby racial minority had a lower
propensity to use a physician’s office as USOC and more
likely to seek immediate care from ERs, hospital outpatient
departments, community health centers or to lack a USOC.
The current study did not replicate most American evi-
dence, with the exception that racialized immigrants tended
to lack a USOC than CB Whites. Moreover, contrary to
the expectation, immigrants, irrespective of race, were even
less likely to use hospital-based ambulatory care over a
doctor’s office as the USOC than CB Whites. There was no
significant racial–nativity difference in receiving poor/fair-
coordinated care from usual providers. These results may
collectively suggest that, under a universal health system
in Canada, racial–nativity inequities in access to care are
much less salient than in the United States (Lasser et al.,
2006; Siddiqi et al., 2016). However, the concerning finding
that poor people are more likely to use ER over doctor’s
office as USOC coincides with prior observed inequities in
ER utilization in Canada (Ouimet et al., 2015).
Inequities in Unmet Need: Barriers and
Undertreatment of Chronic Illness
The robust association between racialized immigrants and
higher perceptions of SUN in later life had not been re-
ported elsewhere. Previous research found that immigrants,
in general, had a lower rate of SUN than nonimmigrants
and established immigrants (>15 years) shared a similar
prevalence of SUN in Canada (Wu et al., 2005). In a lon-
gitudinal study, racialized female immigrants even reported
less SUN than non-immigrants over 12 years (Setia et al.,
2011). However, the current study did not support any of
these patterns, possibly owing to the focus on older popu-
lations whereas samples in prior research were mostly
working-age adults and racialized immigrants, if examined,
were significantly younger than their White peers. Instead,
a central finding of this study reveals that older racialized
immigrants had higher odds of SUN than CB Whites, a
pattern that could not be explained by health needs and
access-to-care factors but was attributable to barriers re-
lated to service acceptability and affordability.
Notably, almost half of such acceptability problems for
racialized immigrants (43.8%) were due to unmeasured
reasons (i.e., “other”—the leading source of disparities vs.
22.6% of CB Whites) beyond attitudinal concerns (e.g.,
“decided not to seek care”). This may reflect, in part, the
fact that immigrant and racialized communities often en-
counter discrimination within Westernized Canadian health
care settings (Edge & Newbold, 2013; Johnson et al., 2004;
Lin, 2021), including biased treatment, racial stereotyping
of medical noncompliance, cross-cultural misapprehen-
sions (Chowdhury et al., 2021), all of which could give rise
to acceptability barriers resulting in racialized immigrants’ delayed access to care. Considering that the SUN percep-
tion captured almost all types of health services, the excess
affordability barrier experienced by racialized immigrants
may stem from care services that were not covered by the
publicly funded health insurance (Martin et al., 2018), such
as dental care (Sano & Antabe, 2021).
It should be noted that the SUN gap (i.e., the magni-
tude of estimate) between racialized immigrants and CB
Whites was substantially accentuated after the block of
specific chronic conditions and medication use was con-
sidered, suggesting that multimorbidity should be an im-
portant factor to contextualize this inequality issue. It is
likely that certain chronic conditions—which were not di-
rectly associated with SUN (primarily driven by the dom-
inant CB White populations) but significantly correlated
with racial minority immigrants—such as greater cardio-
vascular burden of hypertension and diabetes may lead to
a larger racial–nativity discrepancy in the SUN perception.
Most importantly, this compounding effect was more pro-
nounced when the SUN perception was further restricted to
treatment for chronic physical conditions in the subsample
analysis of chronically ill patients, in which racialized im-
migrants almost have triple the likelihood of unmet needs
for chronic care than CB Whites. Such inequities in chronic
illness treatment pose a severe threat to self-management
practice (e.g., medication, monitoring, and testing) for ra-
cialized immigrant older adults, especially when SUN was
linked to future health decline as observed in longitudinal
research (Gibson et al., 2019).
Another key contribution is the finding of poor/fair care
coordination’s adverse association with SUN perception
among older adults with complex care needs, suggesting
that fragmentation of Canada’s health care system could
compromise universal coverage’s goal in achieving eq-
uitable care (Bodenheimer, 2008). It calls for redesigning
primary care infrastructures that could address insufficient
coordination for patients seen by multiple and/or interdis-
ciplinary care clinicians across different settings (Schoen
et al., 2009). Many promising models of integrated team-
based geriatric care have shown to be effective to improve
the continuity of care for older adults living with frailty,
such as the WHO’s Innovative Care for Chronic Conditions
framework (Epping-Jordan et al., 2004), and family health
team model (Rosser et al., 2011). Evidence has found that
diabetic patients in primary care networks (PCNs) were
more likely to be referred to an ophthalmologist/optome-
trist and had better glycemic control than patients outside
of the PCNs (Manns et al., 2012). Lastly, the intersection
of race–migration nexus and care coordination, as found
in CART analysis, indicates that the racial–nativity gap in
SUN perception was still evident even within a relatively
advantaged subgroup (i.e., those who received good coor-
dination between two or more health care providers and
having access to a regular providers). This finding suggests
that even under an integrated health care system, presum-
ably with well-coordinated services, is far from adequately
narrowing the racial–nativity gap in SUN; thus, targeted
policy efforts should be directed to deliver culturally sensi-
tive geriatric care.
Strength and Limitations
This study has several strengths, including its large sample
size, its ability to simultaneously examine social patterning
of health and health care inequalities, its pragmatic cate-
gorization of the race-immigration nexus, and its attention
to the clustering of chronic medical conditions. However,
several methodological biases limited the generalizability
of this study. First, this study relied on self-reported survey
data and therefore is susceptible to recall bias. Second,
many older adults with medical conditions may have not
been diagnosed or survey nonrespondents tend to have
significantly higher rates of chronic conditions; thus, these
under-reporting scenarios would possibly result in an un-
derestimate of multimorbidity prevalence. Third, due to
the small sample size and the resultant statistical power
issue, the estimates for CB non-Whites (N = 175) contain
large variance and should be interpreted with cautions. In
addition, due to the constraint of using PUMF, the meas-
urements related to heterogeneity and intragroup differ-
ences of racialized immigrant communities could not be
examined (Lin et al., 2020), such as admission class (e.g.,
refugee status), country of origin, and ethnic composition.
Furthermore, some covariates (e.g., community belonging-
ness) controlled in the model may be potential mediators,
future research should employ path analysis to investi-
gate the mechanisms that explain how the race–nativity
nexus could lead to health inequity and health care ineq-
uity under Canadian “universal” health system. Lastly, the
CCHS data lack information about more complex reasons
for SUN, such as discrimination during previous health
care interactions (Gaskin et al., 2007), medical mistrust
(Arnett et al., 2016), and so forth, which may help to ex-
plain health care inequities for racialized populations.
Conclusion
Overall, this population-based study of pooled cross-sec-
tional data has demonstrated racial–nativity inequities
in the prevalence of multimorbidity and unmet needs for
chronic care among older Canadians, inequities that dis-
proportionately affect racialized immigrants despite a
universal health system. At the policy level, the findings
underscore that both race and nativity, as intertwined so-
cial determinants of health, still play a fundamental role in
shaping the distribution of health-enhancing resources. At
the practice level, the findings recommend health and social
care professionals bridge the service gap by effective care
management and coordination. To align with the clinical
reality of the growing multimorbidity burden among mul-
ticultural geriatric patients, policymakers could make a dif-
ference if primary care reforms involve adopting a holistic care model, extending the health insurance coverage to
remove affordability barriers, redressing the power imbal-
ance in the provider–patient dynamic to improve service
acceptability and cultural responsiveness, minimizing frag-
mentation in health care services to facilitate care coordina-
tion, and reallocating treatment resources to underserved
racialized immigrant patients."
"Inequities in Mental Health Care Facing 
Racialized Immigrant Older Adults With Mental Disorders Despite 
Universal Coverage: A Population-Based Study in Canada


    
  ","Objectives: Contemporary immigration scholarship has typically treated immigrants with diverse racial backgrounds as a
monolithic population. Knowledge gaps remain in understanding how racial and nativity inequities in mental health care
intersect and unfold in midlife and old age. This study aims to examine the joint impact of race, migration, and old age in
shaping mental health treatment.
Methods: Pooled data were obtained from the Canadian Community Health Survey (2015–2018) and restricted to re-
spondents (aged ?45 years) with mood or anxiety disorders (n = 9,099). Multivariable logistic regression was performed to
estimate associations between race–migration nexus and past-year mental health consultations (MHC). Classification and
regression tree (CART) analysis was applied to identify intersecting determinants of MHC.
Results: Compared to Canadian-born Whites, racialized immigrants had greater mental health needs: poor/fair self-rated
mental health (odds ratio [OR] = 2.23, 99% confidence interval [CI]: 1.67–2.99), perceived life stressful (OR = 1.49, 99%
CI: 1.14–1.95), psychiatric comorbidity (OR = 1.42, 99% CI: 1.06–1.89), and unmet needs for care (OR = 2.02, 99% CI:
1.36–3.02); in sharp contrast, they were less likely to access mental health services across most indicators: overall past-year
MHC (OR = 0.54, 99% CI: 0.41–0.71) and consultations with family doctors (OR = 0.67, 99% CI: 0.50–0.89), psycholo-
gists (OR = 0.54, 99% CI: 0.33–0.87), and social workers (OR = 0.37, 99% CI: 0.21–0.65), with the exception of psychi-
atrist visits (p = .324). The CART algorithm identifies three groups at risk of MHC service underuse: racialized immigrants
aged ?55 years, immigrants without high school diplomas, and linguistic minorities who were home renters.
Discussion: To safeguard health care equity for medically underserved communities in Canada, multisectoral efforts need
to guarantee culturally responsive mental health care, multilingual services, and affordable housing for racialized immi-
grant older adults with mental disorders.","To my best knowledge, this is the first study that illustrates
how multiple social determinants interact in complex ways
to shape the use of mental health care for older popula-
tions. By contrasting health needs and service use, my study
reveals that, in a sample of Canadians aged 45 years and
older with mood/anxiety disorder diagnoses, there were
clear racial–nativity inequalities whereby racialized immi-
grants had a lower prevalence rate of MHC, despite having
greater mental health burden as evident by poor/fair SRMH,
perceived life stress, and mental morbidity than CB Whites;
and consequently, racialized immigrants had double the
likelihood of having unmet needs for care. This observed
disproportionality highlights that mental health needs
among racialized older immigrants were not adequately ad-
dressed in the current Canadian mental health care system.
This study makes three significant contributions to the lit-
erature on immigrant health and minority aging.
First, the findings highlight a serious equity problem
that social determinants are still playing an important
role in determining MHC, including the independent ef-
fects of race–migration nexus, older age, lower income,
lower educational attainment, and lack of a regular doctor.
Importantly, this study illuminates the joint effect of race,
migration, and old age on mental health care use (Lin,
2021). The service gap experienced by racialized immi-
grants may reflect multiple forms of barriers to mental
health treatment that are intrinsically constructed by
broader sociostructural determinants (Kalich et al., 2016;
Koehn et al., 2013; Lin, 2021; Wang et al., 2019). As elu-
cidated in the current study’s sample characteristics, racial-
ized older immigrants were more likely to speak nonofficial
languages at home, to be in the lowest income bracket, and
to use walk-in clinics for primary care, all of which consti-
tute obstacles to health care access (Thomson et al., 2015).
Past research has attributed cross-cultural differences in
health practices to racialized immigrants service underuse
within a health care system dominated by a Western bio-
medical paradigm (Lin, 2021; Reitmanova & Gustafson,
2009; Wang et al., 2019). Racialized immigrants may fear
being stigmatized and seek traditional ways of healing such
as acupuncture, herbal remedies, and other alternative
therapies (Na et al., 2016), rendering Western professional
treatments as the last resort (Fang, 2010).
Another contribution of the present study lies in the in-
vestigation of inequalities in access to diverse mental health
professionals across service sectors. This finding shows that
racialized older immigrants were underserved by family
doctors, psychologists, and social workers but not psy-
chiatrists. Because visits to primary care practitioners are
often patient-initiated, the underuse of family doctors may
likely be due to immigrants’ unfamiliarity with a primary
care-centric system in Canada (Tieu & Konnert, 2014)
where family physicians act as first-contact gatekeepers
to specialists (Kirmayer et al., 2011; Pandey et al., 2021;
Wang et al., 2008). In addition, this finding echoes previous
qualitative studies, which revealed that immigrants rarely
consult family doctors for mental health concerns, perhaps
because they perceived doctors’ role as primarily dealing
with physical problems. The lower probability of psychol-
ogist visits among racialized immigrants may result from
the compounding effect of financial barriers (Steele et al.,
2007). The sensitivity test reveals that the seemingly higher
prevalence of psychiatrist visits among racialized immi-
grants was primarily attributable to the severity of mental
health conditions. Once these conditions were adjusted for,
the initially positive association was no longer statistically
significant. Initial appointments with specialists including
psychiatric care often require medical referrals from family doctors. Considering racialized immigrants’ lower like-
lihood of doctor visits but a comparable pattern of psy-
chiatrist visits relative to Canadian-born White service
users, one may speculate that racialized immigrant patients
present more severe symptoms by the time they eventually
receive formal mental health treatment (Chen et al., 2003;
Na et al., 2016). It may reflect racialized immigrants’ coping
skills that often normalize emotional response to mental
suffering (Kleinman, 2004); and thus, they are less likely to
recognize mild psychological symptoms that require clin-
ical interventions (Kirmayer, 2001; Kirmayer et al., 2011).
This study is novel by adopting a data-driven ma-
chine learning approach to substantiate the utility of
intersectionality theory (Bauer, 2014; Harari & Lee, 2021)
and extends its application to the field of mental health care
inequalities. This intersectionality-informed analysis cap-
tures the interplay between multiple social forces including
race–migration nexus and old age in shaping health care
inequity (Kapilashrami & Hankivsky, 2018). It is worth
mentioning that both homeownership and primary lan-
guage at home stood out as important predictors of MHC
via decision tree modeling but not in traditional logistic
regression. This comparison suggests that the influence of
being home renters (e.g., housing insecurity) linguistic mi-
norities (e.g., language barriers) and home renter as bar-
riers to care were not independent, but rather operated
through the intersecting channels of each other. Language
barriers could take various forms—such as language/ac-
cent discrimination or the lack of bilingual health profes-
sionals—that could give rise to underutilization and unfair
treatment (Fang, 2010; Yoo et al., 2009). Because half of
linguistic minorities (58%) were racialized immigrants in
the current sample, the results further suggest that home-
ownership could be a salient health resource for racialized
immigrants to seek MHC, possibly due to higher stability
of housing, financial security, and autonomy (Finnigan,
2014; Swope & Hernández, 2019).
Limitations
The findings should be interpreted within the context of
limitations. First, the CCHS survey did not specify the time
frame when respondents received their mental health diag-
nosis (Pelletier et al., 2017); thus, it is difficult to ascertain
whether respondents had ongoing conditions with current
psychiatric symptoms, episodic conditions, or a history of
disorder that was already resolved at the time of the survey,
despite mental disorders being chronic and often recurrent.
The time frame of the past-year MHC may be subject to
recall bias (Bhandari & Wagner, 2006). Second, the reliance
on PUMF prohibits further investigations to provide more
nuances. For example, many important measurements re-
lated to the heterogeneity of immigrant communities could
not be examined via PUMF to contextualize research find-
ings, such as country of origin, admission class/purpose of
migration (e.g., refugee claimants, family reunification), or
ethnic compositions. Prior research had found disparities in
mental health service use by these intragroup immigration
characteristics (Durbin et al., 2015; Ng & Zhang, 2021). It
is also unclear whether the measure of “MHC other pro-
fessionals” capture the visits to religious counselors or tra-
ditional healers (e.g., herbalist, spiritualist) that could be
culturally responsive for racialized or immigrant clients, as
the PUMF did not disclose open-ended responses. Third,
the CCHS did not collect the information of informal sup-
port for mental health problems (e.g., communications
with family/relatives/friends), which often constitutes a pre-
ferred source of help among racialized immigrants such as
those with collective cultural orientation (Na et al., 2016).
Alternatively, measures of living arrangement and mar-
ital status were considered as rough proxies for informal
support availability in this study. Fourth, the cross-sec-
tional nature of the CCHS survey prevented inference of
the causal relationships between mental health needs and
service use. Some covariates (e.g., sense of community be-
longing) may serve as potential mediators and future re-
search could employ path analysis to investigate casual
pathways between the race–migration nexus and health
care inequity. In addition, due to the small sample size and
the resultant low statistical power issue, the estimates for
CB non-Whites (N = 95) were subjective to greater varia-
bility (i.e., wider confidence intervals) and it may reduce
chances of detecting a true effect for this vulnerable group.
Lastly, in the CCHS merged sample (2015–2018), respond-
ents with a mental disorder diagnosis (n = 9,099) differed
from individuals without a diagnosis (n = 63,925) to the
extent that they were more likely to be female (66% vs
53.1%), members of low-income household (<20k/year:
21.7% vs 9.3%), home renters (36.3% vs 22%), widow/
single people (55.6% vs 41.5%), those with have chronic
physical conditions (87.5% vs 76.3%), and patients at-
tached to a regular doctor (92.5% vs 89.1%, all ps < .05);
hence, cautions should be given when generalizing the find-
ings to the overall population.
Conclusion
The race–migration nexus in Canada continues to produce
discrepancies in mental health needs and mental health
care use among older persons with mental disorders. To
sum up, these findings make policy and clinical sense in
the context of interprofessional mental health care with
culturally and linguistically diverse clients. The findings
underscore that structurally vulnerable populations with
mental health conditions, including racialized immigrants
and socioeconomic disadvantage communities, are strug-
gling to get adequate treatment for their mental health
concerns in Canada. From a policy perspective, the find-
ings illustrate that the public-funded mental health serv-
ices (Medicare) delivered by safety-net providers such as
family doctors and social workers have been effective
in tackling socioeconomic inequities in mental health treatment. However, the remaining treatment gaps expe-
rienced by older racialized immigrants underline the im-
portance of expanding insurance coverage to additional
mental health services (e.g., psychotherapists) that are out-
side the narrow bracket of Medicare. Moreover, system-
level changes are needed for the federal government by
reallocating funding resources to alternative healing prac-
tices (e.g., religious counseling) that are responsive to
immigrants’ pluralistic understanding of mental health
challenges and cultural shaping of symptoms.
From a care delivery perspective, mental health profes-
sionals should respond adequately and collaboratively to
racialized immigrant older adults with mental disorders
who had entered the mental health care system to receive
a medical diagnosis. It is not solely a matter of training
clinicians to be culturally responsive in a way that incorp-
orates ethnocultural brokerage or ethnoracial pairing to
create a safe therapeutic space, but also a call for engaging
with a “structural competence approach” (Bourgois et al.,
2017; Metzl & Hansen, 2014) that could intervene in
broader systemic conditions affecting many racialized im-
migrants’ choices and behaviors (Zanchetta et al., 2021). It
is essential to implement upstream interventions in dealing
with fundamental social causes of health/illness (Link &
Phelan, 1995; Phelan & Link, 2005), such as housing in-
security (Chen et al., 2022), for older racialized immigrant
clients with mental disorders. For example, policymakers
and practitioners working in the immigration settlement
and mental health sectors could mirror the Housing First
Project, a paradigm shift in the delivery of community
mental health services primarily for homeless populations
with mental disorders (Aubry et al., 2015), and expand its
coverage to enables racialized immigrants’ access to per-
manent housing by providing them long-term rental assis-
tance in the hosting country. In the era of globalization and
mass migration, mental health clinicians should embrace an
anti-oppressive, anti-discriminatory approach to empower
racialized immigrant patients with mental disorders in ac-
cessing health-enhancing resources equitably at later life
stage (Hulko et al., 2020)."
"Effects of clinical, comorbid, and social determinants of
health on brain ageing in people with and without HIV:
a retrospective case-control study","Background Neuroimaging reveals structural brain changes linked with HIV infection and related neurocognitive
disorders; however, group-level comparisons between people with HIV and people without HIV do not account for
within-group heterogeneity. The aim of this study was to quantify the effects of comorbidities such as cardiovascular
disease and adverse social determinants of health on brain ageing in people with HIV and people without HIV.
Methods In this retrospective case-control study, people with HIV from Washington University in St Louis, MO, USA,
and people without HIV identified through community organisations or the Research Participant Registry were
clinically characterised and underwent 3-Tesla T1-weighted MRI between Dec 3, 2008, and Oct 4, 2022. Exclusion
criteria were established by a combination of self-reports and medical records. DeepBrainNet, a publicly available
machine learning algorithm, was applied to estimate brain-predicted age from MRI for people with HIV and people
without HIV. The brain-age gap, defined as the difference between brain-predicted age and true chronological age,
was modelled as a function of clinical, comorbid, and social factors by use of linear regression. Variables were first
examined singly for associations with brain-age gap, then combined into multivariate models with best-subsets
variable selection.
Findings In people with HIV (mean age 44·8 years [SD 15·5]; 78% [296 of 379] male; 69% [260] Black; 78% [295]
undetectable viral load), brain-age gap was associated with Framingham cardiovascular risk score (p=0·0034),
detectable viral load (>50 copies per mL; p=0·0023), and hepatitis C co-infection (p=0·0065). After variable selection,
the final model for people with HIV retained Framingham score, hepatitis C, and added unemployment (p=0·0015).
Educational achievement assayed by reading proficiency was linked with reduced brain-age gap (p=0·016) for people
without HIV but not for people with HIV, indicating a potential resilience factor. When people with HIV and people
without HIV were modelled jointly, selection resulted in a model containing cardiovascular risk (p=0·0039), hepatitis C
(p=0·037), Area Deprivation Index (p=0·033), and unemployment (p=0·00010). Male sex (p=0·078) and alcohol use
history (p=0·090) were also included in the model but were not individually significant.
Interpretation Our findings indicate that comorbid and social determinants of health are associated with brain ageing
in people with HIV, alongside traditional HIV metrics such as viral load and CD4 cell count, suggesting the need for
a broadened clinical perspective on healthy ageing with HIV, with additional focus on comorbidities, lifestyle changes,
and social factors.","By use of neuroimaging, machine learning, and model
selection, we have shown that a combination of clinical
measures, comorbidities, and social determinants of
health are associated with brain-predicted age in people
with HIV and people without HIV. Cardiovascular
disease burden, detectable HIV viral load, and hepatitis
C co-infection were identified as the strongest
univariate correlates of brain-age gap in people with
HIV. Additionally, the effects of social factors such as unemployment and area socioeconomic deprivation
were identified in multivariate regression. Differences
in significant variables between univariate and
multivariate analyses could have several causes. For
example, two predictors with high colinearity,
accounting for shared variance in the response variable,
could both show significant effects on brain-age gap in
independent univariate tests, but not in a multivariate
model.
Brain-age gap was also modelled in people without
HIV. Because our primary goal was to explain within-
group variability in brain ageing rather than test for
between-group differences, and due to sample size
and demographic differences, we elected not to do
head-to-head comparisons between HIV serostatus
groups. Best-subsets selection produced a multivariate
model for people without HIV that included significant
terms for alcohol use, early life stress, and Wide Range Achievement Test reading subscale. The Wide
Range Achivement Test showed a significant inverse
relationship with brain-age gap, indicating that
educational achievement might be a resilience factor for
brain ageing. Finally, modelling people with HIV and
people without HIV together implicated Framingham
score, alcohol use, Area Deprivation Index, unemployment,
male sex, and hepatitis C with older-appearing brain
phenotypes. Notably, HIV itself was not significantly
associated with brain-age gap when modelling these
other factors, suggesting the relative importance of non-
HIV drivers of brain ageing in the combination ART era.
Substantial evidence now implicates non-HIV risk and
resilience factors in ageing effects for people with HIV.23,24
Health disparities between people with HIV and people
without HIV partly reflect the legacy of early uncontrolled
infection, but these residual effects alone are insufficient
to explain the persistence of neurocognitive impairment
among people with well controlled HIV.25 As a result,
comorbidities and social determinants of health are
increasingly salient features in people with HIV with
suppressed viral loads, immune reconstitution, and the
expectation of longevity.
People with HIV have increased average brain-age gap
relative to seronegative peers; however, available data
indicate that within-group variability in brain ageing
exceeds between-group differences, and accounting for
heterogeneity is crucial.12,14 In this study, we approach the
question of brain ageing using an array of multimodal
predictors, including clinical measures, comorbid
disease burden, and social determinants of health. A
novel aspect of this study is the incorporation of
geospatial data on neighbourhood characteristics into
MRI data analysis.
The first group of factors that could affect brain
ageing are direct effects of HIV. We examined four key
variables: viral load, current CD4 lymphocytes, nadir
CD4 count, and hepatitis C co-infection. Detectable
viral load was significantly associated with elevated
brain-age gap, consistent with a large literature
implicating viral suppression and immune re-
constitution in preserved neurocognitive function.26
Hepatitis C was associated with approximately 4 years
of added brain-age gap in people with HIV, suggesting
that the pathological effects of HIV and hepatitis C have
additive effects on brain health.27 Thus, achieving
control of both viruses is likely to be important for
healthy brain ageing.
The strongest and most consistent brain-age gap
association was with Framingham cardiovascular risk
score. The modelled difference in brain-age gap between
individuals at minimum (<2%) and maximum (>60%)
cardiovascular risk in this study was over 10 years. In
univariate modelling, this association was significant
in people with HIV; however, the effect size was similar in
people without HIV, marking cardiovascular disease as a
good candidate for a general brain ageing risk factor.
However, it remains especially relevant for people with
HIV who have increased vascular disease compared
with the general population.28 These findings suggest
that maintenance of normal blood pressure and
cholesterol could be crucial for people with HIV who have established viral control but remain vulnerable to
cardiovascular disease.
Substance use disorders are also more prevalent
among people with HIV than among the general
population, and the effects of a history of drug misuse
must be considered when studying neurocognitive
deficits.29 Previous work has linked drug use with brain
structural and functional changes in people with HIV,
but associations with brain-age gap have not been
characterised. In multivariate analysis of people without
HIV, we found a positive association between brain-age
gap and alcohol use, potentially indicating that neurotoxic
effects of heavy consumption influence MRI-based brain
age. The absence of a similar effect in people with HIV
could be a function of the colinearity between alcohol use
and other factors (eg, cardiovascular disease) for which
stronger links were found.
One unexpected finding was the detection of a
protective effect of educational achievement in people
without HIV alone, in contrast with years of formal
education, which showed no significant association.
The Wide Range Achievement Test reading score had a
significant negative correlation with brain-age gap in
multivariate analysis, such that for each point of
improvement on the Wide Range Achievement Test,
the mean brain-predicted age was reduced by
0·45 years. The apparent absence of this effect in
people with HIV is challenging to interpret but might
indicate that the enhanced cognitive reserve conferred
by quality of education might not be fully realised in
people with HIV who experience clinical and social
stressors related to lower rungs on the hierarchy of
needs (ie, those related to safety, food security, or other
basic needs).
Social determinants of health were given consideration
in this study as economic instability and social
marginalisation disproportionately affect people with
HIV. In addition to education, we examined three major
social factors: childhood stress, residential neighbour-
hood quality from geospatially derived Area Deprivation
Index, and unemployment status. Although neither the
Early Life Stress Questionnaire or Area Deprivation
Index were associated with brain-age gap, Area
Deprivation Index had positive associations with brain-
age gap in the combined cohort model. Finally,
unemployment status showed a strong linkage with
increased brain-age gap in people with HIV, although
causality remains unclear because neurocognitive
impairment associated with accelerated brain ageing
might precede loss of employment.
Anatomically, the brain-age gap was interpreted by
correlation with FreeSurfer volumes. Although this
approach does not capture all the complex patterns
identified by the neural network, it provides an
approximation of relevant features. Results were
congruent with literature on brain structure and ageing:
positive associations with brain-age gap were confined to
CSF compartments and T1 white matter hypointensities,
whereas the strongest negative correlations were in
subcortical structures that atrophy with age, particularly
amygdala, hippocampus, and corpus callosum.30 These
results suggest that DeepBrainNet identifies ageing-
relevant imaging features.
Some limitations should also be noted. The use of over
10 years of participant data resulted in some differences
in the measures collected, producing a degree of data
incompleteness. To mitigate confounding effects of
missing values, we did a sensitivity analysis in the subset
of people with HIV with complete data. Results thus
obtained closely matched those derived from the full
dataset, indicating that missing data were unlikely to
drive results.
Use of self-reported data is another limitation. For
example, self-reported hepatitis C prevalence in people
with HIV (7%) was lower than expected, suggesting
unawareness of infection in some participants. However,
despite likely underestimation of co-infection, we
nonetheless detected a substantial effect on brain-age
gap (4·0 years increase) in people with HIV with
hepatitis C. Hepatitis C serostatus was not assessed in
people without HIV. Additionally, our sample represented
almost exclusively people who self-identified as Black or
White, but not other racial or ethnic groups. Furthermore,
people with HIV and people without HIV were
significantly different on self-identified race, sex, and
age, limiting the comparability of serostatus groups.
The use of best-subsets variable selection runs
some risk of overfitting since all possible predictor
combinations are modelled. This weakness is partly
mitigated by use of Mallows’ Cp, a selection criterion,
which penalises models with numerous predictors.22 For
further validation, we also did variable selection with
LASSO regression, an alternative method that uses
coefficient shrinkage to eliminate weaker predictors.
Again, results corresponded well to the main analysis,
suggesting that findings are robust to overfitting and
insensitive to methodology.
Taken together, these results paint a nuanced picture of
ageing with HIV. Traditional clinical variables such as
viral load and T-cell counts affect neuropathology;
however, non-HIV drivers of health such as comorbid
diseases and socioeconomic status are growing in
importance. Together, such factors could account for
heterogeneity in neurocognitive outcomes in older
people with HIV and people without HIV. Identification
of brain-ageing correlates could lead to a broadened
perspective on health for people ageing with chronic
infectious disease while navigating challenging and often
adverse socioeconomic landscapes."
"Comparison of Natural Language Processing of 
Clinical Notes With a Validated Risk-Stratification Tool to Predict 
Severe Maternal Morbidity","IMPORTANCE Risk-stratification tools are routinely used in obstetrics to assist care teams in
assessing and communicating risk associated with delivery. Electronic health record data and
machine learning methods may offer a novel opportunity to improve and automate risk assessment.
OBJECTIVE To compare the predictive performance of natural language processing (NLP) of
clinician documentation with that of a previously validated tool to identify individuals at high risk for
maternal morbidity.
DESIGN, SETTING, AND PARTICIPANTS This retrospective diagnostic study was conducted at
Brigham and Women’s Hospital and Massachusetts General Hospital, Boston, Massachusetts, and
included individuals admitted for delivery at the former institution from July 1, 2016, to February 29,
2020. A subset of these encounters (admissions from February to December 2018) was part of a
previous prospective validation study of the Obstetric Comorbidity Index (OB-CMI), a comorbidity-
weighted score to stratify risk of severe maternal morbidity (SMM).
EXPOSURES Natural language processing of clinician documentation and OB-CMI scores.
MAIN OUTCOMES AND MEASURES Natural language processing of clinician-authored admission
notes was used to predict SMM in individuals delivering at the same institution but not included in
the prospective OB-CMI study. The NLP model was then compared with the OB-CMI in the subset
with a known OB-CMI score. Model discrimination between the 2 approaches was compared using
the DeLong test. Sensitivity and positive predictive value for the identification of individuals at
highest risk were prioritized as the characteristics of interest.
RESULTS This study included 19 794 individuals; 4034 (20.4%) were included in the original
prospective validation study of the OB-CMI (testing set), and the remaining 15 760 (79.6%)
composed the training set. Mean (SD) age was 32.3 (5.2) years in the testing cohort and 32.2 (5.2)
years in the training cohort. A total of 115 individuals in the testing cohort (2.9%) and 468 in the
training cohort (3.0%) experienced SMM. The NLP model was built from a pruned vocabulary of
2783 unique words that occurred within the 15 760 admission notes from individuals in the training
set. The area under the receiver operating characteristic curve of the NLP-based model for the
prediction of SMM was 0.76 (95% CI, 0.72-0.81) and was comparable with that of the OB-CMI model
(0.74; 95% CI, 0.70-0.79) in the testing set (P = .53). Sensitivity (NLP, 28.7%; OB-CMI, 24.4%) and
positive predictive value (NLP, 19.4%; OB-CMI, 17.6%) were comparable between the NLP and
OB-CMI high-risk designations for the prediction of SMM.
CONCLUSIONS AND RELEVANCE In this study, the NLP method and a validated risk-stratification
tool had a similar ability to identify patients at high risk of SMM. Future prospective research is needed to validate the NLP approach in clinical practice and determine whether it could augment or
replace tools requiring manual user input.","The results of this study demonstrated that automated text processing of admission H&P notes
performed as well as the study institution’s current standard practice for SMM risk stratification,
which is a validated score manually calculated by clinical staff at the start of the admission. The NLP
approach had similar performance for the outcomes of maternal morbidity that included and
excluded transfusion, which is the most common component in the Centers for Disease Control and
Prevention–defined SMM metric. 26 Although the overall NLP model calibration was poor across the
range of all predicted probabilities, the approach identified a subset of individuals at significantly
higher risk of morbidity; this finding likely reflects that most pregnant individuals were healthy and
had no identifiable factors associated with morbidity. This study demonstrated that the NLP
approach identified a somewhat different subset of individuals as being at high risk for morbidity,
including an additional 15 of the 115 individuals who had SMM. We hypothesize that this result may
have been associated with (1) the OB-CMI score threshold of higher than 6 being used to define high
risk in the original validation study (eg, preeclampsia with severe features [OB-CMI score weight of
5] would not have been considered high risk unless other risk factors were present) and (2) the
relative inflexibility of a tool that has limited inputs by design to facilitate implementation.19 Among
individuals who met both the NLP and the OB-CMI criteria, 42.9% experienced SMM, highlighting a
small group at exceptionally high risk of an adverse outcome.
Most labor and delivery units rely on risk-stratification systems that require manually input
data. 19 Such tools require clinical staff to perform additional non–patient-facing tasks at the bedside
and collate information from multiple places within the EHR; this task burden is balanced with the
potential benefits of having an informed team that is prepared to mitigate and manage potential
adverse events. 31,32 The application of machine learning and artificial intelligence, such as NLP,
presents an innovative opportunity to improve clinical care, such as maternal risk-stratification
methods, without generating additional work for health care professionals. Although these
NLP-based analytic approaches are not routinely used within EHR systems yet, they are commonly
used in nonmedical applications (eg, internet searches) and have potential translatability to health
care. 21,22,33,34 Because clinical documentation is generated predictably with each hospital admission,
successful NLP-based analyses may ultimately have widespread generalizability across EHR
platforms.
We demonstrated the potential application of NLP in routine clinical documentation for risk
stratification of maternal morbidity. The next steps of this work involve the use of this
NLP-augmented risk-stratification approach in a pilot study to understand its performance in practice
and how this additional information may change behavior of health care professionals and teams as it relates to risk preparedness. To mirror the workflow and implementation of the OB-CMI tool during
the original prospective study, 19 we chose a screen-positive rate for the NLP model that was identical
to the rate when using the OB-CMI tool. The screen-positive threshold (for both the OB-CMI and the
NLP models) could be tailored for individual birthing units to align model performance with the
resources and personnel available in a given unit.
Strengths and Limitations
This study has strengths, including its large sample size, prospective manual assignment of the
OB-CMI score, and the relative simplicity of the NLP methods, all of which may facilitate future
implementation into practice. The study used clinical documentation that best reflected what was
known by the clinical team at the start of the delivery encounter, which contrasts with diagnosis
code–derived scores. This admission-based approach is also relevant for determining risk-
appropriate care.
This study also has limitations. The main limitation is its population sample, which was derived
from patients delivering at a single large academic center. Although health care professionals at the
center include obstetricians, certified nurse midwives, and trainees, all of whom may author
admission notes, the documentation practices may reflect institutional practices and local
vernacular. Prior NLP work has demonstrated that the individual vocabularies generated from a
single institution may limit the generalizability to other applications or centers; future external
validation studies should be conducted. 35 In some instances, H&P notes may have been written after
an outcome had already occurred; to limit the likelihood of this, we restricted the analysis to the
earliest written note before delivery during the encounter. This approach used monograms (single
words), which may fail to recognize concepts in more complex sentence structures (eg, negation);
whereas prior work by some of us showed that bigram and trigram models did not improve model
performance, 20 more advanced NLP methods could be used in future studies. As with all models,
performance characteristics are dependent on the probability thresholds used to define risk
categories; the sensitivity-specificity plots were included to show how these metrics would change
across all predictive probabilities for the NLP model. Risk of morbidity can evolve during admission as
additional data become available; the addition of intrapartum factors in future studies may allow for
a more tailored risk estimate that is updated throughout a patient’s delivery admission. In addition,
the outcome (SMM) was defined using diagnosis and procedure codes, which inherently lack
temporality, as is true of all population-based studies using encounter-level data; it is likely some
individuals had conditions classified as SMM at admission."
"Establishing an interdisciplinary research team for cardio-
oncology artificial intelligence informatics precision and health
equity","Study objective: A multi-institutional interdisciplinary team was created to develop a
research group focused on leveraging artificial intelligence and informatics for cardio-oncology
patients. Cardio-oncology is an emerging medical field dedicated to prevention, screening, and
management of adverse cardiovascular effects of cancer/ cancer therapies. Cardiovascular disease
is a leading cause of death in cancer survivors. Cardiovascular risk in these patients is higher
than in the general population. However, prediction and prevention of adverse cardiovascular
events in individuals with a history of cancer/cancer treatment is challenging. Thus, establishing
an interdisciplinary team to create cardiovascular risk stratification clinical decision aids for
integration into electronic health records for oncology patients was considered crucial.
Design/setting/participants: Core team members from the Medical College of Wisconsin
(MCW), University of Wisconsin-Milwaukee (UWM), and Milwaukee School of Engineering
(MSOE), and additional members from Cleveland Clinic, Mayo Clinic, and other institutions have
joined forces to apply high-performance computing in cardio-oncology.
Results: The team is comprised of clinicians and researchers from relevant complementary and
synergistic fields relevant to this work. The team has built an epidemiological cohort of ~5000
cancer survivors that will serve as a database for interdisciplinary multi-institutional artificial
intelligence projects.
Conclusion: Lessons learned from establishing this team, as well as initial findings from
the epidemiology cohort, are presented. Barriers have been broken down to form a multi-
institutional interdisciplinary team for health informatics research in cardio-oncology. A database
of cancer survivors has been created collaboratively by the team and provides initial insight into
cardiovascular outcomes and comorbidities in this population.","Clinicians, researchers, and individuals with diverse scientific backgrounds and
complementary interdisciplinary perspectives with a rich and wide knowledge base have
been brought together to establish a team of Cardio-Oncology Artificial Intelligence and
Precision (CAIP) research investigators (Figs. 1 and 2, Table 3). The group is composed of
members with expertise in wide-ranging disciplines. The team members bring key research
skills allowing the team to approach the overall problem of cardiovascular health among
cancer patients in new and innovative ways. Team principles evident in the group include
a high diversity of membership, deep knowledge integration, and geographic dispersion of
team members [32,33], as well as high task interdependence. The members of the team work
together and depend on each other to complete tasks. Individual tasks contribute to the larger
goal of the team and the project. The weekly meetings promote this aspect. As is typical in
other successful interdisciplinary teams [35], the team includes research and data scientists,
statisticians, clinicians, entrepreneurs, and trainees – with many individuals wearing more
than one of these hats.
Team project steps are foundational for subsequently building clinical decision aids
based on the artificial intelligence algorithm and other machine learning predictive
analytics, regionally. The team has created a database of ~5000 cancer survivors,
with laboratory, physiology, demographics, outcomes, medications, imaging, and medical
history information relevant to cardiovascular risk ascertainment in cancer survivors. This
information is consistent with the data needed for input to the artificial intelligence
algorithm published by our multi-institutional team members [27,28]. While the findings
from initial analyses are provided here, more granular analyses are ongoing, including
testing for interactions and adjusting for age and other potential confounders. Working
closely with the machine learning biostatistician, the data team members will collectively
apply the artificial intelligence algorithms to F&MCW patient data to ultimately build a
clinical decision aid for physicians to use with patients to help prevent cardiovascular
disease in cancer survivors. The primary long-term goal of team projects is to complete
the creation and integration of clinical decision aids in electronic health records for use
by cardiologists, oncologists, and other clinicians to improve outcomes for cardio-oncology patients using artificial intelligence models for more accurate prediction. As feedback from
clinician users will be incorporated into future iterations of the clinical decision aids, this
interdisciplinary clinical and translational research will thrive in the learning health system
in this illustrative “bedside to bench to bedside” approach.
Another goal being achieved by the team is building a stronger medical tech community
in SE WI, regarding artificial intelligence and data science. Expertise from CTSI member
institutions in SE WI in the team has been combined to create a bridge between the use
of high-performance supercomputing clusters at MCW and MSOE. This allows for running
algorithms and analyses on secure patient data in controlled environments by the team in this
newly developed informatics community in SE WI. The supercomputers are instrumental
in accurately reproducing output of the artificial intelligence algorithm prediction models
for outcomes of cancer patients with cardiovascular disease. Leveraging the state-of-the-art,
high-performance supercomputing center at MSOE facilitates one of MSOE’s strategic
initiatives of exceptional learning and discovery. Partnering with the CTSI and other member
institutions in SE WI on this community-engaged health informatics project embodies the
MSOE strategic plan’s byline, being Extraordinary Together [36].
An additional goal successfully being accomplished by the team is expanding collaborative
research in advanced medical machine learning applications among MCW, MSOE, and
UWM, combining strengths in medicine, advanced computing, and facilities, and strong
PhD research programs at UWM in electrical engineering, computer sciences, and health
sciences. The collaborative team will continue conducting research activities that will lead
to future extramural grant funding and ultimately to tangible clinical improvements. The
multi-institutional team will continue to integrate research into clinical workflows of the
health systems, analyze data from electronic health records, and advance the applications of
research-related information technology.
The team has parsed initial information regarding racial and ethnic minorities in the database
of ~5000 cancer survivors. The existing and future models are built on databases that
include African American cancer survivors. Application of these algorithms to data from
diverse patient populations will hopefully continuously provide output to help assess and
address health disparities in Milwaukee. Motivated by the core location in SE WI, team
studies on subpopulations from among the established epidemiological cohort of ~5000 will
intentionally over-sample ethnic and racial minorities among the population, particularly
African Americans, given the prevalence of related cardiovascular health disparities in
cancer survivors [15–19]. Indeed, research proposals in the team will purposefully recruit
sufficient representatives from this high-risk population to draw statistically significant
conclusions and examine the ability of this approach to improve disparities research
in cardio-oncology. This is the first of many collaborative team publications for the
dissemination of findings to the broader medical and scientific community. With the
publication of each collaborative study, the team will advance the establishment of this
multi-institutional center for high-performance computing informatics to protect the hearts
of cancer survivors, especially for racial and ethnic minorities. Publication of these findings
could be ground-breaking for furthering efforts at applying artificial intelligence algorithms
for optimal personalized patient care in cardio-oncology. The epidemiological cohort published by our team can serve as an external validation cohort for cross-institution
collaborations and consortia. In the Cardiology Oncology Innovation Network (COIN;
CardioOncCOIN.Org), for example, providing this large sample size for collaborating
institutions in this consortium can advance applications of artificial intelligence and other
health informatics methodologies to overcome current limitations in Cardio-Oncology
regarding small sample sizes in this specialized field.
Lessons learned in the team building process include the following. Team building
is necessary to build a foundation for successfully applying the artificial intelligence
algorithms in cardio-oncology. Highly organized and well-structured team meetings help
streamline and clarify team goals, vision, and output. Frequent participation and engagement
by all team members in team communications, whether in the large group meeting and/or
in the meetings of smaller teamlets, promotes unified forward in a singular direction. A
variety of perspectives on the team is essential to help sort out obstacles and surmount or
navigate roadblocks. Various backgrounds, experience, and expertise among team members
informs the best ways in which to take on and eliminate these obstacles. Understanding
differences among institutions regarding access to resources and managing of data and
so on help generalize these practices for groups performing team science. It is key that
the team as a whole creates and adheres to timelines and milestones in order to advance
and make timely and meaningful progress together. This is especially important when the
team has funding which requires periodic reports to the sponsoring group such as the
National Institutes of Health to keep track of the utilization of funds and achievement of
deliverables and milestone goals. Some team meetings are in the format of workshops as
needed, to enhance the development, training, and knowledge acquisition and translation of
the collective team relevant to the selected projects. Teaching of the team across education
levels and areas of expertise from multiple team members over the course of the team’s
tenure together can be useful to share knowledge among the group, so that team members
can better understand and participate in discussions of esoteric or specialized topics that are
needed for the research projects together. Challenges from previous literature that have also
been noted and overcome include scheduling conflicts for meetings albeit virtual, managing
multiple points of view, enhancing project management, and addressing “language barriers”
across disciplines [37–39]. In conclusion, interdisciplinary barriers have been overcome and
a multi-institutional team has been established, bringing the disciplines and skills required to
implement artificial intelligence algorithms in clinical decision aid for cardio-oncology that
will ultimately be integrated into the electronic health records to promote health equity and
improve health outcomes in individuals with cancer."
"COVID-19 Mortality Prediction From Deep 
Learning in a Large Multistate Electronic Health Record and Laboratory 
Information System Data Set: Algorithm Development and Validation","Background: COVID-19
 is caused by the SARS-CoV-2 virus and has strikingly heterogeneous 
clinical manifestations, with most individuals contracting mild disease 
but a substantial minority experiencing fulminant cardiopulmonary 
symptoms or death. The clinical covariates and the laboratory tests 
performed on a patient provide robust statistics to guide clinical 
treatment. Deep learning approaches on a data set of this nature enable 
patient stratification and provide methods to guide clinical treatment.Objective: Here,
 we report on the development and prospective validation of a 
state-of-the-art machine learning model to provide mortality prediction 
shortly after confirmation of SARS-CoV-2 infection in the Mayo Clinic 
patient population.Methods: We
 retrospectively constructed one of the largest reported and most 
geographically diverse laboratory information system and electronic 
health record of COVID-19 data sets in the published literature, which 
included 11,807 patients residing in 41 states of the United States of 
America and treated at medical sites across 5 states in 3 time zones. 
Traditional machine learning models were evaluated independently as well
 as in a stacked learner approach by using AutoGluon, and various 
recurrent neural network architectures were considered. The traditional 
machine learning models were implemented using the AutoGluon-Tabular 
framework, whereas the recurrent neural networks utilized the TensorFlow
 Keras framework. We trained these models to operate solely using 
routine laboratory measurements and clinical covariates available within
 72 hours of a patient’s first positive COVID-19 nucleic acid test 
result.Results: The
 GRU-D recurrent neural network achieved peak cross-validation 
performance with 0.938 (SE 0.004) as the area under the receiver 
operating characteristic (AUROC) curve. This model retained strong 
performance by reducing the follow-up time to 12 hours (0.916 [SE 0.005]
 AUROC), and the leave-one-out feature importance analysis indicated 
that the most independently valuable features were age, Charlson 
comorbidity index, minimum oxygen saturation, fibrinogen level, and 
serum iron level. In the prospective testing cohort, this model provided
 an AUROC of 0.901 and a statistically significant difference in 
survival (P<.001, hazard ratio for those predicted to survive, 95% CI 0.043-0.106).Conclusions: Our
 deep learning approach using GRU-D provides an alert system to flag 
mortality for COVID-19–positive patients by using clinical covariates 
and laboratory values within a 72-hour window after the first positive 
nucleic acid test result.: ","In this study, we collected and processed over 50 laboratory and 
clinical covariates in a population of nearly 12,000 Mayo Clinic 
patients who tested positive for SARS-CoV-2 by PCR. In this large and 
geographically diverse data set, we found that the GRU-D RNN could 
provide state-of-the-art mortality prediction. This performance remained
 strong even in a held-out test set that mimics how a deployed system 
would be trained retrospectively and then prospectively utilized in a 
clinically evolving pandemic setting. Our cross-validation experiments summarized in Table 2
 indicated that the top performing model to predict mortality in our 
cohort was the GRU-D RNN. We thus selected the GRU-D method to predict 
the mortality of patients with COVID-19 and prospectively found an AUROC
 of 0.901, accuracy of 78% (95% CI 76%-79%), recall of 85% (95% CI 
77%-91%), precision of 14% (95% CI 12%-17%), negative predictive value 
of 99% (95% CI 99%-100%), and a statistically significant difference in 
survival (P<.001, hazard ratio for those predicted to 
survive, 95% CI 0.043-0.106). As can be expected in prospective 
validation, we observed a modest drop in AUROC although most of the 
performance characteristics were close to their original 
cross-validation estimates, that is, the negative predictive value was 
largely unchanged, while precision and accuracy showed minor decreases 
with the recall showing modest improvements.We 
chose a prospective/retrospective split in time since this is the most 
realistic way to assess the potential performance of a system if 
launched clinically, because it would be trained on data up until its 
go-live date and then run prospectively in a potentially evolving 
pandemic environment. Notably, the cutoff date for the 80/20 split 
creating the prospective test set was December 15, 2020, which is the 
day after the first COVID vaccine received the United States Food and 
Drug Administration approval, meaning that our prospective cohort 
represented a distinctly different clinical environment compared to the 
period in which the model was trained. The relatively minor loss of 
performance in prospective validation shows the robustness of the 
modeling herein, but the observed loss of performance also demonstrates 
the need for continued retraining/validation of such a model during a 
constantly evolving pandemic. 
The application and deployment of ML methods in 
clinical practice require concerted care and diligence. One may be 
inclined to interpret the high negative predictive value of our 
prediction algorithm as an indication that the best use of the algorithm
 in practice is as a screening mechanism to discharge patients who are 
not at risk in order to conserve resources for higher-risk individuals. 
However, such a conclusion illustrates a pitfall of using a correlative 
prediction algorithm to make causal conclusions. The algorithm is highly
 confident that under the current standards of care at Mayo Clinic, 
these individuals are not likely to succumb to their illness; this is 
quite distinct from asserting that it is safe to reduce the care for 
these patients. Arriving at this latter conclusion would likely require a
 randomized controlled trial, and given the much lower survival rate 
published in the New York City data set [11]
 where medical systems were overcapacity, it seems unlikely that 
reducing care from those who survived in our cohort would have been a 
safe measure. Because the Mayo Clinic health systems have not been 
overcapacity, our mortality predictions should be viewed as representing
 patient stratification when full clinical support is available.Therefore,
 we conclude that the algorithm is better deployed as an alert system 
that flags only those patients it deems as high risk to provide the 
treating physician with an additional data point that aims to summarize 
the many covariates and the laboratory values routinely available. In 
this context, the algorithm has had abundant experience in the 
provider’s system, effectively “seeing” all patients with COVID-19 that 
have attended Mayo Clinic and conveying these lessons to physicians who 
could not have gained such experience personally.A web interface 
to this model may allow for widespread usage but given the complexity 
and error-prone nature of users providing the high dimensional 
time-series measurements with correct units, the system is better suited
 for integration within the EHR/LIS infrastructure. We are now exploring
 the details of deployment of such a GRU-D alert system, which involves 
discussions with physicians to assess numerous implementation details, 
for example, deciding whether the alerts would be passive 
EHR/chart-based flags or a direct page to the frontline clinical 
provider. Passive chart alerts are less intrusive to existing workflows 
(ie, a direct page interrupts a physician while tending to other 
patients) but also provide less-immediate feedback. Additionally, active
 alerts could also be sent to a triage group to consider if evaluation 
is needed (for example, from the registered respiratory therapist) 
rather than interrupting bedside clinicians. Furthermore, for either 
type of alert, there is the question of prescribing a bedside assessment
 or leaving it to provider discretion, which is again a matter of 
balancing disruption of the workflow with the likelihood of missing a 
critical event. There will not be a universally appropriate 
implementation for all hospital systems owing to staffing and procedural
 differences. However, since our algorithm predicts overall COVID-19 
mortality and is not tailored to flag imminent events such as 
cardiopulmonary arrest, it may be appropriate to consider less intrusive
 chart alerts without prescribed bedside follow-ups.We have also 
seen nuances in the challenges and opportunities presented by MNAR data.
 In the context of traditional statistical inference and imputation, 
MNAR data is a worst-case scenario so challenging that many practical 
applications effectively ignore the reality and proceed with algorithms 
designed for the missing completely at random or missing at random 
settings. A diligent statistician making this decision may perform a 
sensitivity analysis under a very limited set of assumed MNAR mechanisms
 to provide some assurances regarding the robustness of the chosen 
imputation or analytical strategy [18].
 However, here we have demonstrated that classification problems can be 
quite distinct in this regard. Specifically, if the missing data 
mechanism is tightly coupled to the ultimate prediction task, it is 
entirely possible for MNAR data to be an asset rather than an 
impediment. One can construct a context where the class label is so 
tightly linked to the missing data mechanism that the patterns of 
missingness provide more discriminative power than the underlying values
 themselves (see Multimedia Appendix 2) [19].
 In LIS systems, the number of potential laboratory tests that could be 
ordered at any time is astronomical, and it is unlikely that a 
practicing physician will ever order a “complete observation” of every 
test available on a single patient at every point in time. Instead, 
tests are ordered based on reasonable clinical suspicion that a test 
might return an abnormal result. From a prognostication point of view, 
this clinical suspicion is an enormously valuable piece of information 
that will almost never be captured in a structured data field in the 
EHR. If an algorithm cannot build off of this clinical suspicion as a 
starting point, it is also likely that its conclusions may appear to be a
 “step behind” the ordering clinician. Instead, an algorithm should 
learn what it can from the MNAR data patterns (here partly encoding 
clinical suspicion) in addition to the final value returned by the 
laboratory test.We also note some of the real-world challenges 
that are faced when attempting to deploy such an alert system into 
clinical practice. First, in the retrospective experimental design 
followed here and by other papers in the literature, the time series 
data are constructed using the time of sample collection since this is 
the most biologically accurate way to represent the data and build 
predictive models. However, in practice, if there can be delays in the 
turnaround for certain tests, this will either result in delayed 
predictions (so that the deployed testing data match its retrospective 
training counterpart) or result in biased predictions when delayed 
laboratory test results are treated as missing. Therefore, although 72 
hours is early in the course of illness, it is crucial that we have 
demonstrated reasonable performance even when only considering data 
collected on the same day as the first positive PCR result, because a 
real-world delay of 48 hours on certain laboratory test values may occur
 during a global pandemic, and thus, it is critical that the system can 
still provide accurate and timely predictions even when laboratory test 
results are delayed. Additionally, with vaccines now being delivered, 
the models presented herein should be considered as mortality 
predictions for an unvaccinated individual, and in practice, a 
vaccinated individual will be expected to be at low risk for mortality 
based on the clinical trials data.Another 
challenge in dealing with LIS data comes from nonstandardization of test
 coding prior to reporting to the EHR. In a multisite system, the same 
laboratory test may have multiple test codes to account for the 
different ordering facilities or variability in local billing 
regulations. This creates the potential for discrepancies in the values 
stored within the underlying database such as differing units of 
measure. Substantial effort is therefore devoted to linking the LIS 
results to the EHR to ensure consistency across test codes and complete 
coverage of results in the EHR. The COVID-19 pandemic has created added 
complexity due to the rapidly evolving and continuously updating 
availability of COVID-19 nucleic acid and antibody tests. Therefore, 
effective data collection and deployment of ML methodologies 
necessitates extensive team-based laboratory and medical expertise to 
ensure that data aggregation and modeling efforts can be rapidly 
modified to suit the changing nature of the underlying data set. 
Scalability also presents practical challenges. This is illustrated by a
 scenario in which internal workflows began to fail due to limitations 
in the number of query results being returned by Tableau, necessitating 
that SQL queries take place on a high-performance computing cluster 
using a Python/Pandas toolchain. Although these logistical challenges 
may be of limited academic interest, they are important to document, as 
such barriers have been a greater impediment to rapid real world 
deployment than more traditional topics in the ML literature such as the
 identification of appropriate classification algorithms. 
For context, in Table 4,
 we summarize some of the largest published COVID-19 mortality studies 
and specifically, the cohorts analyzed and the most relevant features 
identified. When smaller cohorts see insufficient numbers of deaths for 
direct mortality prediction, studies tend to focus on the prediction of 
severe outcomes. For instance, in a cohort of 123 patients with COVID-19
 in Vulcan Hill Hospital, China, in the study of Pan et al [20],
 the mortality classifier based on XGBoost yielded an AUC of 0.86-0.92. 
Likewise, in a cohort of 372 Chinese cases (99.7% cohort survival rate),
 Gong et al [9]
 found that the following variables provided an AUROC of 0.85. 
Similarly, in a study of 375 patients with COVID-19 conducted by Ko et 
al [21],
 the mortality prediction model based on XGBoost had 92% accuracy. In a 
study of 398 COVID-19–positive patients by Abdulaal et al [22],
 86% accuracy was achieved (95% CI 75%-93%). In a large study of 2160 
cases over 54 days from 3 hospitals in Wuhan, China with sufficient 
cases to assess mortality (88% cohort survival rate), Gao et al [8] reported 0.92-0.98 as the AUROC using an ensemble classifier. Furthermore, Vaid et al [11]
 used 4098 inpatient cases over 68 days in New York City (83% cohort 
survival rate) to achieve an AUROC of 0.84-0.88 in mortality prediction.
 Kim et al [23]
 studied 4787 patients and their XGBoost-based classifier demonstrated 
an AUC of 0.88-0.89 (95% CI 0.85-0.91) in predicting the need for 
intensive care, which is distinct from mortality prediction. Bolourani 
et al [24]
 studied 11,525 patients to achieve an AUROC of 0.77 in predicting 
respiratory failure within 48 hours of admission, which is also distinct
 from mortality prediction, based on data from the emergency department 
by using an XGBoost model.The dramatically different cohort 
mortality rates and the associated predictive accuracies may be in part 
due to the differing straining of the local health care systems at the 
time of study (both Wuhan and New York City experienced waves of 
patients that at different times overwhelmed the health care 
infrastructure), and the relatively geographically narrow nature of each
 of these data sets underscores why it is unlikely that these mortality 
predictions would extend directly to our patient population in a health 
care system spanning 3 time zones and multiple locales unrepresented in 
the literature.As indicated in Table 4,
 this study represents the largest cohort collected for mortality 
prediction in COVID-19, and the GRU-D algorithm shows state-of-the-art 
performance. Notably, many papers selected models based on XGBoost, 
which also showed strong cross-validation performance in our data. 
However, Table 2 demonstrates that XGBoost was not even in the top 5 algorithms that we assessed. Additionally, in agreement with Gao et al [8],
 we find that ensemble algorithms such as AutoGluon can provide stronger
 performance, although as noted previously, the GRU-D algorithm ended up
 ranked most highly in our cross-validation experiments."
"
Natural language processing for the assessment of cardiovascular disease comorbidities: The cardio-Canary comorbidity project


    
  ","Objective: Accurate ascertainment of comorbidities is paramount in clinical
research. While manual adjudication is labor-intensive and expensive, the adop-
tion of electronic health records enables computational analysis of free-text doc-
umentation using natural language processing (NLP) tools.
Hypothesis: We sought to develop highly accurate NLP modules to assess for the
presence of five key cardiovascular comorbidities in a large electronic health record
system.
Methods: One-thousand clinical notes were randomly selected from a cardiovascular
registry at Mass General Brigham. Trained physicians manually adjudicated these
notes for the following five diagnostic comorbidities: hypertension, dyslipidemia, dia-
betes, coronary artery disease, and stroke/transient ischemic attack. Using the open-
source Canary NLP system, five separate NLP modules were designed based on
800 “training-set” notes and validated on 200 “test-set” notes.
Results: Across the five NLP modules, the sentence-level and note-level sensitivity,
specificity, and positive predictive value was always greater than 85% and was most
often greater than 90%. Accuracy tended to be highest for conditions with greater
diagnostic clarity (e.g. diabetes and hypertension) and slightly lower for conditions
whose greater diagnostic challenges (e.g. myocardial infarction and embolic stroke)
may lead to less definitive documentation.
Conclusion: We designed five open-source and highly accurate NLP modules that
can be used to assess for the presence of important cardiovascular comorbidities in
free-text health records. These modules have been placed in the public domain and
can be used for clinical research, trial recruitment and population management at any institution as well as serve as the basis for further development of cardiovascular
NLP tools.","Through a meticulous development and validation process, we
designed five highly accurate NLP modules that can be used to assess
for the presence of important cardiovascular comorbidities in free-
text electronic health records. When putting our metrics in the con-
text of other methods of extracting such data—such as using ICD billing codes—it is clear that rigorous NLP modules have the potential to
significantly improve the accuracy of coding cardiovascular comorbid-
ity data. Across all five modules, we almost always achieved sensitiv-
ity, specificity, and PPV of greater than 90%. This compares to
sensitivities as low as 35% for stroke,6 61% for hypertension2 and
57% for coronary artery disease2 in previously published work on the
accuracy of ICD coding for the ascertainment of cardiovascular risk
factors.
Unlike administrative billing codes which are coded for episodi-
cally and intermittently, our NLP modules accurately extract data
from individual sentences within free-text documentation. This allows for a significant increase in the sensitivity of extracting such
data, especially for patients who have only a limited number of
medical encounters. Additionally, because administrative billing
codes were not designed for medical research purposes, they are
subject to both miscoding and under-coding, realities which signifi-
cantly impact their validity. Our NLP modules demonstrate the
power of accurately extracting data from the rich narrative of free-
text documentation that is the backbone of clinical electronic
health data.
Another commonly used approach for computational analysis of
text is statistical analysis, also known as machine learning. Machine
learning methods can also attain high accuracy but typically result in
“black box” models where reasons for categorization of a particular
piece of text are not clear to an external observer. This leads to diffi-
culties in adaptation of machine learning-based NLP tools between
different institutions that may have distinct clinical vernacular and
forces development of NLP tools from scratch at every organization
and for every task, consuming scarce resources and impeding progress
of the field.29 With that in mind, in this study we pursued the
approach of a more transparent, human-designed heuristic-based NLP
technology that allows tracing of each step of text analysis as well as
easy modification of NLP tools to correct errors or add new function-
ality. We have placed the NLP modules we have designed in the pub-
lic domain.30 We expect that their portability and transparency will
allow them to serve as the foundation for a family of cardiovascular
NLP tools that could be used for population management, clinical
research, and clinical trial recruitment across multiple healthcare
organizations.
Additional strengths of our work include the rigorous manual
adjudication process by physicians of the training and test set notes,
the accuracy of our modules, and the ability of our NLP systems to
extract granular data from sentence-level documentation. Further-
more, given that the repository of notes used for both the training
and test sets spanned from the years 2000–2019 within a large medi-
cal system, our NLP modules likely capture the majority of linguistic
formulations used to describe the clinical diagnoses of interest.
Despite the accuracy of our modules, our NLP system has some
limitations. First, because our NLP modules extract data only from
narrative notation—without being able to corroborate diagnoses
with primary data such as imaging or laboratory results—it cannot
determine if a given sentence contains accurate or inaccurate infor-
mation. Accordingly, if a clinician mistakenly documented that a
given patient has a history of coronary artery disease, our systems
will not be able to recognize that error. Second, although the over-
all accuracy of our modules was excellent, the performance of our
modules on the disease subcategories (such as the type of diabetes,
CAD subcategory, and type of stroke) is harder to categorize given
that there was a limited number of such sub-diagnoses present in
the test set notes. Finally, because our clinical notes came from a
large cardiovascular repository from two academic medical centers
in the United States, the performance of our modules on other sets
of documentation or those from other institutions may be
different. The accurate extraction of data from clinical records is critically
important for prospective and retrospective clinical research, including
for recruitment for clinical trials and for population-based studies. As
demonstrated through our work, NLP has the potential to accurately
identify disease states from the electronic medical record, enabling
the robust description of baseline characteristics. Our five NLP
modules—specifically built to identify individuals with cardiovascular
disease comorbidities—is a highly accurate and open-source system
that will allow researchers to better understand the baseline charac-
teristics of the patients in their research cohorts."
Use of Machine Learning Models to Predict Death After Acute Myocardial Infarction,"Importance:     
Accurate prediction of adverse outcomes after acute myocardial 
infarction (AMI) can guide the triage of care services and shared 
decision-making, and novel methods hold promise for using existing data 
to generate additional insights.
Objective:
To evaluate whether contemporary machine learning methods can 
facilitate risk prediction by including a larger number of variables and
 identifying complex relationships between predictors and outcomes. Design, setting, and participants:
This cohort study used the American College of Cardiology Chest 
Pain-MI Registry to identify all AMI hospitalizations between January 1,
 2011, and December 31, 2016. Data analysis was performed from February 
1, 2018, to October 22, 2020.
Main outcomes and measures:
Three machine learning models were developed and validated to 
predict in-hospital mortality based on patient comorbidities, medical 
history, presentation characteristics, and initial laboratory values. 
Models were developed based on extreme gradient descent boosting 
(XGBoost, an interpretable model), a neural network, and a 
meta-classifier model. Their accuracy was compared against the current 
standard developed using a logistic regression model in a validation 
sample.
Results:
A total of 755 402 patients (mean [SD] age, 65 [13] years; 495 202
 [65.5%] male) were identified during the study period. In independent 
validation, 2 machine learning models, gradient descent boosting and 
meta-classifier (combination including inputs from gradient descent 
boosting and a neural network), marginally improved discrimination 
compared with logistic regression (C statistic, 0.90 for best performing
 machine learning model vs 0.89 for logistic regression). Nearly perfect
 calibration in independent validation data was found in the XGBoost 
(slope of predicted to observed events, 1.01; 95% CI, 0.99-1.04) and the
 meta-classifier model (slope of predicted-to-observed events, 1.01; 95%
 CI, 0.99-1.02), with more precise classification across the risk 
spectrum. The XGBoost model reclassified 32 393 of 121 839 individuals 
(27%) and the meta-classifier model reclassified 30 836 of 121 839 
individuals (25%) deemed at moderate to high risk for death in logistic 
regression as low risk, which were more consistent with the observed 
event rates.
Conclusions and relevance:
In this cohort study using a large national registry, none of the 
tested machine learning models were associated with substantive 
improvement in the discrimination of in-hospital mortality after AMI, 
limiting their clinical utility. However, compared with logistic 
regression, XGBoost and meta-classifier models, but not the neural 
network, offered improved resolution of risk for high-risk individuals.
    
  

  


              
            ","
In this cohort study, in a large national registry of 
patients with AMI, machine learning models did not substantively improve
 discrimination of in-hospital mortality compared with models based on 
logistic regression. However, 2 of these models were associated with 
improvement in the resolution of risk over logistic regression and with 
improved classification of patients across risk strata, particularly 
among those at greatest risk for adverse outcomes. One of these models, 
XGBoost, is interpretable and represents the collection of 
individualized decision trees that address complex relationships among 
variables. The second model, meta-classifier, which aggregated 
information from multiple machine learning models, also had better model
 calibration than logistic regression. Despite almost no improvements in
 discrimination, these models led to reclassification of 1 in every 4 
patients deemed moderate or high risk for death with logistic regression
 as low risk, which was more consistent with their observed event rates.
 However, machine learning models were not uniformly superior to 
logistic regression, and a neural network model had worse performance 
characteristics than a logistic regression model based on the same 
inputs.            
The study builds on prior studies5-13,15 that used machine learning in predicting AMI outcomes. Most of these studies5-13,15
 found improved prediction with applications of classification 
algorithms of varying complexity. However, they were limited by smaller 
patient groups, with limited generalizability in the absence of standard
 data collection.5-13,15
 In a large national registry with standardized data collection across 
more than 1000 hospitals, improvements in risk prediction for 
in-hospital mortality with machine learning models were small and likely
 do not meet the threshold to be relevant for clinical practice.            
However, there are notable aspects of the new models. 
Without the cost of collecting additional data or a reliance on 
literature review or expert opinion for variable selection, the models 
achieved similar model performance characteristics as logistic 
regression, which is relevant for predictive modeling in clinical areas 
where disease mechanisms are not well defined. Moreover, 2 of the 3 
models were much better calibrated across patient groups based on age, 
sex, race, and mortality risk and were therefore better suited for risk 
prediction despite only modest improvement in overall accuracy. Notably,
 this improvement in predictive range occurred in critical areas by 
accurately reclassifying individuals at high risk to categories more 
accurately reflecting their risk. A focus on traditional measures of 
accuracy underperform in capturing the scale of these improvements 
because the events are rare and model discrimination is driven by 
patients not experiencing the mortality event.28,29
 In this respect, the Brier score offers a more comprehensive assessment
 of model performance, combining model discrimination and calibration. 
The Brier score represents the mean squared difference between the 
predictions and the observed outcome. A perfect model has a Brier score 
of 0, and when 2 models are compared, a smaller Brier score indicates 
better model performance. Both XGBoost and meta-classifier models had 
scores that were lower than the logistic models by several multiples of 
the SDs of the score. Given the only marginal improvements in model 
discrimination, the lower Brier scores reflect the improved calibration 
noted in the calibration slope and shift tables.            
Of note, 1 of the models that performs well is 
interpretable because it represents a collection of decision trees, 
thereby ensuring transparency in its application that specifically 
addresses the concerns with black-box machine learning models. 
Furthermore, although their development is computationally intensive, 
their eventual deployment at an individual patient level does not 
require substantial computational resources. Therefore, the clinical 
adoption of these models likely depends on whether their gains in 
prediction accuracy are worth their computationally intensive 
development and lack of interpretability. Some machine learning models 
may, therefore, have greater clinical utility in higher-dimensional data
 where they can uncover complex relationships among variables30-32
 and of variables with outcomes but only provide limited gains in 
relatively low-dimension registry data. Furthermore, not all machine 
learning performed well. The neural network model developed using all 
available variables in the registry was inferior to the logistic 
regression based on similar inputs, indicating that not all machine 
learning models are uniformly superior to traditional methods of risk 
prediction.            
                        

                            
                                Limitations
                            
                        
            
This study has limitations. First, although the CP-MI 
registry captures granular clinical data on patients with AMI, relevant 
information, such as duration of comorbidities and control of chronic 
diseases (besides diabetes), was not captured in the registry and is, 
therefore, not included in the assessment. Furthermore, certain 
prognostic characteristics of the patients’ general health are not 
included.33,34
 Second, although models are based on sound mathematical principles, the
 study does not identify whether the excess risk identified with the 
models is modifiable. Third, shift tables judge classification across 
risk thresholds but may overemphasize small effects around thresholds. 
However, other calibration metrics also suggest more precise risk 
estimation by XGBoost and the meta-classifier among patients classified 
as being at high risk by logistic regression. Fourth, the study was not 
externally validated. Therefore, although the observations may be 
generalizable to the data in the NCDR CP-MI Registry, they may not apply
 to patients not included or hospitals not participating in the 
registry. However, because the data are collected as a part of routine 
clinical care at a diverse set of hospitals, other hospitals that 
collect similar data could likely apply these modeling strategies."
"A machine learning approach to identify
distinct subgroups of veterans at risk for
hospitalization or death using administrative
and electronic health record data","Background
Identifying individuals at risk for future hospitalization or death has been a major priority of
population health management strategies. High-risk individuals are a heterogeneous group,
and existing studies describing heterogeneity in high-risk individuals have been limited by
data focused on clinical comorbidities and not socioeconomic or behavioral factors. We
used machine learning clustering methods and linked comorbidity-based, sociodemo-
graphic, and psychobehavioral data to identify subgroups of high-risk Veterans and study
long-term outcomes, hypothesizing that factors other than comorbidities would characterize
several subgroups.
Methods and findings
In this cross-sectional study, we used data from the VA Corporate Data Warehouse, a
national repository of VA administrative claims and electronic health data. To identify high-
risk Veterans, we used the Care Assessment Needs (CAN) score, a routinely-used VA
model that predicts a patient’s percentile risk of hospitalization or death at one year. Our
study population consisted of 110,000 Veterans who were randomly sampled from
1,920,436 Veterans with a CAN score?75th percentile in 2014. We categorized patient-level
data into 119 independent variables based on demographics, comorbidities, pharmacy, vital
signs, laboratories, and prior utilization. We used a previously validated density-based clustering algorithm to identify 30 subgroups of high-risk Veterans ranging in size from 50 to2,446 patients. Mean CAN score ranged from 72.4 to 90.3 among subgroups. Two-year mortality ranged from 0.9% to 45.6% and was highest in the home-based care and meta-
static cancer subgroups. Mean inpatient days ranged from 1.4 to 30.5 and were highest in
the post-surgery and blood loss anemia subgroups. Mean emergency room visits ranged
from 1.0 to 4.3 and were highest in the chronic sedative use and polysubstance use with
amphetamine predominance subgroups. Five subgroups were distinguished by psychobehavioral factors and four subgroups were distinguished by sociodemographic factors.
Conclusions
High-risk Veterans are a heterogeneous population consisting of multiple distinct sub-
groups–many of which are not defined by clinical comorbidities–with distinct utilization and
outcome patterns. To our knowledge, this represents the largest application of ML clustering
methods to subgroup a high-risk population. Further study is needed to determine whether
distinct subgroups may benefit from individualized interventions.","In this retrospective cohort study of patients identified as high-risk by a predictive algorithm,
we found substantial heterogeneity in subgroups of high-risk patients using a density-based
clustering algorithm and variability in health care utilization (inpatient admission range 10.5–
98.6%) and mortality (range 0.9–45.6%). Four of 30 subgroups were identified by predomi-
nantly sociodemographic rather than clinical features. To our knowledge, this is one of the first applications of machine learning to subgroup high-risk patients using EHR-enriched
administrative data.
We identified 30 distinct subgroups among a heterogeneous national population of high-
risk Veterans who receive care within the VA. Several clusters were identified by non-comor-
bidity-based -factors, including sociodemographic (e.g. high Medicaid and/or Hispanic pre-
dominant) and psychobehavioral (e.g. polysubstance abuse, psychoses) characteristics.
Compared to existing work, this study has several unique strengths. Many traditional methods
of defining high-risk individuals rely on pre-specifying the number of subgroups or defining
risk using expert-driven frameworks [14], which may not account for certain factors, including
health care use patterns or adverse social factors, that contribute to risk. In contrast, our clus-
tering methods did not pre-specify numbers of subgroups and indeed did not cluster nearly
25.2% of all Veterans-. This ensured greater homogeneity within the identified subgroups. The
identification of subgroups identified by comorbidity-based or non-comorbidity-based fea-
tures provides a greater opportunity to develop, test, and implement tailored interventions.
Identifying homogenous subgroups of high-risk populations, whether related to comorbidity-
based, demographic, or psychobehavioral characteristics, is a critical first step.
Our clustering utilized both routinely collected administrative data and detailed utilization
and laboratory data. Some identified subgroups have been previously described in other work
to cluster high-risk individuals, such as liver disease, congestive heart failure, and renal disease
clusters [15]. By utilizing richer data related to utilization, laboratory values, and pharmacy
data that may vary over time, we were able to identify distinct subgroups that have not rou-
tinely been identified by traditional methods and were characterized by sociodemographic
characteristics. As an example, we identified a group characterized by a high prevalence of
home-based care that would not have been separately identified if utilization data was not used
to define subgroups. This group had high mortality but relatively low inpatient or emergency
room utilization. Management strategies would likely differ between the home-based care sub-
group–who may have low mobility and cognitive deficits and for whom efforts such as hospice
and telemedicine may be prioritized–and a subgroup with high utilization but low mortality,
such as the uncomplicated surgery subgroup–where counselling prior to readmission may be
prioritized. Thus, our insights may have operational relevance within the healthcare system by
using a routinely-used, prospective method to identify high-risk individuals.
As another example, we identified several clusters characterized by psychobehavioral fac-
tors. We identified four clusters of polysubstance abuse—sedative predominance, amphet-
amine predominance, opioid predominance, and not otherwise specified–with distinct
patterns of inpatient utilization and mortality, since patients using sedatives and amphet-
amines had higher rates of inpatients utilization (54.2%, 51.1%) than the opioid group (36.9%)
and higher mortality (9.2%, 6.0% vs. 4.3%). Current case management approaches to polysubstance abuse have been primarily based on those with opioid use disorder [33]. Our results
suggest that groups characterized by predominantly sedative or amphetamine use have distinct
patterns of outcomes and may warrant targeted substance use disorder programs.
Another significant advantage of our clustering method is that it does not assign every
high-risk Veteran to a subgroup. Several clustering algorithms, including k-means and hierarchical clustering, require assignment to a cluster; thus, there may be considerable heterogeneity within a cluster due to a high predominance of outliers. Of note, other clustering methods,
including latent class analysis, also do not. Indeed, 25.2% of our cohort was identified as an
outlier–an individual who, potentially due to unique combinations of comorbidities, utilization patterns, and/or other features–was not sufficiently similar to other members of a cluster.
These represent individuals who may have characteristics that are shared across multiple sub-
groups or are not captured (e.g. other sociobehavioral characteristics), potentially necessitating increased diversity in treatment programs to meet varying treatment needs. Identifying these
outliers is important, as care management solutions can be costly and resource-intensive to
deploy on a population-wide level. This may illustrate the need for additional survey data to
unmask Veterans who are not easily subgrouped. While our approach may identify outliers
who need to be further characterized, there may be logistical limitations given the relatively
large proportion of individuals who were unable to be clustered.
These findings have important implications for the design of programs targeted at high-risk
or high-needs populations. Currently, the VA and other organizations often suggest a range of
varied interventions for high-risk individuals. Unsupervised learning could inform which sub-
groups of high-risk patients have distinct characteristics or patterns of utilization. While all
will not be targetable, certain care interventions may be more appropriate for certain sub-
groups over others. Furthermore, these subgroups could facilitate testing and randomized
experimentation of programs for specific subgroups, so-called “precision delivery”, in order to
test hypotheses about care delivery [34, 35]. Having prior knowledge of Veteran population
sub-structures could benefit the estimation of optimal intervention strategies using statistical
methods for precision medicine.
There are several limitations to this analysis. First, this analysis was performed among a
cohort of Veterans and may be difficult to generalize to non-Veterans. However, our popula-
tion represented a diverse cohort with a variety of comorbidities and is representative of high-
risk individuals across the US [6, 22]. Second, we did not have access to non-VA data, limiting
our ability to examine utilization outside of the VA. However, our data source is representative
of what is used in standard VA operations to identify high-risk individuals. Third, computa-
tional constraints required us to limit our sample to 110,000 Veterans. However, we took sev-
eral steps to ensure generalizability across the entire Veteran population, including employing
a stratified sampling strategy to reflect the CAN distribution across the VA and using a num-
ber of representative training sets to tune model parameters before clustering a final represen-
tative subsample of Veterans. Fourth, we did not have access to several relevant socioeconomic
covariates (e.g., income, education level), although we did include indirect metrics such as
enrollment priority and Medicaid status. We specifically did not include zip- or area-level
socioeconomic metrics because there is significant heterogeneity in socioeconomic outcomes
in these areas, and thus we limited ourselves to available individual-level data. Fifth, several
identified clusters were relatively small, representing <1% of the population. It is unclear
whether such subgroups would be large enough to warrant an individual system-wide pro-
gram. Finally, while we used more detailed socioeconomic data to subgroup high-risk Veter-
ans, our identification of high-risk Veterans was based on primarily clinical data. However, we
used a standard operational algorithm to identify such high-risk Veterans in order to ensure
operational relevance."
"Using Machine Learning to Differentiate Risk of Suicide Attempt
and Self-Harm after General Medical Hospitalization of Women
with Mental Illness","Background: Suicide prevention is a public health priority, but risk factors for suicide after
medical hospitalization remain understudied. This problem is critical for women, for whom
suicide rates in the United States are disproportionately increasing.
Objective: To differentiate the risk of suicide attempt and self-harm following general medical
hospitalization among women with depression, bipolar disorder, and chronic psychosis.
Methods: We developed a machine learning algorithm that identified risk factors of suicide
attempt and self-harm after general hospitalization using electronic health record data from 1,628
women in the University of California Los Angeles Integrated Clinical and Research Data
Repository (UCLA-xDR). To assess replicability, we applied the algorithm to a larger sample of
140,848 women in the New York City Clinical Data Research Network (NYC-CDRN).
Results: The classification tree algorithm identified risk groups in UCLA-xDR (Area Under the
Curve [AUC] 0.73, sensitivity 73.4, specificity 84.1, accuracy 0.84), and predictor combinations
characterizing key risk groups were replicated in NYC-CDRN (AUC 0.71, sensitivity 83.3,
specificity 82.2, and accuracy 0.84). Predictors included medical comorbidity, history of
pregnancy-related mental illness, age, and history of suicide-related behavior. Women with
antecedent medical illness and history of pregnancy-related mental illness were at high risk (6.9-17.2% readmitted for suicide-related behavior), as were women < 55 years old without
antecedent medical illness (4.0-7.5% readmitted).
Conclusions: Prevention of suicide attempt and self-harm among women following acute
medical illness may be improved by screening for sex-specific predictors including perinatal
mental health history.","In this study, we developed a predictive model of medically serious suicide attempt and self-
harm following general hospitalization among women with serious mental illness. We used a
machine learning approach to identify key predictors and combinations of predictors
differentiating hospitalizations followed by a suicide-related readmission from
hospitalizations followed by a non-suicide related readmission. By applying this approach in
two separate populations spanning diverse demographics, case mixes, geographies, and
health systems, we derived an aggregate model highlighting common risk groups.
The model identified index hospitalizations at high risk for suicide-related readmission
(accuracy 0.84) when applied to a moderately-sized population from a single institution
(8,408 hospitalizations) and when subsequently applied to a multi-institution data network
two orders of magnitude larger (841,834 hospitalizations). The most important predictors of
suicide-related readmission were antecedent medical illness, history of suicide-related
behavior, age, and history of pregnancy-related mental illness. Notably, the classification
trees demonstrated consistent patterns across datasets, replicating common predictor
combinations characterizing high risk hospitalizations. The model performed comparably to
other predictive models of suicide attempts based on EHR data (AUC 0.71-0.84)28-30 and
similarly to other published EHR-based models of clinical prediction (0.83), hospitalization
(0.71), and service utilization (0.71)31. Our approach suggests that risk of suicide-related behavior may be best characterized by
combinations of predictors, rather than single linear relationships between individual
predictors and outcome. Antecedent medical illness (i.e. degree of medical comorbidity prior
to index hospitalization) was the most important risk factor in both datasets. Presence of
antecedent medical illness alone did not differentiate risk, but rather determined which
combinations of predictors were relevant in differentiating risk. For example, women with
moderate-to-high antecedent medical illness were at elevated risk if they experienced prior
suicide-related behavior or pregnancy-related mental illness, whereas women with low or no
antecedent medical illness were at elevated risk if they were younger than 55-years-old. This
finding affirms recent studies emphasizing the importance of considering multiple
interacting and interdependent predictors when modeling suicidality risk.32,33 
Our study adds to the literature by providing the first model of suicide-related behavior
focusing on women with concomitant medical and mental illness. Because women
experience different patterns of suicide-related behavior compared with men,34,35 suicide
screening and prevention strategies during medical hospitalization may be advanced by the
assessment of sex-specific risk factors. Recent work suggests men, relative to women, may
be more vulnerable to suicidality following physical illness.32 However, in general there has
been a paucity of information on women’s risk for suicide after acute medical illness.
Current suicide screening protocols generally do not assess sex-specific predictors36, such as
history of pregnancy-related mental illness.
We found that history of pregnancy-related mental illness was associated with increased risk
of suicide-related behavior after (non-obstetric) medical hospitalization, particularly among
women with moderate-to-high antecedent medical comorbidity. Substantial work has
focused on characterizing suicide-related behavior during pregnancy and the peripartum,
with the most evidence for rise in suicide risk during the postpartum period.12,37,38
Vulnerability to sex hormone shifts has been posited as a mechanism for the enduring
predisposition to mental illness following postpartum depression, particularly during
menopause.39 Our finding that a history of pregnancy-related mental illness is associated
with suicidality after medical hospitalization in a large sample of predominantly post-
menopausal women (mean age 57.5-60 years) may suggest additional risk mechanisms. For
example, women who have had a history of pregnancy-related mental illness may be
particularly susceptible to stressors associated with acute medical illness and medical
intervention, including loss of identity, threats to autonomy, and health/illness transitions.
Moreover, women who experienced psychological trauma associated with hospitalization for
childbirth may retain vulnerability to trauma reminders during subsequent hospitalizations.40
The relationship between pregnancy-related mental illness and subsequent risk of suicidality
after medical illness warrants further exploration, particularly with regard to the role of
hormone replacement therapy, neuroinflammatory markers, and intergenerational role
transitions.
The results of our study should be interpreted in light of the following considerations. We
focused on medical hospitalizations and thus did not include individuals with low-lethality
suicide-related behavior. Although we used data collected from multiple institutions, care
outside of our health systems was not captured. To address this “open” system problem and model a known outcome, we subsetted our data to focus on women who were rehospitalized
within our health systems. As with all studies using EHR data, these data are imperfect.
Inclusion of suicide risk assessment (such as the Columbia Suicide Screening Rating
Scale43) would almost assuredly enhance the predictive accuracy of our model, however
encoding of these scales was poor in our datasets. Suicidal ideation and behavior are
notoriously undercoded, and thus the rate of suicide-related behaviors is likely
underestimated.17,41 Future studies should consider use of clinical text and natural language
processing to enhance cohort identification of patients presenting for suicide-related care.
Our analyses focused on natal sex, future work should explore post-medical hospitalization
suicide risk in cohorts with patient-identified gender."
"Natural Language Processing with Machine Learning to Predict
Outcomes after Ovarian Cancer Surgery","Objective: To determine if natural language processing (NLP) with machine learning of
unstructured full text documents (a preoperative CT scan) improves the ability to predict
postoperative complication and hospital readmission among women with ovarian cancer
undergoing surgery when compared with discrete data predictors alone.
Methods: Medical records from two institutions were queried to identify women with ovarian
cancer and available preoperative CT scan reports who underwent debulking surgery. Machine
learning methods using both discrete data predictors (age, comorbidities, preoperative laboratory
values) and natural language processing of full text reports (preoperative CT scans) were used to
predict postoperative complication and hospital readmission within 30 days of surgery.
Discrimination was measured using the area under the receiver operating characteristic curve
(AUC). Results: We identified 291 women who underwent debulking surgery for ovarian cancer. Mean
age was 59, mean preoperative CA125 value was 610 U/ml and albumin was 3.9 g/dL. There were
25 patients (8.6%) who were readmitted and 45 patients (15.5%) who developed postoperative
complications within 30 days. Using discrete features alone, we were able to predict postoperative
readmission with an AUC of 0.56 (0.54 – 0.58, 95% CI); this improved to 0.70 (0.68–0.73, 95%
CI) (p<0.001) with the addition of NLP of preoperative CT scans.
Conclusions: Natural language processing with machine learning improved the ability to predict
postoperative complication and hospital readmission among women with ovarian cancer
undergoing surgery.","In this study, we evaluted the use of machine learning with natural language processing to
attempt to improve upon current methods for prediction of postoperative complication and
readmission among women with ovarian cancer. We found that machine learning models,
such as random forest or XGBoost, led to very minimal improvements in the discrimination
of the models over traditionally-used logistic regression, and thus the ability to predict
postoperative events. In contrast, the addition of natural language processing of preoperative
CT scans, which allows for the conversion of full text data into discrete data that can then be
placed into a predictive model, resulted in an approximately 20–25% improvement in the
ability to predict both postoperative complications and postoperative readmissions.
Artificial intelligence, which refers to the use of computer systems to perform tasks that
normally require human intelligence, is finding increasing applications in medicine. Within
artificial intellingence, we used two techniques: machine learning and natural language
processing. Machine learning automates analytical model building and allows for the
computer or “machine” to find insights or relationships hidden within the data rather than to
rely on pre-specified hypotheses. Natural language processing allows for the computer to
take a free text document and transform the words or phrases within that document into
variables or vectors that can then be analyzed by an analytical model. These techniques been
used to improve accurate diagnosis in mammography and to screen for diabetic retinopathy
[21, 22]. They have also been used to enhance human decision making and improve
prediction of events. This includes applications such as predicting survival from cancer and
predicting complications among patients admitted to ICUs after cardiothoracic surgery [23].
Predicting complications and hospital readmissions after surgery for ovarian cancer is useful
for many reasons. Patient counseling can be tailored and expectations can be set. Resources
can be directed at those at highest risk through enhanced postoperative monitoring using
both traditional and novel telehealth methods. Additionally, patients at highest risk of
postoperative complications can be identified and their treatment plans adjusted accordingly.
Multiple randomized trials have shown that neoadjuvant chemotherapy decreases the rates of
postoperative complications when compared with primary debulking surgery [1, 3, 4]. Furthermore, postoperative complications can lead to delays in initiation of adjuvant
chemotherapy [2]. For patients at the highest risk, beginning with neoadjuvant chemotherapy
may be a reasonable strategy to try to mitigate the effects of postoperative complications on
adjuvant treatment and survival.
Additional findings of interest from this study include the words and phrases that the
machine learning algorithms found to be predictive of postoperative complication and
hospital readmission. Some are words which clinically we already think of as describing the
extent of disease present, such as “ascites”, “bulky lymphadenopathy” or “liver lesion” and
may not come as a surprise. However, others are words which we do not tend to associate
with high morbidity surgery or postoperative complication, such as “ill-defined”,
“stranding”, “lobulated”, and “exophytic”. It is possible that these words, which refer to the
appearance or composition of the tumor itself, indicate a more invasive tumor that requires
more morbid surgery, or some other characteristic associated with complication being
conveyed by these descriptive words. An additional group of words focused on the health of
the patient, such as “compression fracture,” which is indicative of the frailty of the patient
and their ability to withstand the surgical insult.
Strengths of this study include the use of novel methods which have not been applied in
gynecologic oncology or ovarian cancer surgery to date. We also chose predictors that could
be obtained without direct chart review to enhance the ability to scale this predictive model.
Limitations include that this study was performed with medical records from two
institutions; thus generalizability may be limited and the models may suffer from over-
fitting. Specifically with natural language processing of CT scan reports there may be
regional or local differences in words that are used which limits genealizability outside the
two studied institutions. Additionally, although this study found significant improvements
with the use of natural language processing of free text information, the models still have an
overall discrimination that was fair, indicating that there is more work to be done to improve
the use of these techniques before they can be used in this clinical setting. Examining
predictors found in other places within the electronic medical record could be considered,
such as within operative reports, pathology reports, or clinic notes. Additionally, using the
CT images themselves, rather than reports, as predictors could also improve the
discrimination of these models. However, despite these limitations and future directions, our
results indicate that artifical intelligence, specifically machine learning methods using
natural language processing, may improve the ability to predict postoperative complications
and readmission after ovarian cancer surgery. Incorporating these methods will be important
for future research in gynecologic oncology."
"Development and Validation of a Machine Learning Algorithm for
Predicting the Risk of Postpartum Depression among Pregnant
Women","Objective—There is a scarcity in tools to predict postpartum depression (PPD). We propose a
machine learning framework for PPD risk prediction using data extracted from electronic health
records (EHRs).
Methods—Two EHR datasets containing data on 15,197 women from 2015 to 2018 at a single
site, and 53,972 women from 2004 to 2017 at multiple sites were used as development and
validation sets, respectively, to construct the PPD risk prediction model. The primary outcome was
a diagnosis of PPD within 1 year following childbirth. A framework of data extraction, processing,
and machine learning was implemented to select a minimal list of features from the EHR datasets
to ensure model performance and to enable future point-of-care risk prediction.
Results—The best-performing model uses from clinical features related to mental health history,
medical comorbidity, obstetric complications, medication prescription orders, and patient
demographic characteristics. The model performances as measured by area under the receiver
operating characteristic curve (AUC) are 0.937 (95% CI 0.912 – 0.962) and 0.886 (95% CI 0.879–
0.893) in the development and validation datasets, respectively. The model performances were
consistent when tested using data ending at multiple time periods during pregnancy and at
childbirth.
Limitations—The prevalence of PPD in the study data represented a treatment prevalence and is
likely lower than the illness prevalence.Conclusions—EHRs and machine learning offer the ability to identify women at risk for PPD
early in their pregnancy. This may facilitate scalable and timely prevention and intervention,
reducing negative outcomes and the associated burden.","Results from this study suggest a promising direction to leverage routinely collected EHR
data to identify pregnant women at risk for PPD. Selected EHR-driven predictors
characterize women’s health history, pregnancy health, demographics, and healthcare
utilization. Several known PPD risk factors from the literature were represented by variables
extracted in the sequential feature selection process, including history of anxiety, mood disorder, and other mental disorders, antidepressant use, incidental mental health illnesses
during pregnancy, cesarean section, and single motherhood.(Forman et al., 2000; Stewart
and Vigod, 2016) Our model further identifies additional comorbid predictors, including
palpitations, diarrhea, vomiting during pregnancy, hypertensive disorders and
hypothyroidism. Among these comorbidities, thyroid dysfunction and hypertensive disorders
have been associated with PPD onset in previous literature.(Le Donne et al., 2017;
Strapasson et al., 2018) Palpitation, a common cardiac symptom, may also be a symptom of
depression that was discovered by the model.(Alijaniha et al., 2016; Barsky et al., 1994) In
addition, medication prescriptions of beta blocking agents and antihistamines were identified
as predictors. Literature has reported the use of both beta blockers and antihistamines in
association with depression although not conclusively (Yudofsky, 1992)(Gerstman et al.,
1996; Ozdemir et al., 2014). Related to mode of delivery, our model selected cesarean
section as a risk factor for PPD, as also studied in the previous literature.(Carter et al., 2006;
Xu et al., 2017) Lastly, the number of ED visits during pregnancy and postpartum may be an
indicator of a lack of proper access to primary and obstetric care.(Sheen et al., 2019)
As seen in our experiments, the risk computed by the PPD prediction algorithm updates in
response to the new health information that accumulates overtime with repeated visits during
pregnancy, thereby potentially allowing care providers to take timely actions according to
the risk evolution. (Committee, 2018; Earls et al., 2019) With these automatically extractable
features, an EHR-based prediction tool may assist with existing EHR interventions for
screening to minimize variations across clinical practices in screening and information
collection.(Long et al., 2019)Previous studies have reported that while the rates of screening
and referrals for mental health care can be high when obstetricians recognize a risk for PPD,
but they are low if symptoms are unnoticed by the care provider.(Goodman and Tyer-Viola,
2010) Our risk prediction model, by identifying women with elevated risk, may assist with
tacitly raising clinician awareness of PPD and potentially increasing screening and referral
rates. Several limitations exist in our research. First, our study cohort as derived from the EHR in
an urban academic medical center is not representative of the general US population
suffering from PPD and differs from cohorts reported in previous studies with respect to
PPD prevalence (Hahn-Holbrook et al., 2017). This prevalence is likely the treatment
prevalence rather than the illness prevalence, as the data may not capture patients outside of
the studied health system and geographical location. The prevalence may also reflect the
clinician coding practices on recording a diagnosis of PPD at the study sites. Persistent
stigma and social consequences of having depression coded in the EHR may prevent
providers from ‘officially’ coding the diagnosis even if it is made clinically. Further, also due
to stigma, patients may withhold symptom information from providers preventing accurate
diagnosis. In addition to using diagnostic codes, we also defined PPD using antidepressant
use while excluding those for pain indications. However, it is possible that some
antidepressants were used for anxiety rather than PPD. Anxiety disorders are so frequently
comorbid with depression in the peripartum period such that a diagnosis of one may even be
a proxy for unidentified depression. Thus, we decided it was important not to exclude anxiety disorder indications even at the expense of specificity, although we recognize this as
a limitation of our study. Our ongoing and future work will attempt to parse these indications
further by applying natural language processing to the unstructured clinical notes.
Relatedly, in this study, we did not specifically include only patients with incident
depression. This decision was meant to acknowledge the powerful effect that mental health
histories have on risk for developing PPD as well as to provide a clinically meaningful risk
stratification for real-world obstetric providers who have large cohorts of patients with
mental health histories and those who are actively seeking treatment in their practices. Due
to the lack of comprehensive screening at our health systems and clinics in the study sites,
we did not capture EPDS and PHQ-9 scores to define PPD. We also did not compare
effectiveness of primary prevention via the prediction algorithm to current widely
recommended secondary prevention efforts via EPDS or PHQ-9 screening. However, we did
compare with algorithms reported in prior literature as a potential primary intervention
approach, and demonstrated improved model performance. Compared to prior work by
Camdeviren et al (Camdeviren et al., 2007), Tortajada et al (Tortajada et al., 2009), and
Natarajan et al (S. et al., 2017), our algorithm was built by exhaustively selecting most
predictive features from a larger number of candidate features from the EHR data, with an
eventual goal of integrating such risk prediction models within the EHR systems and clinical
workflows. Furthermore, compared to our initial pilot work (Wang et al., 2019) which did
not include prior mental health diagnosis and treatment history as predictors, the prediction
algorithm from this study demonstrated a significant increase in AUC, sensitivity, and
specificity.
A number of future works are under preparation to address these limitations. We found
White and Asian races to be predictive features in this study. However, a substantial
proportion of race was unknown in both the training and validation datasets, potentially due
to lack of proper documentation in the EHR (Lee et al., 2016). This is an important area for
further consideration in future studies.(Sholle et al., 2019) These include a comparison of
the data-driven primary intervention against usual care as a clinical trial, and additional
validation work at study sites in the greater US and abroad using datasets with different PPD
prevalence to evaluate the algorithm generalizability. While findings from this study present
a promise for PPD risk identification using available EHR data, we realize that EHR data
capture only a limited portion of patients’ live s which contribute to PPD. Therefore, we will
also evaluate whether the addition of patient-reported outcomes or information derived from
mobile health devices, such as wearables, can contribute to higher algorithm performance.
Lastly, improvement in the machine learning framework will include techniques to adjust for
differing outcome distributions such that the method can be more generally applied to other
populations."
"
Comparing machine learning algorithms for multimorbidity prediction: An example from the Elsa-Brasil study","Background
Multimorbidity is a worldwide concern related to greater disability, worse quality of life, and
mortality. The early prediction is crucial for preventive strategies design and integrative
medical practice. However, knowledge about how to predict multimorbidity is limited, possibly due to the complexity involved in predicting multiple chronic diseases.
Methods
In this study, we present the use of a machine learning approach to build cost-effective multi-
morbidity prediction models. Based on predictors easily obtainable in clinical practice (socio-
demographic, clinical, family disease history and lifestyle), we build and compared the
performance of seven multilabel classifiers (multivariate random forest, and classifier chain,
binary relevance and binary dependence, with random forest and support vector machine
as base classifiers), using a sample of 15105 participants from the Brazilian Longitudinal
Study of Adult Health (ELSA-Brasil). We developed a web application for the building and
use of prediction models.
Results
Classifier chain with random forest as base classifier performed better (accuracy = 0.34,
subset accuracy = 0.15, and Hamming Loss = 0.16). For different feature sets, random forest based classifiers outperformed those based on support vector machine. BMI, blood pressure, sex, and age were the features most relevant to multimorbidity prediction. Conclusions
Our results support the choice of random forest based classifiers for multimorbidity prediction.","Recent studies on multimorbidity have implemented ML techniques to identify patterns of
association between chronic conditions [11–13]. However, knowledge about modeling and
prediction is still limited. Few studies deal with predictions for multiple diseases, mostly used
counts [9, 10, 39]. When predicting with counting, the full complexity of relationships between
diseases and between diseases and predictors is neglected in exchange for a less complex
modeling process. The difficulties in predicting multimorbidity and how to overcome these
issues when building a classifier have not yet been reported. The present study is the first to
present the main problems in predicting multiple diseases and how to solve them, besides pro-
viding a user-friendly web application for building and using prediction models.
We present a machine learning approach based on multilabel classifiers, and use it to build
and compare the performance of seven classifiers for predicting multimorbidity. This method-
ology and developed web application can be used to build feasible models, for screening indi-
viduals with multimorbidity from a small set of characteristics easily accessible in clinical
practice. In the context of LMIC´s, with scarce resources, where it is known that multimorbid-
ity represents a high burden of disease, these models can help as a tool for prevention and targeting of interventions based on the modifiable factors [7, 8]. Moreover, since the models can
predict the disease’s cooccurrence, they can support the design of integrated clinical practices
of care instead of conventional fragmented strategies for each chronic disease, one of the main
challenges for multimorbidity prevention and management [8].
The prevalence of multimorbidity observed in this study (56.3%) was higher than expected
in Brazil (reported in previous studies between 16.8% and 24.2%) [10, 40–43]. Differences
between prevalences can occur due to several reasons, such as the different sets of diseases and
differences in variables such as age. The majority of participants with multimorbidity were
women and had higher average age. Among participants with at least two chronic conditions,
we observed a gradient of increase in the proportion of people with primary or secondary edu-
cation, and decrease in average per capita income. Most studies found that age is a risk factor
for multimorbidity as well as sex, however, there is still no consensus on other factors such as
education and income [8].
Dyslipidemia, migraine and common mental disorder were the most prevalent diseases and
were also among the most cooccurring. High cooccurrences were also found between diabetes
and dyslipidemia, heart disease and dyslipidemia, migraine and common mental disorder,
asthma and migraine, and joint problems and migraine. These results are in agreement with
previous studies that reported a high prevalence of dyslipidemia, as well as associations
between mental disorders, joint problems, migraine, and respiratory diseases [8, 41–45]. The
patterns of association found are compatible with the patterns previously identified in Brazil
and LMIC’s named as ""cardio-metabolic"" and ""musculoskeletal-mental"" [41, 44, 45].
Considering the full set of features, the best performance was achieved for RF-CC. SVM-
based transformation methods performed worse than both adaptive and transformation RF-
based methods. Decision trees have already been indicated as a good method for multiple-dis-
ease classification over SVM-based methods, and our results support the choice of RF-based
classifiers for multimorbidity prediction [14]. The analysis for the different feature sets con-
firmed that RF-based classifiers should be preferred over SVM-based classifiers from 15 predictors, selected by BR+IG. Despite the assumption of independence between diseases,
RF-BR was consistently among the best predictors, with the advantage of easy implementation
and scalability, However, RF-CC should be considered if subset accuracy is the most important
metric, i.e. when we want to achieve the best performance regarding the accurate identification
of all diseases a patient may have. The performance of the CC can decrease significantly for a
dataset with a high number of labels and high dependency or cardinality, so more studies are
needed for evaluation of this classifier for multimorbidity prediction [46, 47].
BMI combined with age, sex, waist-hip ratio, systolic and diastolic BP, sleep symptom and
sleeping problem composed the set of eight best predictors, for MTV-RF, according variable
importance. They also had the highest information gains, which was expected given that they
are measures based on decision tree algorithms. Such features, found to carry the highest
importance, are related to chronic diseases and multimorbidity. Sex and age are commonly
reported as risk factors for multimorbidity, BMI, BP, and waist-hip ratio are also pointed out
as a risk factor, especially for cardio-metabolic multimorbidities, and sleep disturbances and
multimorbidity have been related previously, in particular associated with neuropsychiatric
and musculoskeletal conditions [8–10, 48–50].
Our study has some limitations. Because this is a cross-sectional study, it is not possible to
identify a well-established cause-effect relationship between the variables. In general, ML
methods use features to predict a response, but they may not be causal factors, so the directionality between predictor and response is difficult to establish. It should be noted that the importance of the variables found is limited to the model and population studied, using other
algorithms may change the relevance of the variables. Therefore, although ML provides some
insights into risk factors, it is not a conclusive analysis for this purpose.
Despite these limitations, our study is the first to present a ML approach to solve the main
problems in multimorbidity prediction in a scenario where the number of chronic diseases is
the most used outcome in predictions. The ML methodology used and web application devel-
oped are comprehensive, and can be applied to any clinical field in which multiple outcomes
are considered. Starting from a large sample size, which makes the ML process more robust,
we show that it is possible, from a small set of features, that are easy to collect in clinical practice, to build a feasible tool for multimorbidity prediction."
"Identification of Multimorbidity Patterns in Rheumatoid Arthritis
through Machine Learning","Objective: Recognizing that the interrelationships between chronic conditions that complicate
rheumatoid arthritis (RA) are poorly understood, we aimed to identify patterns of multimorbidity
and to define their prevalence in RA through machine learning.
Methods: We constructed RA and age- and gender-matched (1:1) non-RA cohorts within a large
commercial insurance database (MarketScan®) and the Veterans Health Administration (VHA).
Chronic conditions (n=44) were identified from diagnosis codes from outpatient and inpatient
encounters. Exploratory factor analysis was performed separately in both databases, stratified by
RA diagnosis and sex, to identify multimorbidity patterns. The association of RA with different
multimorbidity patterns was determined using conditional logistic regression.
Results: We studied 226,850 patients in MarketScan® (76% female) and 120,780 patients in
the VHA (89% male). The primary multimorbidity patterns identified were characterized by the
presence of cardiopulmonary, cardiometabolic, and mental health and chronic pain disorders.
Multimorbidity patterns were similar between RA and non-RA patients, females and males,
MarketScan® and the VHA. RA patients had higher odds of each multimorbidity pattern (odds ratios [ORs] 1.17 to 2.96), with mental health and chronic pain disorders being the multimorbidity
pattern most strongly associated with RA (ORs 2.07 to 2.96).
Conclusions: Cardiopulmonary, cardiometabolic, and mental health and chronic pain disorders
represent predominant multimorbidity patterns, each of which are over-represented in RA. The
identification of multimorbidity patterns occurring more frequently in RA is an important first step
in progressing toward a holistic approach to RA management and warrants assessment of their
clinical and predictive utility.","We selected over 347,000 patients from two national, real-world healthcare databases to
identity and contrast the patterns and prevalence of multimorbidity between patients with
RA and those without. In both datasets and in both sexes, we consistently identified
between 3 to 5 prevailing patterns of multimorbidity. While the derived multimorbidity
patterns were similar between RA and non-RA patients, the prevalence of these patterns was
significantly higher in those with RA. These findings suggest the interrelationships between
chronic conditions are consistent among RA and non-RA patients, but patients with RA
are disproportionally afflicted with multimorbidity. These patterns may help advance and
harmonize the assessment of multimorbidity in clinical and research settings.
The primary patterns of multimorbidity we identified through our machine learning
approach were cardiopulmonary, cardiometabolic, and mental health and chronic pain
disorders. The validity of these patterns was demonstrated by the reproducibility of
identifying these patterns in men and women, RA and non-RA, and across independent
datasets with heterogenous patient characteristics. These patterns are also consistent with
those that have been identified in the general population in other multimorbidity studies
(13). Prior comorbidity research in RA has recognized individually the links between
RA and cardiovascular disease (21), diabetes (22), lung diseases (23), mental health
conditions (24, 25), and chronic pain (26). However, this study is among the first to move
beyond recognition of individual comorbidities in RA and comprehensively characterize
multimorbidity and multimorbidity patterns. Moreover, our findings reiterate the crucial
importance of mental health and chronic pain multimorbidity in RA. This multimorbidity
pattern explained the highest proportion of variance in factor analysis and was also the
pattern most closely associated with RA status, irrespective of data source or sex.
Our findings build on prior efforts to phenotype RA patients based on their chronic
coexisting health conditions. Curtis and colleagues selected a somatization comorbidity
phenotype in RA characterized by the use of medication with indications of depression,
anxiety, or neuropathic pain or a diagnosis of depression, chronic pain, fibromyalgia, or
myalgias (27). This phenotype predicted poorer treatment response and a higher likelihood
of adverse events in a randomized controlled trial of certolizumab pegol (27). Curtis and
colleagues also used machine learning approaches to phenotype patients with RA in the
Brigham and Women’s Rheumatoid Arthritis Sequential Study (28). Principal component
analysis was performed on a large number of variables from patient questionnaires,
physician reports, laboratory tests, and radiographs. Subsequently, patients were clustered
based on these principal components. Despite a large number of RA specific variables,
multimorbidity burden was a key construct for differentiating clusters of RA patients (28).
Our study is distinct by comprehensively characterizing the interrelatedness of chronic
conditions that define unique patterns of multimorbidity in RA and demonstrating their
overrepresentation among patients with RA. Beyond individual multimorbidity patterns, we also found RA patients to possess a higher number of unique multimorbidity patterns.
This multimorbidity pattern count approach represents a novel method for evaluating
multimorbidity, rather than comorbidity, in RA and warrants further study. With the identification of multimorbidity patterns in RA, there exists the potential to
advance our assessment of multimorbidity in both clinical and research settings. In contrast
to widely used multimorbidity and comorbidity indices that provide a score regarding
the burden of chronic conditions, these multimorbidity patterns uniquely capture the
interconnectedness of chronic conditions. The interconnectedness of chronic conditions is
demonstrated in our study by the loadings of several distinct chronic conditions to each
multimorbidity pattern and may reflect shared disease mechanisms (e.g., atherogenesis
underlying cardiovascular disease and peripheral vascular disease), shared disease risk
factors (e.g., cigarette smoking for chronic obstructive pulmonary disease and cardiovascular
disease), and disease complications (e.g., cardiovascular disease resulting from hypertension
or diabetes mellitus). Building on these interrelationships, these patterns may be useful for
risk stratification to guide disease screening or more holistic care (e.g., case management,
mental health referrals) or to enable accurate risk adjustment for quality-of-care metrics.
These patterns could also be useful for predicting clinical outcomes, serving a similar or
complementary role to existing multimorbidity and comorbidity indices. Uniquely, these
patterns may also support clinical and translational research that aims to identify the
mechanisms of interrelated conditions and could support precision medicine initiatives to
identify patients with differential likelihood of treatment response or adverse events.
There are limitations to this study. As in the general population (13), there is no consensus
regarding the optimal set of chronic conditions to include in RA multimorbidity studies.
To identify chronic conditions, we relied on diagnostic codes which may result in
misclassification. This is anticipated to be non-differential as the same algorithms were
used for both RA and non-RA patients. Both data sources utilized diagnoses from specialists
(e.g. rheumatologists) and primary care physicians but did not include medications. Our
approach may be useful, although the specifics are likely to differ, in alternative data sources
that rely on single specialty data or medications. Our study was cross sectional in nature,
and it is possible that patterns of multimorbidity change over the RA disease course. To
ensure an adequate sample for factor analysis, we combined some highly related chronic
conditions and excluded some rare chronic conditions in stratified analyses. While this
results in some differences in the chronic conditions included between analyses, there was
a striking consistency of multimorbidity patterns across analyses and datasets that supports
the fidelity of our findings. We assigned descriptive names to the multimorbidity patterns
for illustrative purposes but recognize these names cannot fully reflect the contribution of
each individual condition to the derived patterns. In the general population, several machine
learning approaches have been used to derive multimorbidity patterns including factor
analysis, cluster analysis, and latent class analysis (13). It is possible that the multimorbidity
patterns identified may vary with alternative machine learning approaches or with the use of
different criteria for selecting patterns to retain. Finally, the clinical utility of the patterns and
their predictive value were not tested in this study. This important next step to demonstrate
the utility of our approach on endpoints such as medication initiation and adherence, clinical
outcomes, and healthcare costs will need to be established in follow-up studies.
In conclusion, we used machine learning approaches in two, large, independent datasets
to identify the patterns of multimorbidity afflicting real-world patients with RA. We found
the patterns of multimorbidity in RA to be similar to those in the general population, but more prevalent. The primary multimorbidity patterns in RA are cardiopulmonary,
cardiometabolic, and mental health and chronic pain disorders. The identification of these
patterns may be helpful to clinicians caring for patients with RA and may help to harmonize
the assessment of multimorbidity between research and clinical settings. Future research is
needed to test the clinical value of these multimorbidity patterns and how multimorbidity
patterns may change during the disease course."
"Network analytics and machine learning
for predicting length of stay in elderly patients
with chronic diseases at point of admission","Background: An aging population with a burden of chronic diseases puts increasing pressure on health care
systems. Early prediction of the hospital length of stay (LOS) can be useful in optimizing the allocation of medical
resources, and improving healthcare quality. However, the data available at the point of admission (PoA) are limited,
making it difficult to forecast the LOS accurately.
Methods: In this study, we proposed a novel approach combining network analytics and machine learning to
predict the LOS in elderly patients with chronic diseases at the PoA. Two networks, including multimorbidity network
(MN) and patient similarity network (PSN), were constructed and novel network features were created. Five machine
learning models (eXtreme Gradient Boosting, Gradient Boosting Decision Tree, Random Forest, Linear Support Vector
Machine, and Deep Neural Network) with different input feature sets were developed to compare their performance.
Results: The experimental results indicated that the network features can bring significant improvements to the
performances of the prediction models, suggesting that the MN and PSN are useful for LOS predictions.
Conclusion: Our predictive framework which integrates network science with data mining can forecast the LOS
effectively at the PoA and provide decision support for hospital managers, which highlights the potential value of
network-based machine learning in healthcare field.","This study has proposed a novel approach to extracting
creative and representative network features for early
LOS prediction due to the limited data available at the
PoA. To the best of our knowledge, this is the first time
that features have been extracted from MN and PSN on
such a large dataset for LOS prediction. Comparison with the other studies of the present literature
Several similar studies that have applied machine learning methods to predict LOS at the PoA are listed in
Table 5 for comparison purposes. Due to differences in
the data sources, we compare our work with existing
studies mainly from the aspects of feature composition
and model performance. As shown in Table 5, most exist-
ing studies were confined to a single disease and the sizes
of their datasets were much smaller than ours, which
affects the utility, generalization, and reliability of the
model. Moreover, few studies have considered historical
features. They tend to calculate only the mean and counts
of the LOS of historical records which means that the
potential has not been fully realized. We found that other
descriptive statistics of historical LOS also impact significantly the LOS predictions as listed in Table 4, such as the median and the maximum. None of the existing studies used the MN and PSN features, which are essential factors in predicting LOS. The historical features, MN
features, and PSN features are relatively independent and
can enhance model performance from different perspectives, as shown in Table 5. The means of LOS differ from
each other, and the truncation strategies for the LOS
also vary. Some studies considered the qualified LOS of
less than 30 days [17] or truncated at the 98% percentile
[46], whereas other studies did not take any action, which
accounts for prediction results that are not comparable and vary widely. However, a review study concluded
that a model has a strong predictive ability for the LOS if
R2 > 0.36 [47], which implies that our proposed approach
has a superior predictive ability with R2 = 0.375. The present study has some limitations. First, we adopted
zero to fill missing values, which might influence the
predictive ability even though the tree-based models are
not sensitive about a fill strategy. An appropriate miss-
ing value filling strategy, such as k-nearest neighbor [51],
might achieve better LOS predictions. Second, although
we made full use of the historical LOS information, other
historical data was not taken into account, such as histor-
ical medication use and historical comorbidities. In addi-
tion, we used a fixed time window of three years, whereas
multi-scale time windows such as going back six months,
one year, and three years are likely to be helpful in
improving model performance [17]. Third, we extracted
the EVC features and disease risk features from the MN
to improve prediction accuracy. The potential of the
MN can be further excavated, such as network cluster-
ing information [28]. Moreover, since the validity of the PSN has been proven by the PSN features, future work
could develop a Graph Neural Network (GNN) to use the
structural information of PSN, such as Graph SAmple
and aggreGatE (GraphSAGE) [52] and Graph Attention
Network (GAT) [53], which can construct an end-to-
end model by using both network topology information
and a node’s feature vectors. In addition, the skewness of
LOS results in poor prediction ability of the model when
LOS higher than 30 days, as shown in Fig. 7. Some resa-
mpling techniques can enhance the number of the tail
LOS data, which may bring extra improvement for our
proposed methods [54]. Despite these limitations, our
proposed approach has sufficient robustness to predict
with a certain level of accuracy the hospital LOS for the
elderly with chronic diseases at the PoA. Future work will
explore the directions indicated above to further improve
accuracy."
"
Examining the predictability and 
prognostication of multimorbidity among older Delayed-Discharge 
Patients: A Machine learning analytics


    
  ","Background: Patient complexity among older delayed-discharge patients complicates discharge planning,
resulting in a higher rate of adverse outcomes, such as readmission and mortality. Early prediction of multi-
morbidity, as a common indicator of patient complexity, can support proactive discharge planning by prioritizing
complex patients and reducing healthcare inefficiencies.
Objective: We set out to accomplish the following two objectives: 1) to examine the predictability of three
common multimorbidity indices, including Charlson–Deyo Comorbidity Index (CDCI), the Elixhauser Comor-
bidity Index (ECI), and the Functional Comorbidity Index (FCI) using machine learning (ML), and 2) to assess the
prognostic power of these indices in predicting 30-day readmission and mortality.
Materials and Methods: We used data including 163,983 observations of patients aged 65 and older who expe-
rienced discharge delay in Ontario, Canada, during 2004 – 2017. First, we utilized various classification ML
algorithms, including classification and regression trees, random forests, bagging trees, extreme gradient
boosting, and logistic regression, to predict the multimorbidity status based on CDCI, ECI, and FCI. Second, we
used adjusted multinomial logistic regression to assess the association between multimorbidity indices and the
patient-important outcomes, including 30-day mortality and readmission.
Results: For all ML algorithms and regardless of the predictive performance criteria, better predictions were
established for the CDCI compared with the ECI and FCI. Remarkably, the most predictable multimorbidity index
(i.e., CDCI with Area Under the Receiver Operating Characteristic Curve = 0.80, 95% CI = 0.79 – 0.81) also
offered the highest prognostications regarding adverse events (RRRmortality = 3.44, 95% CI = 3.21 – 3.68 and
RRRreadmission = 1.36, 95% CI = 1.31 – 1.40).
Conclusions: Our findings highlight the feasibility and utility of predicting multimorbidity status using ML al-
gorithms, resulting in the early detection of patients at risk of mortality and readmission. This can support
proactive triage and decision-making about staffing and resource allocation, with the goal of optimizing patient
outcomes and facilitating an upstream and informed discharge process through prioritizing complex patients for
discharge and providing patient-centered care.","Using large longitudinal data on older delayed-discharge patients,
this two-stage study evidenced the ML-predictability of patient
complexity based on multimorbidity indices and their prognostication
significance in predicting patient-important outcomes, such as mortality
and readmission. We provided first-hand evidence on this topic through
robust comparisons of several predictive accuracy measures using
multiple ML algorithms with three common multimorbidity indices. Our
results indicate that regardless of the type of the ML algorithm or the
measure for predictive performance assessment, the CDCI is the most
predictable index, and FCI is the least predictable. Interestingly, these
two indices had the least agreement regarding their predictions. More
remarkably, the prognostication analytics in our study revealed that the
most predictable index (i.e., CDCI) also has the greatest strength in
predicting adverse events.
The differences between multimorbidity indices (in terms of both
predictability and prognostication) as well as their degree of prediction
agreements may stem from the type of comorbidity items included in the
index and the way the items are combined. Although the CDCI consists
of fewer comorbidities than the ECI and FCI do, the combination of the
comorbidity items in the CDCI is weighted based on their severity to
account for the disease burden, which contrasts with the simple summation of the comorbidity items in the ECI and FCI. Several studies have
reported the superior prognostic performance of weighted measures of
multimorbidity compared with simple measures [39,60-62]. We
contribute to this literature by highlighting the importance of accounting for the severity of the diseases for better prognostications with
multimorbidity indices, but we are the first to highlight such importance for improving the predictability of multimorbidity indices.
The FCI does not include severe chronic conditions, such as metastatic cancer, dementia, or HIV, which are highly associated with
adverse events, particularly mortality [21]. Instead, by including physical functioning-related conditions (e.g., arthritis, hearing and visual
impairment, osteoporosis, and degenerative disk disease) –– which are
better predictors of hospital readmission, particularly among older
adults [63,64] –– the FCI is essentially a better predictor for hospital
readmission than it is for death [22,37]. Consistent with our research,
other studies have reported that the CDCI outperforms the ECI in predicting adverse events in the cohort of patients with diabetes [65] and
lung cancer [66]. Our results also agree with the literature supporting
the superior prognostic significance of the CDCI compared with the FCI
[45]. Finally, our findings agree with previous studies providing evidence that the CDCI and ECI are associated with an increased risk of
mortality and readmission [35-42]. Because the multimorbidity burden is associated with an increased
risk of adverse outcomes, such as readmission and mortality, as well as
exponential increases in healthcare costs, healthcare providers should
aim to rethink the management of the delayed discharge patients’ care
needs and implement holistic prevention policies to reduce their risk of
adverse outcomes. Recent studies have concluded that patients experi-
ence catastrophic health service use and health expenditures when the
number of their multimorbid conditions increases [67,68].
Advanced knowledge of multimorbidity status and in-hospital
development can be used to support clinical decision-making in the
care of older adults awaiting community placement. More specifically,
data from prior hospital encounters could be used in conjunction with
current medical conditions to determine an overall multimorbidity sta-
tus of the patient. This, in turn, could be used to flag high-risk patients
who may require more intensive monitoring despite readiness for
discharge. Given they are ready for discharge, older adults with delayed
discharge are often overlooked by hospital staff. Instead, they should be
monitored frequently because high rates of multimorbidity and geriatric
complexity result in transient health conditions and an increased risk of
poor patient outcomes, underscoring the need for monitoring these patients in the hospital. Where possible, consideration should be given to
implementing ML algorithms within electronic medical systems to pro-
actively predict multimorbidity status. Our findings are also relevant to
policymakers, considering that both mortality and readmission are
common metrics used by hospitals to gauge the quality of care provided.
5.3. Strengths and limitations
This study presents robust analytics to examine the predictability
and prognostication of three common multimorbidity indices using
several ML algorithms and various predictive accuracy measures based
on large longitudinal data. However, our study is not without limitations. First, this was a retrospective cohort study with limited control
over data collection. Second, we did not include frailty indices, which
could complement the examined multimorbidity indices in capturing
patient complexity among older adults [12]. Finally, this study focused
on the older delayed-discharge patients only; hence the generalizability
of our study to other populations is limited."
Machine Learning–Based Models Incorporating Social Determinants of Health vs Traditional Models for Predicting In-Hospital Mortality in Patients With Heart Failure,"IMPORTANCE Traditional models for predicting in-hospital mortality for patients with heart
failure (HF) have used logistic regression and do not account for social determinants
of health (SDOH).
OBJECTIVE To develop and validate novel machine learning (ML) models for HF mortality
that incorporate SDOH.
DESIGN, SETTING, AND PARTICIPANTS This retrospective study used the data from the
Get With The Guidelines–Heart Failure (GWTG-HF) registry to identify HF hospitalizations
between January 1, 2010, and December 31, 2020. The study included patients with acute
decompensated HF who were hospitalized at the GWTG-HF participating centers during the
study period. Data analysis was performed January 6, 2021, to April 26, 2022. External
validation was performed in the hospitalization cohort from the Atherosclerosis Risk in
Communities (ARIC) study between 2005 and 2014.
MAIN OUTCOMES AND MEASURES Random forest-based ML approaches were used to develop
race-specific and race-agnostic models for predicting in-hospital mortality. Performance was
assessed using C index (discrimination), regression slopes for observed vs predicted mortality
rates (calibration), and decision curves for prognostic utility.
RESULTS The training data set included 123 634 hospitalized patients with HF who were
enrolled in the GWTG-HF registry (mean [SD] age, 71 [13] years; 58 356 [47.2%] female
individuals; 65 278 [52.8%] male individuals. Patients were analyzed in 2 categories: Black
(23 453 [19.0%]) and non-Black (2121 [2.1%] Asian; 91 154 [91.0%] White, and 6906 [6.9%]
other race and ethnicity). The ML models demonstrated excellent performance in the internal
testing subset (n = 82 420) (C statistic, 0.81 for Black patients and 0.82 for non-Black
patients) and in the real-world–like cohort with less than 50% missingness on covariates
(n = 553 506; C statistic, 0.74 for Black patients and 0.75 for non-Black patients). In the
external validation cohort (ARIC registry; n = 1205 Black patients and 2264 non-Black
patients), ML models demonstrated high discrimination and adequate calibration (C statistic,
0.79 and 0.80, respectively). Furthermore, the performance of the ML models was superior
to the traditional GWTG-HF risk score model (C index, 0.69 for both race groups) and other
rederived logistic regression models using race as a covariate. The performance of the ML
models was identical using the race-specific and race-agnostic approaches in the GWTG-HF
and external validation cohorts. In the GWTG-HF cohort, the addition of zip code–level SDOH
parameters to the ML model with clinical covariates only was associated with better
discrimination, prognostic utility (assessed using decision curves), and model reclassification
metrics in Black patients (net reclassification improvement, 0.22 [95% CI, 0.14-0.30];
P < .001) but not in non-Black patients.
CONCLUSIONS AND RELEVANCE ML models for HF mortality demonstrated superior
performance to the traditional and rederived logistic regressions models using race as a
covariate. The addition of SDOH parameters improved the prognostic utility of prediction
models in Black patients but not non-Black patients in the GWTG-HF registry.","In this cohort study, we developed and validated ML-based
race-specific and race-agnostic risk models to predict in-
hospital mortality among individuals with hospitalization
for HF. We observed that the race-specific and race-agnostic
ML-based models demonstrated excellent performance in the
testing data sets, including those with substantial missing-
ness in model covariates. Furthermore, the ML-based mod-
els had superior discrimination, calibration, and clinical util-
ity in the external validation cohort than the original GWTG-HF
risk scores and other rederived logistic regression models
using race as a covariate. The addition of zip code–level SDOH
to the ML model was associated with an improvement in risk
reclassification and prognostic utility of the model in Black pa-
tients. We also observed significant race-specific differences
in the population-attributable risk of in-hospital mortality
associated with the SDOH with a significantly greater contri-
bution of these parameters to the overall in-hospital mortal-
ity risk in Black patients vs non-Black patients. Overall, the
present study demonstrates the potential utility of ML mod-
els for better and more equitable prediction of in-hospital mor-
tality risk among Black patients and non-Black patients hos-
pitalized for HF.
Novel Approach to Risk Prediction:
Role of Machine Learning and Race-Specific Approach
The most significant advancement with our risk models is the
use of ML-based approach to risk prediction. Several models
exist for predicting the risk of adverse outcomes among pa-
tients with HF hospitalization. Established models, such as the
GWTG-HF, OPTIMIZE-HF, ADHERE, and AHFI (Acute Heart
Failure Index) risk scores, use traditional statistical modeling
techniques, provide acceptable risk stratification, and have
been well validated in external cohorts. 4,5,30 A summary of
prior risk-prediction models is provided in eTable 9 in the the present study, we developed race-specific models for
in-hospital mortality. We observed superior performance of
the race-specific logistic regression model compared with
models using race as a covariate, highlighting the potential
utility of race-specific risk-prediction models. However,
when using an ML-based approach, the discrimination and
calibration metrics for the race-specific ML model were com-
parable with the ML model using race as a covariate. This is
related to the modeling approach used by random forest
learning methods, which assigned race a lower minimal
depth (higher importance). Thus, the random forest model
takes a race-specific approach, even when race is included as
a covariate, creating a decision tree after the first few nodes,
comparable with a race-specific model. Furthermore, even
with the use of a race-agnostic approach (without race as a
covariate), the performance of the ML model was compa-
rable with the race-specific model. Taken together, the
ML-based approach represents the most novel aspect of
our risk model that is associated with improved and more
equitable prediction of individual-level risk across race
groups. Thus, even though the risk of mortality among
Black patients was lower than non-Black patients in our
study cohorts, the proportion of Black patients identified to
be above specific risk thresholds was higher with the ML
models than with the traditional GWTG-HF model. Future
studies are needed to determine if similar ML models may
facilitate more equitable risk-based allocation of care.
This is particularly relevant considering the concerns raised
about the potential unintended effects of assigning lower
risk to Black patients using the existing risk models that use
race as a covariate, which may add to existing disparities in
HF care.7
Incorporation of SDOH to Improve Risk Prediction
SDOH are stronger predictors of in-hospital mortality in Black
patients vs non-Black patients with HF. 32-34 Previous studies
have observed improved model performance incorporating
SDOH data in the risk prediction equations. 35-37 In the pre-
sent study, we observed that zip code–level SDOH contrib-
uted more than 11% of the total in-hospital mortality risk in
Black patients compared with 0.5% in non-Black patients.
Consistent with the greater relative importance of SDOH in pre-
dicting the risk of in-hospital mortality in Black patients, we
observed a significant improvement in risk reclassification
and calibration with the addition of these parameters in Black
patients but not non-Black patients. Furthermore, the model
performance for the clinical and SDOH model was compa-
rable among patients hospitalized at disproportionate share vs
non–disproportionate share hospitals, highlighting the gen-
eralizability of the risk models among patients hospitalized in
low- and high-resource hospitals.
While the improvement in risk prediction with the incor-
poration of zip code–level SDOH parameters is encouraging,
future studies are needed to understand better how SDOH
factors can be better incorporated for risk prediction in HF
patients. First, it would be more informative to include
individual-level data on SDOH rather than zip code–level
data alone. Second, the ML-based approach used in the pre-
sent study allowed us to evaluate the clinical and SDOH
model in a data set with up to 50% missingness in clinical
parameters. It is plausible that with better capture of clinical
parameters in the model, the relative improvement in model
performance with the incorporation of zip code–level SDOH
may be attenuated.
Limitations
Our study has some notable limitations. First, we only in-
cluded variables that were regularly captured in the GWTG
registry. Data on certain laboratory measures, such as hemo-
globin A1c and lipid profiles, which are associated with in-
creased mortality risk,38 had significant missingness and were
excluded from the candidate covariates. Second, the race-
specific models were developed using self-reported race. In-
dividuals who may not identify with a specific race on the
GWTG-HF data form may not be accurately represented. How-
ever, sensitivity analysis across available races and ethnici-
ties showed similar performance. Third, only zip code–level
SDOH data were available for the present analysis. Incorpo-
rating participant-level SDOH parameters in risk models may
further improve their predictive performance. Fourth, race was
self-reported data on genetic ancestry were not available.
Fifth, we could not externally validate the clinical and SDOH
models given the lack of zip code data in the ARIC cohort."
"Development and internal validation of a clinical prediction model
using machine learning algorithms for 90 day and 2 year mortality
in femoral neck fracture patients aged 65 years or above","Purpose Preoperative prediction of mortality in femoral neck fracture patients aged 65 years or above may be valuable in the
treatment decision-making. A preoperative clinical prediction model can aid surgeons and patients in the shared decision-
making process, and optimize care for elderly femoral neck fracture patients. This study aimed to develop and internally
validate a clinical prediction model using machine learning (ML) algorithms for 90 day and 2 year mortality in femoral neck
fracture patients aged 65 years or above.
Methods A retrospective cohort study at two trauma level I centers and three (non-level I) community hospitals was con-
ducted to identify patients undergoing surgical fixation for a femoral neck fracture. Five different ML algorithms were
developed and internally validated and assessed by discrimination, calibration, Brier score and decision curve analysis.
Results In total, 2478 patients were included with 90 day and 2 year mortality rates of 9.1% (n = 225) and 23.5% (n = 582)
respectively. The models included patient characteristics, comorbidities and laboratory values. The stochastic gradient boost-
ing algorithm had the best performance for 90 day mortality prediction, with good discrimination (c-statistic = 0.74), calibra-
tion (intercept = ? 0.05, slope = 1.11) and Brier score (0.078). The elastic-net penalized logistic regression algorithm had the
best performance for 2 year mortality prediction, with good discrimination (c-statistic = 0.70), calibration (intercept = ? 0.03,
slope = 0.89) and Brier score (0.16). The models were incorporated into a freely available web-based application, including
individual patient explanations for interpretation of the model to understand the reasoning how the model made a certain
prediction: https://sorg-apps.shinyapps.io/hipfracturemortality/
Conclusions The clinical prediction models show promise in estimating mortality prediction in elderly femoral neck frac-
ture patients. External and prospective validation of the models may improve surgeon ability when faced with the treatment
decision-making.","The aim of this study was to develop and internally vali-
date a clinical prediction model that can predict 90 day
and 2 year mortality in femoral neck fracture patients aged
65 years or above to aid the challenging treatment decision-
making. The developed and internally validated models
show promise in estimating mortality in this frail patient
population. Limitations
The results of this study should be viewed in light of sev-
eral limitations. First, the study was a retrospective study
beholden to limitations inherent to such research design and
prospective validation remains to be evaluated. Second, the
mortality rate in our cohort was relatively low compared to
other populations of hip fracture patients [38]. This resulted
in predicted probabilities as shown in the calibration plots,
up to 50% and 80% risk for respectively 90 day and 2 year
mortality. This means that our model is likely more accu-
rate in healthier hip fracture patients. To ensure external
validation, our model should be validated in a cohort with
representative rates, and future studies should assess the
transportability of the developed algorithm to datasets with
patients with higher mortality rates. Third, for this study,
we chose a 80/20 ratio for data splitting into training and
test set, which has been mostly used in previous literature
[20–22, 39]. There is no fixed rule for the ratio of data split-
ting but a different ratio for algorithm training may have
led to different model performances. Fourth, preoperative
risk stratification for mortality is needed to guide the diffi-
cult treatment decision-making, although intraoperative and
postoperative factors associated with complications, such as
reoperation or postoperative infection, may be confounding
with mortality after surgery. Future research may estimate
this influence looking at causality for confounding factors
[40]. Fifth, patients were included in the study undergoing
femoral neck fracture surgery. However, patients who were
suspected by the clinician of a very short survival predic-
tion (e.g. 30 day) were chosen to be treated conservatively
and were not investigated in this study. In future studies,
both conservative and surgical treated patients should be
included to optimize mortality prediction in all patients
sustaining a femoral neck fracture to guide the challeng-
ing treatment decision-making (i.e. whether to operate or
not?). Sixth, evaluating possible co-injuries occurring during
trauma, some of which may cause significant disability, may
influence survival outcome. Evaluating these co-injuries
and calculating their injury severity score may have had an
influence as candidate input variable on the model perfor-
mance. In addition, we did not investigate the influence of
the presence of advanced directives, which may influence the
decision-making process in patients aged 65 years or above.
In future research, when comparing treatment effects in con-
servatively and operatively treated patients, we recommend
these influences to be investigated. Lastly, the 2 year mortal-
ity was chosen on the basis of endpoints in prior prospective
randomized controlled trials [5, 6]. The 90 days was chosen
to predict short-term mortality and accounts for a possible
underestimation in outcomes seen with only a 30 day mortal-
ity. From a patient and provider perspective, a death 90 days
post hip fracture is just as significant as one within 30 days.
It takes in to account not just acute in-hospital complications
but also short-term complications that may occur in skilled
nursing facility and discharge to the community. There is
growing evidence in other specialties that 30-day mortality
underestimates short-term mortality [41, 42]. Future stud-
ies may additionally investigate earlier time points, such as
30 days or 1 year.
Findings
In the ranges of risk where we think clinical utility of the
model is to be expected, the 2 year model clearly adds
clinical utility over treating everyone or none with total
hip arthroplasty. However, we assumed a more simplified
scenario, since there are multiple treatment options avail-
able, namely nonoperative management, surgical fixation
and arthroplasty surgery. The 90 day mortality model might
add clinical utility for decisions between these tiered treat-
ment options, which are more subtle and complex to assume.
Moreover, clinical utility should be reassessed after external
validation, and with input from multiple institutions from
different countries. If found to be externally valid (gener-
alizable to independent populations), future studies should
prospectively evaluate the developed and validated tool. In
patients with limited life expectancy, patients predicted with
a high risk of short-term mortality, nonoperative manage-
ment might be a viable option in the shared decision-making
process compared to surgical fixation [8]. If patients have
a high chance of surviving beyond the 90 day endpoint,
surgical management would be in place [43]. Frail patients
with a nondisplaced hip fracture may be favored to surgical
fixation compared to arthroplasty surgery [6, 18]. However,
arthroplasty is associated with a lower risk of reoperation
and better long-term functional outcomes, at the cost of
greater infection rates, blood loss, and operative time and
possibly an increase in early mortality rates and may be rec-
ommended in patients with a longer-term life expectancy
(e.g., high probability of surviving beyond the 2 year end-
point) [44].
When aiming to develop a prediction model that is appli-
cable in daily practice, variables should be included in the
trained algorithm that are readily available and use of defini-
tions that are in line with daily practice should be followed.
In this study, variables derived from variable selection are
clinically readily available and in line with daily practice. It
is important to emphasize that treatment decision-making
should not be solely based on the outcome of an individual-
ized probability calculator. The orthopaedic surgeon should
discuss the available treatment options and reach a treatment
decision following a shared decision-making process. Pre-
diction of mortality is only one of the aspects to be consid-
ered in treatment decision-making. The most important factors associated with a greater
risk of 90 day mortality included in the SGB algorithm
were INR, age, creatinine level, absolute neutrophil, CHF,
male gender, hemoglobin level, displaced fracture, hemi-
plegia and COPD. For 2 year mortality, the most impor-
tant factors were age, male gender, absolute neutrophil,
CHF, use of beta-blocker, COPD, CVA, hemoglobin,
creatinine level and INR. Our findings are in line with
previous research on proximal femoral neck fractures in
general and broader populations. Regarding age and sex,
prior studies revealed a higher risk for higher age and the
male gender [45–47]. The effect of CHF, CVA and COPD
is in line with the high risk reported for a higher ASA
classification in earlier studies [48, 49]. A possible expla-
nation for this effect might be a lower physical condition
of the patient at baseline and therefore a less adequate
recovery after complications (e.g. pneumonia). Another
explanation for comorbidities in general could be a lower
life expectancy as a result of the comorbidity itself. In
regard to displacement of the fracture, a reasonable expla-
nation for the higher risk might be the disruption of the
vascularization of the femoral head and the tendency that
a displaced fracture comes from a frailer patient to start
with where more displacement occurred compared to a
younger patient (with the same level energy of trauma).
This could lead to multiple complications and secondary
surgery eventually resulting in death [50]. The prognostic
value of laboratory characteristics in predicting mortality
after hip surgery is a less explored subject. But the eleva-
tion of creatinine and absolute neutrophil count reflects
respectively declined renal function and inflammation
[51]. Which again is linked to a higher ASA score and a
lower baseline physical condition. Whereas a higher INR
is reflecting the inability to coagulate and most likely the
use of anticoagulants, resulting in a higher risk for bleed-
ing and as a result of this a higher risk for morbidity and
mortality [46, 51]. On the contrary a lower hemoglobin
is related to chronic comorbidities, which might reflect in
a lower odds for mortality for higher hemoglobin levels
[51].
Over the recent years, a lot of research has been done
predicting mortality in femoral neck fracture patients. The
greater part of these tools developed made an estimation
of risk based on age, gender and in general the presence
of comorbidity [52, 53], whereas the other part looked
at postoperative factors, such as early ambulation after
surgery and postoperative lab values [54, 55]. In contrast
to the broader presence of comorbidity, our study used
the ability of ML algorithms to differ between the effects
of different types of comorbidity in a large database to
estimate the individual value of each factor. This resulted
in a more patient centered prediction tool.
Future perspectives
External validation is essential before testing and imple-
menting the ML algorithm in clinical practice. Subsequently, a prospective observational study of the comparison of the current ML model prediction compared to a
physician’s prediction of mortality can assess the clinical
usefulness of the developed model. This will assess if the
model’s prediction was more accurate than those of the
treating physician [56]. An internally and externally vali-
dated algorithms can then be integrated into the electronic
health record with an active feedback loop to improve the
model performance and ultimately be integrated in the
clinical workflow [57, 58]."
"The Effects of Race/Ethnicity, Age, and Area Deprivation Index (ADI)
on COVID-19 Disease Early Dynamics: Washington, D.C. Case Study","The COVID-19 pandemic and its associated mitigation strategies have significant psychosocial, behavioral, socioeconomic,
and health impacts, particularly in vulnerable US populations. Different factors have been identified as influencers of the
transmission rate; however, the effects of area deprivation index (as a measure of social determinants of health, SDoH) as a
factor on COVID-19 disease early dynamics have not been established. We determined the effects of area deprivation index
(ADI) and demographic factors on COVID-19 outcomes in Washington, D.C. This retrospective study used publicly avail-
able data on COVID-19 cases and mortality of Washington, D.C., during March 31st–July 4th, 2020. The main predictors
included area deprivation index (ADI), age, and race/ethnicity. The ADI of each census block groups in D.C. (n=433) were
obtained from Neighborhood Atlas map. Using a machine learning-based algorithm, the outcome variables were partitioned
into time intervals: time duration (P i , days), rate of change coefficient (E i ), and time segment load (P i ×E i ) for transmission
rate and mortality. Correlation analysis and multiple linear regression models were used to determine associations between
predictors and outcome variables. COVID-19 early transmission rate (E 1 ) was highly correlated with ADI (SDoH; r= 0.88,
p=0.0044) of the Washington, D.C. community. We also found positive association between ADI, age (0–17 years, r=0.91,
p=0.0019), and race (African American/Black, r=0.86; p=0.0068) and COVID-19 outcomes. There was high variability in
early transmission across the geographic regions (i.e., wards) of Washington, D.C., and this variability was driven by race/
ethnic composition and ADI. Understanding the association of COVID-19 disease early transmission and mortality dynam-
ics and key socio-demographic risk factors such as age, race, and ADI, as a measure of social determinants, will contribute
to health equity/equality and distribution of economic resources/assistance and is essential for future predictive modeling
of the COVID-19 pandemic to limit morbidity and mortality.","In this study, we determined the effects of area deprivation
index (ADI, a measure of social determinant) and demo-
graphic factors (age and race/ethnicity) on COVID-19 dis-
ease early transmission dynamics and burden (morbidity and
mortality) in Washington, D.C. We demonstrated that early
transmission is highly correlated with racial/ethnic compo-
sition, age structure, and the social determinants (as meas-
ured by ADI) of the Washington, D.C. community, and our
findings support previous studies (e.g., [5]). Various studies
on COVID-19 disease show that many US populations that
were previously known to bear the consequences of dispari-
ties in health outcomes are now experiencing the additional
burden of disproportionate COVID-19 disease morbidity
and mortality. These vulnerable populations share several
social determinants of health (SDoH). Using ADI, as meas-
ure of SDoH, we observed a great disparity in ADI among
wards in Washington, D.C. Our findings also showed signifi-
cant positive association between ADI and COVID-19 early
transmission rate. Our results support recent study findings
by Madhav et al. [23] in Louisiana state that ADI affects
COVID-19 risk at the granular neighborhood environment
level. Area depravation index (ADI) as a measure of social
determinants considers income, education, employment, and
housing quality [10]. Systemic inequities in health, social
status, and income contribute to increased risk of COVID-19
transmission in deprived neighborhoods [6, 24]. The D.C.
wards 7 and 8 with higher ADI score (more-disadvantaged
neighborhood) also recorded higher initial transmission rate
for COVID-19, which may be due to their lack of resources.
Other studies also show that in low-income or deprived
regions, higher COVID-19 incidence, and mortality have
been observed [25–27]. Recent findings by Bilal et al. [26]
reported a 36% increased incidence of COVID-19 in less
deprived neighborhoods.
Racial/ethnic distribution has been reported as a major
moderating factor in COVID-19 disease. Our findings
demonstrated positive association between initial trans-
mission rate E 1 and African American population in D.C.
This result suggests African Americans are at higher risk
of increased initial disease transmission rate which support
previous findings by Raifman and Raifman [28]. This situ-
ation is likely because African American/Blacks and other
vulnerable minority population live in low-income regions
coupled by structural disparities in wealth, medical insur-
ance, facilities, and others [29]. Furthermore, African
Americans are more likely to have chronic medical condi-
tions such as hypertension, diabetes, heart disease, and 1 3obesity, which are related to increased risk of illness from
COVID-19. Additionally, African Americans also usu-
ally live in crowded housing, work at crowed workplace/
worksite, and are more likely to rely on public transporta-
tion that exposes them to an increased risk for COVID-19
disease [30, 31]. On the other hand, our findings showed
no association between mortality and other race/ethnicity
except Latinos/Hispanics. Most of recent studies found
that African Americans comprise higher proportion of
deaths relative to their percentage in those jurisdictions’
population [32]. In contrast with Egbert [32] study, we
found no association between AA population and disease
mortality for Washington, D.C., during early transmission
period.
For Washington, D.C., our findings were contrary to
previous studies that found 65-and-older group has higher
risk of disease transmission rate and mortality [33, 34].
A study by Wu et al. [33] reported that in Wuhan, China
(where the disease was first reported), ages below 30 and
above 59 have higher mortality rate when they develop
symptoms of the disease. Although some previous stud-
ies have shown that children and young adults (ages 0–17
years) are susceptible to the SARS-CoV-2 infection, very
few studies have focused on the potential severity of the
disease in the younger age group. In the current study,
we found a strong positive association between age (0–17
years) and confirmed cases (per 100,000), early transmis-
sion rate (E 1 ), and deaths (per 100,000). Our results sup-
port the finding of DeBiasi et al. [35] that highlighted the
potential COVID-19 severity in pediatric population (0–15
years) in Washington, D.C.
In summary, we observed a great disparity in area dep-
rivation index among Washington, D.C. wards. There is
high variability in early transmission across the geographic
regions of Washington, D.C. (neighborhood), and this vari-
ability is driven by race/ethnic composition and social deter-
minants (as measured by ADI). Mortality on the other hand
is not correlated with race, age, and ADI at community-wide
population. This may enable better interpreting the pandemic
trajectory in terms of social risk factors in Washington, D.C."
"Framework for Integrating Equity Into
Machine Learning Models
A Case Study","Predictive analytic models leveraging machine learning methods increasingly have become vital
to health care organizations hoping to improve clinical outcomes and the efficiency of care
delivery for all patients. Unfortunately, predictive models could harm populations that have
experienced interpersonal, institutional, and structural biases. Models learn from historically
collected data that could be biased. In addition, bias impacts a model’s development, appli-
cation, and interpretation. We present a strategy to evaluate for and mitigate biases in machine
learning models that potentially could create harm. We recommend analyzing for disparities
between less and more socially advantaged populations across model performance metrics (eg,
accuracy, positive predictive value), patient outcomes, and resource allocation and then
identify root causes of the disparities (eg, biased data, interpretation) and brainstorm solutions
to address the disparities. This strategy follows the lifecycle of machine learning models in
health care, namely, identifying the clinical problem, model design, data collection, model
training, model validation, model deployment, and monitoring after deployment. To illustrate
this approach, we use a hypothetical case of a health system developing and deploying a ma-
chine learning model to predict the risk of mortality in 6 months for patients admitted to the
hospital to target a hospital’s delivery of palliative care services to those with the highest
mortality risk. The core ethical concepts of equity and transparency guide our proposed
framework to help ensure the safe and effective use of predictive algorithms in health care to
help everyone achieve their best possible health. ","This case study of developing and deploying a machine
learning model to predict 6-month mortality illustrates
the potential power as well as the equity challenges
associated with implementing machine learning models
into a health system. We present a structured strategy to
evaluate for and mitigate harmful bias in machine
learning models for health care organizations. The use of
this framework is one way to ensure the safe, effective,
and equitable deployment of predictive models,
especially those using machine learning, in health care
that can advance health equity.
The main goal of our framework is for machine learning
models deployed in health systems to achieve health
equity paired with transparency to clinicians and patients. As outlined and reviewed through the lens of
this hypothetical case, we recommend that health system
model developers and leaders look for disparities in
model performance metrics, patient outcomes when the
model is turned on, and allocation of resources to
patients based on model’s output. Because each health
system and machine learning model are different, no
simple solution exists to ensure health equity in
developing and using machine learning algorithms.
However, we believe our framework can ensure the best
chance for safe, effective, and equitable deployment of
machine learning models in health care if used within a
broader organizational culture in which equity activities
move beyond a check-the-box mentality and everyone
takes responsibility for proactively identifying inequities
and addressing them. 16
Despite our concerns that machine learning has the
potential to exacerbate health disparities and inequity,
we remain optimistic that this technology can improve
the care delivered to patients substantially if it is
integrated into health care systems thoughtfully. Because
each system is unique, we recommend that each system
assemble a team of diverse stakeholders that can create a
local approach for model building and deployment that
advances equity based on our framework. All patients
can benefit fairly from machine learning in medicine if
equity is a key consideration in how machine learning
models are developed, deployed, and evaluated."
"Machine learning risk prediction
model for acute coronary
syndrome and death from use
of non?steroidal anti?inflammatory
drugs in administrative data","Our aim was to investigate the usefulness of machine learning approaches on linked administrative
health data at the population level in predicting older patients’ one?year risk of acute coronary
syndrome and death following the use of non?steroidal anti?inflammatory drugs (NSAIDs). Patients
from a Western Australian cardiovascular population who were supplied with NSAIDs between 1 Jan
2003 and 31 Dec 2004 were identified from Pharmaceutical Benefits Scheme data. Comorbidities
from linked hospital admissions data and medication history were inputs. Admissions for acute
coronary syndrome or death within one year from the first supply date were outputs. Machine learning
classification methods were used to build models to predict ACS and death. Model performance was
measured by the area under the receiver operating characteristic curve (AUC?ROC), sensitivity and
specificity. There were 68,889 patients in the NSAIDs cohort with mean age 76 years and 54% were
female. 1882 patients were admitted for acute coronary syndrome and 5405 patients died within one
year after their first supply of NSAIDs. The multi?layer neural network, gradient boosting machine
and support vector machine were applied to build various classification models. The gradient boosting
machine achieved the best performance with an average AUC?ROC of 0.72 predicting ACS and 0.84
predicting death. Machine learning models applied to linked administrative data can potentially
improve adverse outcome risk prediction. Further investigation of additional data and approaches are
required to improve the performance for adverse outcome risk prediction.","This study presents a set of machine learning models for predicting the risk of ACS and all-cause death after
dispensing of NSAIDs using data from PBS, HMDC and death in Western Australia. We focused specifically
on elderly patients (age ? 65 years) who had at least one NSAID supply. The prediction is based on the features
including age, sex, medication history and disease history, which are routinely collected in administrative data.
This approach encompasses a wide array of patients to reflect the population of patients taking NSAIDs in
Western Australia. The machine learning based predictive models showed greater sensitivity, specificity and
AUC-ROC values compared with the classical Cox-regression approach. GBM presented the best predictive
performance for the machine learning models we tested.
Several studies have reported the risk of adverse outcomes with NSAIDs, and rofecoxib was withdrawn from
the market due to its increased risk of CV outcomes. Our models predict ACS, all-cause death and composite
outcome. The performance for predicting death was the best with AUC-ROC values ranging from 0.76 (Cox
regression) to 0.84 (GBM). This demonstrates that the predictive models built based on administrative data work
well and can predict the risk of death. The performance of the ACS risk prediction was lower, with AUC rang-
ing from 0.66 (Cox) to 0.72 (GBM). The performance may be limited by the low event rate of ACS (4%), which
makes the class distribution highly imbalanced. As shown in Table 2, GBM has slightly outperformed MLNN
and SVM for predicting the risks of ACS, and SVM for predicting the risks of ACS and death. This difference
may result from the nature of the boosting power in GBM, which is an ensemble method using many trees to
make a decision as it gains power by repeating itself. MLNN is also a powerful model as it can learn complex data
representations from underlying data, but is prone to overfitting31
. Other studies have also found GBM can result
in higher prediction accuracies compared with MLNN and SVM 32,33 . We considered the range of AUC-ROC we
measured to be of moderate to high accuracy in predicting the risk of ACS or death in this population. While an
ideal precision would be an AUC-ROC > 0.90, such high values are not easy to achieve in medical applications of machine learning due to the variations in patient characteristics we see in humans. Furthermore, this is our
initial investigation on the potential for machine learning models to be applied for prediction of ACS and all-
cause death using population-level administrative data. Further work needs to be done to determine if model
performance can be improved, especially if other datasets can be added at the population level. We acknowledge
that the outputs from the machine learning models do not necessarily suggest a causal link between the drug
and the ACS admission or death. Instead, its purpose is to create an alert so that humans (clinicians, researchers,
administrators) can investigate further and make a decision on whether the risk requires clinical or regulatory
action. Hence, the machine learning application here will have clinical value as a decision support tool.
Risk prediction models have been used on different data sources (e.g. electronic medical record, adminis-
trative data) to identify risk of adverse outcomes for drugs. For example, predicting opioid overdose risk on
administrative data with opioid prescriptions using deep neural networks and GBM 34 , predict adverse drug
reactions from ICD-10 codes using machine learning models 35 and comparing logistic regression with machine
learning in predicting the risk of death from drug intoxication36
. The AUC-ROC of the models from these studies
ranged from 0.69 to 0.91. Our study made use of multiple linked administrative datasets, focusing on drugs and
outcomes, and our machine learning risk prediction models achieved a range of AUC-ROC from 0.70 to 0.84.
This is consistent with the performance attained in the previous studies reported above. Moreover, these studies
found that the machine learning approach did not show better performance than a classical generalised regres-
sion approach 17,37 . However, our machine learning models performed better than the Cox regression models. This could be because most of the input features in our model were continuous variables, and machine learning
models outperform on complex variables.
To our knowledge, there are no studies that explore the predictive capabilities of machine learning models for
ACS and all-cause death in patients supplied with NSAIDs. Our study has several strengths. The risk prediction
model we developed can be used to identify specific CV adverse outcomes of NSAIDs. The models can inform
doctors on which NSAID has the lowest risk of these CV outcomes based on individual patient’s medication his-
tory and disease history. Moreover, our models have been developed using population-based datasets to identify
patients with a high risk of adverse outcomes.
Our study found that the inclusion of demographic features such as marital status, Indigenous ethnicity
from linked hospital admissions data improved the performance of the prediction models. The average AUC
was similar for predicting ACS (AUC 0.71). However, the performance was higher while predicting the risk of
all-cause mortality (AUC 0.81 vs 0.84) and composite outcome (AUC 0.77 vs 0.78), with no overlap in their
confidence intervals. Previous studies have shown that marital status is associated with adverse cardiovascular
outcomes and mortality was higher in an unmarried population 38,39 . Studies have also shown that Indigenous
Australians have a greater risk of cardiovascular disease and death 40,41 .
We extracted additional features from the hospital admissions dataset, including patients’ previous length
of stay (days) in the hospital for each comorbidity, and the number of days patients spent in intensive care units
(ICU) before their first supply. This set of features were presented as continuous variables. We included this set
of features to test whether it would improve the risk prediction. However, there were no performance gains by
adding continuous variables such as length of hospital stay of previous comorbidities and days in ICU. The AUCs
of all the outcomes were similar to models that did not include these extra features. Hence, we dropped these
features to reduce model complexity.
In our study, we observed minimal performance improvement when using binary variables for comorbidities
or drug history, indicating the presence of comorbidities and history of drugs. However, ML models achieved
better performance than Cox regression when we used continuous variables for total counts of medication his-
tory and comorbidity history. This may be because machine learning approaches do not assume linearity for a
predictor-outcome association. They are more adept at generating predictions based on continuous variables 42 .
Our machine learning model ranked COX-2 inhibitors higher among other NSAIDs for ACS risk predic-
tion. Multiple previous studies have reported an increased risk of CV events from the use of selective COX-2
inhibitors1,3–6
. Rofecoxib was withdrawn from markets based on evidence that showed an increased risk of ACS5
.
Naproxen and ibuprofen have been reported in several studies to be NSAIDs with less risk 1,43 . Compared with
other popular NSAIDs, the rank of naproxen and ibuprofen was lower in our study, which is consistent with
previous research. A previous study has confirmed that heart failure substantially increases the risk of death 44 .
This verifies that our machine learning model is reliable in ranking feature importance as it showed the same
relationship.
Despite the value of this study, there are some limitations. As with all administrative database studies, this
study relies on the accuracy of administrative coding of diagnoses and procedures. However, the point of our
study is that is makes use of multiple administrative datasets, which are large datasets that capture information
at the population level. Despite whatever issues there may be with potential coding errors, we need these types of
datasets to be able to adequately build a machine learning solution with potential for patient risk management.
The PBS dataset did not include all dispensing supplies of NSAIDs such as ibuprofen, as this is also available
over the counter. Moreover, the PBS dataset did not contain information about the actual drug dosage. Hence,
in our study, we calculated the total number of supplied scripts rather than the dose used. In our study, we used
state-level linked data to predict patients’ adverse CV outcomes after their NSAIDs supply. The models can be
further extended to national linked data in the future. Also, for general applicability, the models can be potentially
extended to other drugs or drug groups and different outcomes, and this can also be tested in future studies.
Implementing ML models on linked administrative data, including pharmacy claims (e.g. PBS), morbidity,
and mortality has the potential to identify patients supplied with NSAIDs that may have a high risk of adverse CV
outcomes. These can then be monitored closely by humans. Further investigation of additional data is required to
validate the ML prediction performance on patients’ risk of CV adverse outcomes using population-level linked
data. At this early stage our models were built with specific inputs from the research team, including looking
at a specific follow-up period from NSAID use. However, further research will move towards more autonomy
where the machine learning models will decide which drugs are potential problems and flag them for further
investigation."
Sex-Specific Patterns of Mortality Predictors Among Patients Undergoing Cardiac Resynchronization Therapy: A Machine Learning Approach,"Background: The relative importance of variables explaining sex-related differences in
outcomes is scarcely explored in patients undergoing cardiac resynchronization therapy
(CRT). We sought to implement and evaluate machine learning (ML) algorithms for the
prediction of 1- and 3-year all-cause mortality in CRT patients. We also aimed to assess
the sex-specific differences in predictors of mortality utilizing ML.
Methods: Using a retrospective registry of 2,191 CRT patients, ML models were
implemented in 6 partially overlapping patient subsets (all patients, females, or males
with 1- or 3-year follow-up). Each cohort was randomly split into training (80%) and test
sets (20%). After hyperparameter tuning in the training sets, the best performing algorithm
was evaluated in the test sets. Model discrimination was quantified using the area under
the receiver-operating characteristic curves (AUC). The most important predictors were
identified using the permutation feature importances method.
Results: Conditional inference random forest exhibited the best performance with
AUCs of 0.728 (0.645–0.802) and 0.732 (0.681–0.784) for the prediction of 1- and
3-year mortality, respectively. Etiology of heart failure, NYHA class, left ventricular ejection
fraction, and QRS morphology had higher predictive power, whereas hemoglobin was
less important in females compared to males. The importance of atrial fibrillation and
age increased, while the importance of serum creatinine decreased from 1- to 3-year
follow-up in both sexes.
Conclusions: Using ML techniques in combination with easily obtainable clinical
features, our models effectively predicted 1- and 3-year all-cause mortality in CRT
patients. Sex-specific patterns of predictors were identified, showing a dynamic variation
over time.","Using data from a single-center cohort of HF patients undergoing
CRT implantation, we developed and evaluated ML-based
algorithms for the prediction of 1- and 3-year all-cause mortality.
The resulting CIRF models demonstrated good discriminatory power in assessing the risk of mortality with an AUC over
0.700 at 1- and 3-year follow-up. Moreover, ML performed
substantially well across patient subsets containing exclusively
males or females (AUCs ranging from 0.681 to 0.798). Serum
sodium, creatinine, hemoglobin, age, and HF etiology were
among the most important determinants of short- and mid-term
mortality; however, their relative importance varied over time.
As expected, female sex was associated with significantly better
survival rates in our cohort as well. Sex-specific patterns were
also identified in the predictors of mortality. The role of HF
etiology (ischemic or non-ischemic), NYHA functional class, and
LVEF were more pronounced in females, whereas hemoglobin
concentration, QRS morphology, and treatment with allopurinol
were notably more predictive for all-cause mortality in males. The personalized prediction of prognosis is fundamental to
patient-centered care, both in optimizing treatment strategies
and informing patients as part of shared decision making. For
this purpose, an abundance of prediction models has been
developed; however, most of them had achieved only modest
success, particularly when they were applied in HF populations
other than those from which the scores were derived (22, 23).
The unsatisfactory results of previous HF risk scores are likely
due to multiple causes, including the fact that most of them
were created using conventional statistical methods that failed
to capture high-dimensional interactions among predictors that
bear relevant prognostic information.
In contrast to traditional statistics, ML was explicitly designed
to reveal and harness these correlations. Several studies have
proved that these advanced data analytic approaches can leverage
the complex, higher-level interplay between predictors and
outcomes to achieve better discrimination. ML can improve
the care of HF patients in various ways, e.g., by augmenting
the prediction of readmission after HF hospitalization or by
predicting the risk of mortality (16, 17, 19). In HF patients
undergoing CRT implantation, our research group has previously
confirmed the superiority of ML over pre-existing risk scores
(24), and similar results have been reported by others as well
(25, 26). Underpinning these findings, we were able to predict the
1- and 3-year mortality of CRT patients with good discrimination
and excellent calibration, even in subsets of patients divided by
sex. In light of the promising results of our single-center study,
we will endeavor to validate our models in external cohorts in a
multi-centric manner.
In our analysis, CIRF exhibited the best discriminative ability
for predicting both 1- and 3-year mortality. To understand
the outstanding performance of tree-based approaches such as
CIRF in outcome prediction, an important difference between
conventional regression models and tree-based methods should
be highlighted. The former favors variables that have a uniform
effect across the entire patient population, whereas the latter can
uncover variables that might act differently in different patient
subgroups. This is essential for personalized prognostication as in
an individual patient, the discriminatory power of a given feature
may be significantly enhanced or overshadowed by others. Due
to this attribute, tree-based methods such as TRF and CIRF are
extremely suitable for application as clinical decision-making
tools (27). Sex is increasingly recognized as an important modulator of
outcomes in CRT patients, and several studies such as the
MADIT-CRT (10), the RAFT (28), or the MASCOT (29) trials
have suggested a greater CRT benefit in women. Despite the
expanding knowledge about sex-related differences in HFrEF, the
reason women benefit more than men from CRT remains unclear
(14). Numerous plausible explanations have been proposed,
such as the dissimilarities between sexes in the frequency of
ischemic cardiomyopathy (30), AF, and comorbidities (9), or
the sex-related differences in body height, LV size, and QRS
duration (31, 32). In addition, the impact of sex hormones on
the pathophysiology of HF or the sex-specific characteristics of
pharmacodynamics and pharmacokinetics are also considerable
factors (4, 33).
The sex-specific effects of QRS prolongation and morphology
on outcomes have been intensively investigated in CRT patients
(30, 31, 34–37). Thus, the findings of these studies have prompted
calls for sex-specific guideline recommendations regarding the
selection of CRT recipients. As women have shorter QRS
durations than men in the absence of any conduction delay, they
are more likely to exhibit a true LBBB compared to men at shorter
QRS duration (38, 39). It has also been reported that among
patients with LBBB and non-ischemic etiology, women have
electrical dyssynchrony more frequently compared to men at
any given QRS duration, and consequently, they would exhibit a
better response to CRT (35). According to the study conducted by
Beela et al., the interaction between HF etiology and mechanical
dyssynchrony seems to represent another important aspect:
due to the lower rate of ischemic etiology and the lower
extent of scarred myocardium, women have more frequently
uncomplicated patterns of LBBB-like mechanical dyssynchrony
which is better amendable by CRT (30).
The beneficial effects of CRT also depend on device
programming and the percentage of effective biventricular
pacing. Notably, that latter significantly varies by sex, and
therefore, sex-specific CRT programming has attracted increased
attention (40). According to the results of the SMART-AV trial,
the optimization of atrioventricular delay intervals is associated
with improved outcomes in women but not in men (41), which
might be attributable to the inherent sex-related differences
in atrial geometry and PR intervals. A higher percentage of
biventricular pacing has also been reported in women (29, 41, 42), most probably due to the lower rate of atrial fibrillation compared
to men (43, 44). This could also contribute to the observed
differences in mortality between sexes as even a small increment
in the biventricular pacing rate may improve outcomes (45).
Although there are still many open questions, it is clear that
multiple intercorrelated factors contribute to this phenomenon.
Therefore, during the search for answers, ML-based approaches
may come in handy, as they are particularly helpful in
uncovering hidden patterns in large datasets by simultaneously
interpreting predictors even in the presence of complex, non-
linear interactions.
Sex-Specific Patterns in Mortality
Predictors
Given the sex-related differences in the anatomy and physiology
of the cardiovascular system, encountering dissimilarities in the
importance of prognostic predictors between males and females
is to be expected in CRT patients. Nevertheless, there is only
a limited number of publications dedicated to the thorough
exploration of this topic. To the best of our knowledge, our
study is the first that evaluated the sex-related differences and
similarities in mortality predictors of CRT patients using ML. In
our analysis, we observed significant variations in the importance
of several predictors such as HF etiology, NYHA functional class,
LVEF, and AF between sexes, to name a few.
Utilizing the tools of conventional statistics, the sex-specific
prognostic value of HF etiology has been previously investigated
in large cohorts of HFrEF patients. In the MAGGIC meta-
analysis, the ischemic etiology appeared to attenuate the
protective effect of female sex on prognosis (46). In addition,
ischemic cardiomyopathy and the extent of myocardial scar were
found to be significant predictors of mortality in females but not
in males among CRT patients (30). In line with this evidence, the
paramount importance of HF etiology in women was proved in
our study as well.
When analyzing the interaction between sex and different
covariates in the prediction of survival after CRT implantation,
Beela et al. reported that NYHA class was a significant predictor
in males only (30). Moreover, among HFrEF patients, NYHA
class had a more prominent prognostic value in men than in
women (3). Contrary to these findings, a stronger association of
NYHA functional class with outcomes was observed in females
in our current analysis and the BEST trial as well (47).
Another well-established prognostic factor is LVEF, whose
interaction with sex in the prediction of all-cause death has
been demonstrated in CRT patients (30). Complementing these
findings and the results of the BEST trial (47), we have also
demonstrated that LVEF is a stronger predictor of prognosis in
women than in men.
In HFrEF patients, most studies agree on the prognostic value
of AF; however, there is some inconsistency regarding its exact
role as some investigations attribute more prognostic impact
to AF in females (47), whereas others observed comparable
predictive power in males and females (3, 30). Our results support
the former as we found AF to have a more prominent effect on
outcomes in females.
According to our analysis, the prognostic relevance of
hyponatremia and renal function should also be emphasized in
CRT patients. Our results are in accordance with the findings
of Zusterzeel et al., who reported that despite being significant
determinants in both sexes, serum creatinine and hyponatremia
appeared to be stronger predictors in women than in men (34).
Lately, the interplay between sex and diabetes in HFrEF
patients has attracted increased attention among researchers.
Confirming the findings of the MAGGIC (46), the recently
published analysis of the ASIAN-HF registry demonstrated that
diabetes is coupled with a greater risk of adverse outcomes in
women than in men (48). In contrast, diabetes was associated
with a higher risk of all-cause death or HF hospitalization in
males in the Swedish HF Registry (3), and it was proven to
be a significant predictor only in men in the BEST trial (47).
Interestingly, in our study, diabetes was not ranked among the
top five predictors in any of the analyzed patient subsets, and
we detected inter-sex differences in its importance only at 3-
year follow-up.
Some of our findings coincide with those of previous studies,
whereas some others may not. These apparent discrepancies
might be partly attributable to the fact that most studies
applied Cox proportional hazards regression, whereas we utilized
an entirely different methodology that captures other aspects
of associations between risk factors and outcomes. Although
the exact reasons behind these contradicting results should
be clarified in further investigations, our findings underscore
the necessity of sex-specific approaches in the management of
HFrEF patients.
Limitations
Despite the highlighted advantages, there are a few limitations
to be acknowledged. First, our study represents results from
a single center. As we were aware of this limitation, we
performed hyperparameter tuning with 10-fold cross-validation
in the training cohorts, and we also tested our models in
statistically independent test cohorts to enhance generalizability.
Nonetheless, as the next step, the robustness of our models
should be tested in external populations as well. Second, the
utilized database bears the inherent limitations of retrospective
data collection, such as the higher proportion of missing data
(compared to prospective trials) and the heterogeneity partly
attributable to the changes in guideline recommendations
over the years. However, the use of such real-world data holds
the potential for better generalizability. Third, our models
use baseline (pre-implant and procedural) variables without
incorporating the time-varying values of these parameters.
Although a dynamic model integrating values of the same
parameter from multiple time points may be superior, in the
present study, we aimed to predict 1- and 3-year mortality using
clinical data that could be acquired at device implantation.
Finally, there may remain additional domains of variables
(e.g., imaging data, novel biomarkers, genetics, or quality of
life questionnaires) that could further improve the predictive
capability of our models. Future work should explore the
addition of such features to enhance the models proposed in the
present study."
"Predicting Mortality Risk After a Hospital or Emergency
Department Visit for Nonfatal Opioid Overdose","BACKGROUND: Survivors of opioid overdose have sub-
stantially increased mortality risk, although this risk is
not evenly distributed across individuals. No study has
focused on predicting an individual’s risk of death after a
nonfatal opioid overdose.
OBJECTIVE: To predict risk of death after a nonfatal
opioid overdose.
DESIGN AND PARTICIPANTS: This retrospective cohort
study included 9686 Pennsylvania Medicaid beneficiaries
with an emergency department or inpatient claim for non-
fatal opioid overdose in 2014–2016. The index date was
the first overdose claim during this period.
EXPOSURES, MAIN OUTCOME, AND MEASURES: Pre-
dictor candidates were measured in the 180 days before
the index overdose. Primary outcome was 180-day all-
cause mortality. Using a gradient boosting machine mod-
el, we classified beneficiaries into six subgroups according
to their risk of mortality (< 25th percentile of the risk
score, 25th to < 50th, 50th to < 75th, 75th to < 90th,
90th to < 98th, ? 98th). We then measured receipt of med-
ication for opioid use disorder (OUD), risk mitigation in-
terventions (e.g., prescriptions for naloxone), and pre-
scription opioids filled in the 180 days after the index
overdose, by risk subgroup.
KEY RESULTS: Of eligible beneficiaries, 347 (3.6%) died
within 180 days after the index overdose. The C-statistic
of the mortality prediction model was 0.71. In the highest
risk subgroup, the observed 180-day mortality rate was
20.3%, while in the lowest risk subgroup, it was 1.5%.
Medication for OUD and risk mitigation interventions af-
ter overdose were more commonly seen in lower risk
groups, while opioid prescriptions were more likely to be
used in higher risk groups (both p trends < .001).
CONCLUSIONS: A risk prediction model performed well
for classifying mortality risk after a nonfatal opioid over-
dose. This prediction score can identify high-risk sub-
groups to target interventions to improve outcomes
among overdose survivors.","In a large state Medicaid program, a risk prediction model
showed good performance in predicting mortality within
180 days after a nonfatal opioid overdose and could effectively
classify beneficiaries into subgroups based on estimated prob-
ability of death. We also found an inverse relationship be-
tween predicted mortality risk after overdose and receipt of
medication treatment for opioid use disorder or risk mitigation
interventions for substance use.
Receiving care for opioid overdose during emergency de-
partment or inpatient visits is an opportunity to identify and
intervene in patients with opioid use disorder.24 Because the
number of opioid-involved overdoses remains high in the
USA, it is important to be able to identify individuals at
increased risk of death after nonfatal overdoses and target
interventions, resources, and attention to those individuals
most likely to benefit. It is important to note that shortly after
our study period ended, Pennsylvania launched several pro-
grams to improve health outcomes of patients with opioid use
disorder, including more intensive warm hand-off programs to
ensure a seamless transition for opioid overdose survivors
from emergency medical care to specialty substance use dis-
order treatment,25 programs to enhance access to naloxone,26
and Centers of Excellence program to include quality of
opioid use disorder treatment.27 To our knowledge, this is
the first study that has focused on developing a tool for index and at the level of 90% sensitivity) to provide
comprehensive understanding on the model performance.
At 90% sensitivity, the model performed well to eliminate
most of the individuals at minimal risk of death (NPV >
98% for 180-day mortality, 99.2% for 90-day mortality).
The risk stratification based on individual prediction score
showed good performance in identifying beneficiaries at
greater risk of death after a nonfatal overdose. In the
subgroup having a risk score in the top 2%, the observed
mortality rate was as high as 20% within 180 days of the
index overdose. Future studies are needed to externally
validate this prediction model.
When trying to predict risk of death among survivors of
opioid overdose, it may be difficult to further differentiate
between individuals who are already at generalized increased
risk of premature mortality3,4 recurrent overdose,28 and other
adverse health outcomes.29 This difficulty is similar to the
challenge that has been described when predicting
readmissions.30–33 Because the current model was developed
based only on claims data, we were not able to incorporate
important clinical and social-behavioral information that could
potentially improve prediction performance, such as vital signs and lab results during the overdose visit, severity of
comorbid conditions, and access to illicit opioids.
Many of the important features in our prediction model
are regional level factors (e.g., rate of persons in deep
poverty and z score of quality of life, seen in Figure S2).
These regional factors may be proxies for other character-
istics or social circumstances of individuals and reflect
variables one might obtain from the medical record. The
prediction performance of the model might be improved
with using more robust linked data including behavioral
factors and medical records. Indeed, Pennsylvania is cur-
rently working towards such integration of data from
multiple sources.
Findings from the post hoc analyses point to potential
uses of risk prediction following nonfatal overdose. Over-
dose survivors at the highest risk of death based on the
prediction score were most likely to have prescription
opioid fills after overdose. While medication treatment
has been shown to prevent premature death in individuals
with opioid use disorder, 10,11 less than one-third of pa-
tients in our cohort received medication treatment after an
overdose, and only half had received risk mitigation interventions for substance use disorders. Patients in the
subgroups at greater predicted risk of death based on
characteristics measured before their overdose were also
less likely to receive treatment or intervention after their
overdoses. This finding could be due to several factors.
Because individuals in the lowest risk group had more
follow-up time, they had greater opportunity to receive
these therapies relative to the higher risk group with a
higher rate of death. It may also be that the factors most
predictive of death following an overdose are also corre-
lated with low perceived need for treatment or care-
seeking behavior. Nevertheless, this post hoc analysis
points to the potential value of risk prediction tools for
identifying patients most at risk and connecting them to
treatment and recovery supports.
Our study has several limitations. First, used Medicaid
claims primarily, and thus any services received, or overdoses
occurring, outside of the healthcare system are not captured.
Thus, the application of our prediction model would be limited
to overdose survivors identified in a medical setting. Second,
because of the lack of access to data on the cause of death, we
were not able to predict cause-specific mortality, such as death
from overdose. Third, because we used Medicaid data from
one state and only up through 2016, the findings may not be
generalizable to other populations or more recent time periods.
Finally, our study is only the first step in risk
prediction—development of the machine learning model.
There are many challenges to implementing such a prediction
model that have not yet been addressed, including addressing
gaps and lags in claims data, updating the model with more
current data, developing the infrastructure that automatically
generates risk scores, and testing its usability and effectiveness
in real-world practice. In addition, our model focuses on
predicting risk of death and not predicting who would most
benefit from an intervention nor which individual risk factors
are causally associated with mortality. Future studies will need
to determine if use of this model for risk prediction can also
identify those most likely to benefit from interventions and
how to reduce risk."
"A novel machine-learning algorithm for predicting mortality risk after
hip fracture surgery","Introduction: Although several risk stratification models have been developed to predict hip fracture mor-
tality, efforts are still being placed in this area. Our aim is to (1) construct a risk prediction model for
long-term mortality after hip fracture utilizing the RSF method and (2) to evaluate the changing effects
over time of individual pre- and post-treatment variables on predicting mortality.
Methods: 1330 hip fracture surgical patients were included. Forty-five admission and in-hospital variables
were analyzed as potential predictors of all-cause mortality. A random survival forest (RSF) algorithm was
applied in predictors identification. Cox regression models were then constructed. Sensitivity analyses and
internal validation were performed to assess the performance of each model. C statistics were calculated
and model calibrations were further assessed.
Results: Our machine-learning RSF algorithm achieved a c statistic of 0.83 for 30-day prediction and 0.75
for 1-year mortality. Additionally, a COX model was also constructed by using the variables selected by
RSF, c statistics were shown as 0.75 and 0.72 when applying in 2-year and 4-year mortality prediction.
The presence of post-operative complications remained as the strongest risk factor for both short- and
long-term mortality. Variables including fracture location, high serum creatinine, age, hypertension, ane-
mia, ASA, hypoproteinemia, abnormal BUN, and RDW became more important as the length of follow-up
increased.
Conclusion: The RSF machine-learning algorithm represents a novel approach to identify important risk
factors and a risk stratification models for patients undergoing hip fracture surgery is built through this
approach to identify those at high risk of long-term mortality.","Using prospectively gathered data, 1) Novel risk stratification
models for hip fracture patients to identify those with a higher
mortality risk are built through a machine learning approach
which has never been used in this population. As compared to
previously reported models, we achieve higher c-statistic values
in both long-term and short-term mortality discrimination, which
may imply a better performance of our model. 2) The machine-
learning algorithm enables us to acquire new knowledge on two
aspects regarding the risk factors whichever identified exclusively
in our study such as “RDW” or those previously reported such as
“post-complication”. These two aspects include firstly a horizontal
comparison at a certain time point of the relative predictive values
across risk factors; and secondly, a changing pattern description of
each risk factor’s contribution to mortality on temporal scales.
A “risk stratification tool” was defined as a scoring system
or model used to predict or adjust for either mortality or mor-
bidity after surgery containing at least two different risk factors
[27]. By applying this definition, Moppett et al [24] found that at
least 25 different models have been used for risk stratification of
hip fracture patients. Among these, ASA, CCI, NHFS, E-PASS, and
O-POSSUM are the most frequently investigated. However, NHFS,
originally constructed and validated using univariate and multivari-
ate logistic regression analyses, is the only model designed specif-
ically for the hip fracture population; this model has produced the
most promising results in predicting short-term (30-day) mortal-
ity risk after hip fracture with a AUC of 0.72 [17,24,25]. Subse-
quently, NHFS prediction for long-term mortality was tested, al-
though no discrimination performance evaluation could be found
from the existing research [43]. Cenzer et al developed a prog-
nostic index for one-year mortality prediction and reported a c-
statistic of 0.73 in their study [4]. Another study by Jiang et al. de-
signed and validated a model for 1-year mortality with a c-statistic
of 0.74 [16]. As compared, we achieved a higher c statistic for 30-
day mortality prediction (0.83) and for 1-year mortality prediction
(0.75) using the machine learning algorithm. Additionally, after
construction of a cox using the variables selected by RSF, c statis-
tics were found to be 0.75 and 0.72 when applying them to 2-year
and 4-year mortality risk prediction. Whereas c statistic of model
by Jiang is 0.64 for 2-year and 0.63 for 4-year follow-up in our
population.
This novel machine-learning RSF algorithm enables us to wit-
ness changing patterns of importance for each variable and their
contribution to mortality along with over increasing lengths of
follow-up. The importance of mechanical ventilation continued to
reduce with longer follow-up, and the association with mortality
becomes negligible when follow-up extends beyond 1 year. A sim-
ilar pattern was seen in the “length of stay” variable in that it re-
mained among the top ten VIMPs regarding 1- and 2-year mor-
tality risk. In contrast, the importance of variables including frac-
ture location, Cr, age, hypertension, anemia, ASA, Hypoproteine-
mia, BUN abnormal, and RDW continued to increase. These find-
ings imply that, in general, in-hospital variables contribute most
significantly to short-term mortality risk, whereas admission vari-
ables contribute most significantly to long-term mortality risk. In
previous different prediction models, these included variables are
mentioned as independent risk factors for hip fracture [5,33,49].
Nkanang, B et al found the 48h mortality was associated with hy-
pertension, age and ASA [31]. Preoperative Hypoalbuminemia was
found as an important predictive index of short-term outcomes for
hip fracture [37]. Seyedi, H. R et al demonstrated a direct correla-
tion between plasma levels BUN and 3 month mortality after hip
fractures [39].Of note, the presence of a postoperative complica-
tion was the strongest risk factor for both short- and long-term
mortality. Prior research also showed a great contribution of post-
operation complications to excess hip fracture mortality [11,35], as
compare to them, thanks to RSF algorithm, our study provides a
relative importance value over time as compare to other risk fac-
tors. As previous research reported, RDW served as an indepen-
dent risk factor for death in the general population [7,19]. Our pre-
vious studies demonstrated that patients undergoing hip fracture
surgery who experience a greater fluctuation RDW between admis-
sion and discharge are at a heightened risk for all-cause mortality
[22,45,47]. In this study, the importance of RDW continue to in-
crease with longer follow-up, and the association with mortality
becomes significant difference when follow-up extends beyond 2
year. What is worth mentioning is that, the reason why surgical
procedure only contributes to 6-month mortality and anesthesia
only contribute to 3-month mortality remains uncovered. One pos-
sible explanation would be: for anesthesia, its effect on mortality
could actually last for 3-month, however, during the first month,
other variables with higher importance value contribute more to
mortality, therefore squeezing out anesthesia from the top-ten list
(rank at 14, as shown in Fig. 2). Similar explanation could also be
applied in surgical procedure. Nonetheless, further research is re-
quired to confirm this theory. At last, we are aware our study has several limitations, Firstly,
this study included data from a single center which may result in
selection bias, and confirmation of our findings should be sought
using other datasets. Secondly, we have not validated either RSF or
cox models with an external cohort from another advanced ortho-
pedic center. Although RSF effectively validates the model by cre-
ating trees with a random group of patients and variables (boot-
strap), performing sensitivity analyses, and internal validation with
four-year survival data, the model still derives these trees from the
original data set. Therefore, performance of these models should
be confirmed using an external cohort. Thirdly, we’re aware of that
traditional way in building risk stratification model also has its
strength in many aspects. Although prior research and our data
both support that RSF is more suitable in dealing with clinical data
which has become more and more complexed and entangled, and
RSF is reportedly outperform other machine learning algorithms in
terms of accuracy in prognostic prediction, we should be aware
that this method is not a one-size-fits all approach, a discussion
with a statistician should be taken before application."
"Validation of a Machine Learning Algorithm to Predict 180-Day Mortality
for Outpatients With Cancer","IMPORTANCE Machine learning (ML) algorithms can identify patients with cancer at risk of
short-term mortality to inform treatment and advance care planning. However, no ML
mortality risk prediction algorithm has been prospectively validated in oncology or compared
with routinely used prognostic indices.
OBJECTIVE To validate an electronic health record–embedded ML algorithm that generated
real-time predictions of 180-day mortality risk in a general oncology cohort.
DESIGN, SETTING, AND PARTICIPANTS This prognostic study comprised a prospective cohort of
patients with outpatient oncology encounters between March 1, 2019, and April 30, 2019. An
ML algorithm, trained on retrospective data from a subset of practices, predicted 180-day
mortality risk between 4 and 8 days before a patient’s encounter. Patient encounters took
place in 18 medical or gynecologic oncology practices, including 1 tertiary practice and 17
general oncology practices, within a large US academic health care system. Patients aged 18
years or older with outpatient oncology or hematology and oncology encounters were
included in the analysis. Patients were excluded if their appointment was scheduled after
weekly predictions were generated and if they were only evaluated in benign hematology,
palliative care, or rehabilitation practices.
EXPOSURES Gradient-boosting ML binary classifier.
MAIN OUTCOMES AND MEASURES The primary outcome was the patients’ 180-day mortality
from the index encounter. The primary performance metric was the area under the receiver
operating characteristic curve (AUC).
RESULTS Among 24 582 patients, 1022 (4.2%) died within 180 days of their index encounter.
Their median (interquartile range) age was 64.6 (53.6-73.2) years, 15 319 (62.3%) were
women, 18 015 (76.0%) were White, and 10 658 (43.4%) were seen in the tertiary practice.
The AUC was 0.89 (95% CI, 0.88-0.90) for the full cohort. The AUC varied across
disease-specific groups within the tertiary practice (AUC ranging from 0.74 to 0.96) but was
similar between the tertiary and general oncology practices. At a prespecified 40% mortality
risk threshold used to differentiate high- vs low-risk patients, observed 180-day mortality was
45.2% (95% CI, 41.3%-49.1%) in the high-risk group vs 3.1% (95% CI, 2.9%-3.3%) in the
low-risk group. Integrating the algorithm into the Eastern Cooperative Oncology Group and
Elixhauser comorbidity index–based classifiers resulted in favorable reclassification (net
reclassification index, 0.09 [95% CI, 0.04-0.14] and 0.23 [95% CI, 0.20-0.27], respectively).
CONCLUSIONS AND RELEVANCE In this prognostic study, an ML algorithm was feasibly
integrated into the electronic health record to generate real-time, accurate predictions of
short-term mortality for patients with cancer and outperformed routinely used prognostic
indices. This algorithm may be used to inform behavioral interventions and prompt earlier
conversations about goals of care and end-of-life preferences among patients with cancer.","In this prognostic cohort study, our results suggest that an ML
algorithm can be feasibly integrated into the EHR to generate
real-time, accurate predictions of short-term mortality risk for
patients with cancer that outperform traditional prognostic in-
dices. This study represents, to our knowledge, one of the first
prospective applications of an ML mortality risk prediction al-
gorithm in clinical oncology and provides 3 important in-
sights into the potential clinical applicability of real-time prog-
nostic algorithms in clinical practice.
First, this study is a true prospective validation of a pre-
viously described ML algorithm. The ML algorithm was trained
using patient data from 2016. In contrast, this prospective co-
hort was enrolled in 2019. Thus, no patient data in this pro-
spective validation was used as part of algorithm training. De-
spite many therapeutic advances since 2016, including the
widespread use of immunotherapy and biomarker-based thera-
pies, our algorithm demonstrated good performance on sev-
eral clinically relevant metrics even when applied to a pro-
spective cohort 3 years later.35 Additionally, the AUC, AUPRC,
and PPV from our prospective validation were equivalent to
or better than what has been reported in retrospective valida-
tions of ML mortality risk prediction algorithms in
oncology.23,25,36 This demonstrates that an ML mortality risk
prediction algorithm trained on retrospective data may still
have good performance and clinical utility when applied in a
recent cohort of patients with cancer. Importantly, the algo-
rithm still had a clinically applicable AUC (0.86) for practices
that were not included in the algorithm training.
Second, the ML algorithm demonstrated better AUC and
PPV compared with the ECOG and Elixhauser classifiers, and
enhanced classifiers that integrated the ML algorithm into ex-
isting prognostic indices resulted in better reclassification of
patients. Published ML prognostic algorithms often either do
not have a comparator or have been compared with regression-
based algorithms that are not used in clinical practice.25,37 In
contrast, ECOG and Elixhauser comorbidity scores are vali-
dated prognostic indices that are commonly used in decision-
making around treatment selection and clinical trial
enrollment. 38 Although Elixhauser comorbidities are in-
cluded in the ML algorithm inputs, ECOG is not included in the
algorithm because it was not available as a structured data ele-
ment in the EHR at the time of algorithm training. 25 As ECOG
and other variables of prognostic significance (eg, stage, line
of treatment, genetic variants) are increasingly coded in the
EHR, the performance of this algorithm could improve.
Third, the differences in prospective performance across
cancer types and stages give important information regard-
ing for which disease settings an automated ML prognostic al-
gorithm may be most useful in general oncology. For malig-
nancies with poorer performance (eg, melanoma, neuro-
oncology), several factors that were not included in our
algorithm may have greater prognostic importance; these in-
clude performance and cognitive status, molecular and ge-
netic biomarker expression, and imaging results. When con-
sidering the implementation of a tool such as this, clinicians
must consider the need to account for prognostic factors that
are unique to specific cancers. Relatedly, clinicians may use
this point-of-care tool differently with different threshold pref-
erences for patients with stage I to III cancer—who are gener-
ally treated with curative intent—compared with patients with
stage IV cancer who are generally treated with palliative in-
tent. Finally, despite recent concerns over algorithm bias
against underrepresented subgroups, 39,40 there were no ma-
jor differences in performance across site of practice, race/
ethnicity, and insurance status.
Although ML algorithms may improve mortality risk
prediction, it is important to demonstrate that improved
accuracy can translate to better clinician and patient
decision-making.41 ML mortality risk prediction tools may help
oncology clinicians better identify patients who are at high risk
of short-term mortality, engage in early goals of care discus-
sions, and meet quality metrics related to end-of-life care. In-
deed, in a subsequent randomized clinical trial, we applied this algorithm in clinical practice to identify oncology patients with
the highest predicted mortality and used behavioral nudges
to encourage clinicians to have end-of-life conversations with
those patients, resulting in significant increases in such
conversations. 28,42 Notably, for a cancer with a high baseline
rate of short-term mortality (eg, pancreatic cancer), the mor-
tality risk threshold for intervention may be higher to avoid a
high alert rate; the opposite may be true for a cancer with a low
rate of short-term mortality (eg, prostate cancer, myeloma). Our
algorithm retained good performance across several mortal-
ity risk thresholds, indicating that such thresholds could be cus-
tomized for clinical use (eTable 6 in the Supplement).
Limitations
There are several limitations to this study. First, using a risk
threshold of 40%, the sensitivity of the algorithm was only 27%.
This may reflect the inability to include important features, such
as stage and ECOG values, in the original algorithm training, in
addition to the high mortality risk threshold used. Indeed, when
using a lower but still clinically meaningful risk threshold of 10%,
sensitivity improved to nearly 70% (eTable 6 in the Supple-
ment). Third, in the comparison of the ML algorithm against the
ECOG classifier, we were limited to only using coded ECOG val-
ues. Patients whose ECOG was coded were systematically dif-
ferent than patients whose ECOG was not coded, as shown in
eTable 3 in the Supplement. However, the purpose of this analy-
sis was not to develop a new classifier that includes ECOG but
rather to assess whether the ML algorithm improves classifica-
tion above and beyond the ECOG for those with a coded ECOG.
Indeed, the positive NRI indicates that even within the sub-
group of those with coded ECOG, the ML algorithm led to favor-
able reclassification of patients compared with the ECOG clas-
sifier alone. Using imputation strategies would not have been
appropriate given the high degree of missingness of ECOG."
"Machine Learning Approaches to Predict 6-Month Mortality
Among Patients With Cancer","IMPORTANCE Machine learning algorithms could identify patients with cancer who are at risk of
short-term mortality. However, it is unclear how different machine learning algorithms compare and
whether they could prompt clinicians to have timely conversations about treatment and end-of-
life preferences.
OBJECTIVES To develop, validate, and compare machine learning algorithms that use structured
electronic health record data before a clinic visit to predict mortality among patients with cancer.
DESIGN, SETTING, AND PARTICIPANTS Cohort study of 26 525 adult patients who had outpatient
oncology or hematology/oncology encounters at a large academic cancer center and 10 affiliated
community practices between February 1, 2016, and July 1, 2016. Patients were not required to
receive cancer-directed treatment. Patients were observed for up to 500 days after the encounter.
Data analysis took place between October 1, 2018, and September 1, 2019.
EXPOSURES Logistic regression, gradient boosting, and random forest algorithms.
MAIN OUTCOMES AND MEASURES Primary outcome was 180-day mortality from the index
encounter; secondary outcome was 500-day mortality from the index encounter.
RESULTS Among 26 525 patients in the analysis, 1065 (4.0%) died within 180 days of the index
encounter. Among those who died, the mean age was 67.3 (95% CI, 66.5-68.0) years, and 500
(47.0%) were women. Among those who were alive at 180 days, the mean age was 61.3 (95% CI, 61.1-
61.5) years, and 15 922 (62.5%) were women. The population was randomly partitioned into training
(18 567 [70.0%]) and validation (7958 [30.0%]) cohorts at the patient level, and a randomly
selected encounter was included in either the training or validation set. At a prespecified alert rate of
0.02, positive predictive values were higher for the random forest (51.3%) and gradient boosting
(49.4%) algorithms compared with the logistic regression algorithm (44.7%). There was no
significant difference in discrimination among the random forest (area under the receiver operating
characteristic curve [AUC], 0.88; 95% CI, 0.86-0.89), gradient boosting (AUC, 0.87; 95% CI,
0.85-0.89), and logistic regression (AUC, 0.86; 95% CI, 0.84-0.88) models (P for comparison = .02).
In the random forest model, observed 180-day mortality was 51.3% (95% CI, 43.6%-58.8%) in the
high-risk group vs 3.4% (95% CI, 3.0%-3.8%) in the low-risk group; at 500 days, observed mortality
was 64.4% (95% CI, 56.7%-71.4%) in the high-risk group and 7.6% (7.0%-8.2%) in the low-risk group.
In a survey of 15 oncology clinicians with a 52.1% response rate, 100 of 171 patients (58.8%) who had
been flagged as having high risk by the gradient boosting algorithm were deemed appropriate for a
conversation about treatment and end-of-life preferences in the upcoming week","In this cohort study, ML models based on structured EHR data accurately predicted the short-term
mortality risk of individuals with cancer from oncology practices affiliated with an academic cancer
center. The gradient boosting and random forest models had good PPV at manageable alert rates,
and all ML models had adequate discrimination (ie, AUC, 0.86-0.88) in predicting 6-month mortality.
The PPVs of the random forest and gradient boosting algorithms were much higher than historical estimates from clinician assessment alone. 7,16 Unlike standard prognostic tools, our models
incorporated variability in laboratory data and many comorbidities into predictions. Moreover,
clinicians expressed reasonable agreement that the patients determined to have the highest
predicted risk of death by 1 of the ML models were appropriate for a conversation about goals and
end-of-life preferences, an early indication that ML-derived mortality predictions may be useful for
encouraging these discussions.
There are several strengths of this analysis. To our knowledge, this is the first investigation
comparing ML classifiers, including regression-based classifiers, to predict mortality in a large general
oncology population. 32 Unlike previously developed ML-based prognostic tools in oncology, 19,20 our
models were trained on all patients seen at oncology or hematology/oncology practices regardless
of receipt of cancer-directed therapy. Because some patients could have received care outside of the
UPHS system and we did not have access to registry or claims data, we could not assess what
proportion of our cohort received systemic therapy after the index encounter. Furthermore,
compared with previously published ML classifiers in oncology, our models used fewer variables, all
of which are commonly available in structured formats in real-time EHR databases. Thus, this model
is more efficient than previously trained ML models in the general oncology setting. Finally, most
patients identified as having high risk by the model were deemed appropriate for a conversation
about goals and end-of-life preferences by oncology clinicians. Our survey findings should be
interpreted with some caution because we used an older version of the gradient boosting model with
less robust feature selection and hyperparameter optimization. Using the fully optimized version of
the gradient boosting or random forest models, which had a higher PPV than the version presented
to clinicians during the survey, may have improved results from the survey.
Machine learning classifiers, in contrast to regression-based classifiers, account for often
unexpected predictor variables and interactions and can facilitate recognition of predictors not
previously described in the literature. 32,33 All models had excellent discriminative performance and
PPV for predicting 6-month mortality, particularly compared with other EHR-based gradient
boosting and random forest machine prognostic models published in the literature. 19,21
In contrast to previous reports, 21 there was no statistically significant difference in AUC among
the gradient boosting, random forest, and logistic regression algorithms after adjusting for multiple
comparisons, although the random forest model had an advantage compared with the logistic
regression model. However, the gradient boosting and random forest models outperformed the
logistic regression model in PPV, which is potentially more clinically relevant than AUC. 34 Finally, all
models placed importance on variables with known prognostic implications, including age, diagnosis
of metastatic cancer, most recent albumin level, and most recent alkaline phosphatase level. The
regression model tended to place more importance on diagnosis codes and demographic
characteristics than the random forest or gradient boosting models, which placed more importance
on recent laboratory values.
Accurate identification of patients at high risk of short-term mortality is important in oncology
given the release of recent guidelines advocating for early palliative care and advance care planning
for high-risk populations. 3,4 Our findings demonstrated that ML algorithms can predict a patient’s
risk of short-term mortality with good discrimination and PPV. Such a tool could be very useful in
aiding clinicians’ risk assessments for patients with cancer as well as serving as a point-of-care prompt
to consider discussions about goals and end-of-life preferences. Machine learning algorithms can be
relatively easily retrained to account for emerging cancer survival patterns. As computational
capacity and the availability of structured genetic and molecular information increase, we expect that
predictive performance will increase and there may be a further impetus to implement similar tools
in practice.
Limitations
There are several limitations to this analysis. First, even with robust feature selection and
hyperparameter optimization, the random forest and gradient boosting models were overfit or fit peculiarities in the training data that may not generalize to other data sources. Despite overfitting,
the gradient boosting and random forest models had excellent discrimination and good PPVs in the
holdout validation set, outperforming the logistic regression model. Nevertheless, these models
should be validated in other oncology settings to determine generalizability.
Second, unlike previous analyses comparing ML approaches with routinely used predictive
tools, 22,33 there was not a criterion-standard prognostic assessment tool for comparison, and it is
unclear whether our models outperformed other previously described tools in disease-specific
settings. A previous analysis found that ML predictions in specific subgroups outperformed
predictions from randomized clinical trials or registry data. 19 Our study was underpowered for these
subgroup comparisons.
Third, these tools were developed to be used in a general medical oncology setting and may not
be generalizable to patients seen in radiation oncology, gynecologic oncology, or other oncology
specialty practices or health systems with different EHRs. However, the features used in our models
are all commonly available in structured data fields in most health system EHRs.
Fourth, our primary outcome relied in part on SSA data, which are commonly used to determine
mortality in health services research. It has recently been shown that the SSA Death Master File may
underestimate actual mortality rates. 35 We attempted to address this by supplementing SSA data
with EHR death information; however, some misclassification may still exist.
Fifth, our survey only assessed the feasibility of an ML model prompting serious illness
conversations and was not a definitive validation of model performance. Clinicians may have had
practical reasons for indicating that high-risk patients were not appropriate for serious illness
conversations, including known patient and family preferences that would have precluded a
conversation that week. Furthermore, we only surveyed clinicians regarding patients identified as
having high risk and thus could have inadvertently biased clinicians toward responding that patients
were appropriate for a conversation about goals and end-of-life wishes."
"Development and Validation of a Deep Learning Algorithm
for Mortality Prediction in Selecting Patients With Dementia
for Earlier Palliative Care Interventions","IMPORTANCE Early palliative care interventions drive high-value care but currently are underused.
Health care professionals face challenges in identifying patients who may benefit from palliative care.
OBJECTIVE To develop a deep learning algorithm using longitudinal electronic health records to
predict mortality risk as a proxy indicator for identifying patients with dementia who may benefit
from palliative care.
DESIGN, SETTING, AND PARTICIPANTS In this retrospective cohort study, 6-month, 1-year, and
2-year mortality prediction models with recurrent neural networks used patient demographic
information and topics generated from clinical notes within Partners HealthCare System, an
integrated health care delivery system in Boston, Massachusetts. This study included 26 921 adult
patients with dementia who visited the health care system from January 1, 2011, through December
31, 2017. The models were trained using a data set of 24 229 patients and validated using another
data set of 2692 patients. Data were analyzed from September 18, 2018, to May 15, 2019.
MAIN OUTCOMES AND MEASURES The area under the receiver operating characteristic curve
(AUC) for 6-month and 1- and 2-year mortality prediction models and the factors contributing to the
predictions.
RESULTS The study cohort included 26 921 patients (16 263 women [60.4%]; mean [SD] age, 74.6
[13.5] years). For the 24 229 patients in the training data set, mean (SD) age was 74.8 (13.2) years and
14 632 (60.4%) were women. For the 2692 patients in the validation data set, mean (SD) age was
75.0 (12.6) years and 1631 (60.6%) were women. The 6-month model reached an AUC of 0.978 (95%
CI, 0.977-0.978); the 1-year model, 0.956 (95% CI, 0.955-0.956); and the 2-year model, 0.943 (95%
CI, 0.942-0.944). The top-ranked latent topics associated with 6-month and 1- and 2-year mortality
in patients with dementia include palliative and end-of-life care, cognitive function, delirium, testing
of cholesterol levels, cancer, pain, use of health care services, arthritis, nutritional status, skin care,
family meeting, shock, respiratory failure, and swallowing function.
CONCLUSIONS AND RELEVANCE A deep learning algorithm based on patient demographic
information and longitudinal clinical notes appeared to show promising results in predicting mortality
among patients with dementia in different time frames. Further research is necessary to determine
the feasibility of applying this algorithm in clinical settings for identifying unmet palliative care
needs earlier.","his study demonstrates that a deep neural network trained using a large data set with patient
demographics and longitudinal clinical notes from the EHR can be accurate and useful in predicting
6-month, 1-year, and 2-year mortality and thus could be used as a proxy for selecting patients who
may benefit from palliative care assessment. The high performance (AUC scores) of all 3 models shows that clinical notes along with patient demographics are informative, and the deep learning
neural network structure can successfully capture short- and long-range longitudinal patterns. In
addition, converting clinical notes into clinically meaningful topics using topic modeling allows us to
trace and visualize how the model made its prediction for each patient (Figure 3). Meanwhile, at the
population level, the model helps us to identify what factors are strongly associated with mortality
risk in different time frames in patients with ADRD (Table 2). In the past, studies of mortality prediction have relied on claims data,38 administrative data, 39
or other types of data (eg, surveys), 40 but few have used clinical notes. We believe that this study is
the first to investigate clinical notes in a deep neural network to identify topics associated with
mortality prediction among patients with ADRD. In the LSTM-based neural network, clinical notes
contribute to mortality prediction in 2 aspects: the longitudinal patterns of the documentation and
the content of clinical notes. First, frequent documentation in the medical record likely indicates
increasing severity of illness and worsening frailty in the context of ADRD; thus, with the help of the
LSTM neural network, long- and short-term longitudinal patterns can be identified for mortality
prediction. Second, topics generated using the topic modeling method captured semantic and
syntactic structures of large quantities of clinical notes, providing rich information for mortality
prediction. Among 500 topics, top-ranked predictive factors associated with 6-month, 1-year, and
2-year mortality include palliative and end-of-life care, cognitive function (eg, dementia status,
delirium), laboratory testing (eg, testing of cholesterol levels), cancer, pain, use of health care
services (eg, hospital or facility care, health care encounter, intensive care, and nursing care),
arthritis, nutritional status, skin care, family meeting, result communication, swallowing function,
shock, respiratory failure, and medication delivery, among others. Some of these topics indicate that
health care professionals may recognize a patient’s decline (such as notation of palliative and
end-of-life care), while others may signal changing patient conditions that health care professionals
have yet to recognize (such as cognitive function, delirium, and functional status). Only a few studies
explicitly list variables used to predict mortality in the ADRD population. For example, Mitchell
et al 29,41 included length of stay, dyspnea, pressure ulcers, total functional dependence, being
bedbound most of the day, insufficient intake, bowel incontinence, body mass index, weight loss,
and congestive heart failure as variables that best predict 6-month survival. Using the topic modeling
method, we were able to capture topics that seem similar to variables selected a priori as well as
additional variables that may not be available in many structured data. Our models can be calculated with much less time and effort in large patient populations
compared with existing screening methods (eg, the “surprise question” method). 24,42 Previous
studies demonstrated that health care professionals, although directionally generally correct, have
trouble estimating the timing of death. 43,44 Long-term predictions are generally more difficult for
humans; this may also apply to the machine, because our mortality prediction models achieved
slightly lower performance when the prediction time frames became longer. However, our 2-year
model still reached a high AUC of 0.943. Therefore, using deep learning predictive models in patient
stratification in clinical practice has notable promise for identifying patients with ADRD who are
approaching their last 1 or 2 years of life.
Although mortality is not the only important factor contributing to assessment of need for
palliative care, tools such as this algorithm may provide an important proxy that health care
professionals and systems can use to consider patients for possible palliative care interventions. By
adjusting the sensitivity and specificity along the receiver operating characteristic curve, deep
learning–based tools may be used to decrease the burden on health care professionals by identifying
a manageable denominator of patients for consideration for interventions according to available
palliative care resources. They may also help guide prioritization of patients’ needs based on
predicted probability of mortality in a certain time frame. Importantly, these models should not be
used in the absence of input from health care professionals, because computer-predicted mortality
alone is not a decisive indicator of palliative care needs; the benefit of palliative care depends on far
more than risk of death (eg, individual preferences, functional and quality of life effects of serious
illness, psychosocial and spiritual needs, and burden of illness on caregiving networks). In addition,
we have chosen a longer time frame of 2 years to target driving earlier conversations about patients’
goals and values in ADRD and to focus on patient-centric conversations rather than system-centric
decisions such as enrollment in hospice.
Limitations
One major limitation of our study is that our models have not been validated using external data sets.
Because of population diversity and clinical documentation variations among different health care
systems, 45 we suspect that models trained from the data of one health care system may require
additional tuning to be adaptable to other systems or EHRs. Therefore, until a systematic validation
is performed, including using a different data set to assess the models’ generalizability, these models
should not be widely applied to other health care systems. Second, machine learning–based models
developed using EHR data may be subject to bias because the EHR generally contains more medical
information for sicker patients, and this decreases the generalizability of the models to non-EHR
settings. 46,47 Third, model-based screening can only make predictions at the times when notes are
available for patients, requiring a minimum of 2 note events. This limitation may affect the model’s
capability to make predictions for all patients with ADRD at any time. Fourth, the ranking of the
predictive topics, which was generated based on the attention of the neural network during the
prediction for the validation cohort, does not directly correlate to the proportion of notes or patients
to whom these topics apply, and such rankings may be subject to change as the predictive cohort
changes. Fifth, prediction of mortality is only one component of identifying patients who may benefit
from palliative care, and future predictive modeling efforts should move beyond mortality prediction
to work on identifying broader needs for populations of seriously ill patients, such as predicting
functional decline and effects on quality of life. 48"
